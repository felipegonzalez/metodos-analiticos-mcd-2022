[["index.html", "Métodos analíticos, ITAM 2022 Temario Evaluación", " Métodos analíticos, ITAM 2022 Felipe González 2022-05-09 Temario Este curso trata sobre diversas técnicas de análisis de datos, en su mayoría diseñadas para escalar a datos grandes. El enfoque del curso se concentra más en el entendimiento y aplicación de los algoritmos y los métodos, y menos en las herramientas para implementarlos. Análisis de conjuntos frecuentes Algoritmo a-priori Market basket analysis Búsqueda de elementos similares Minhashing para documentos Locality Sensitive Hashing (LSH), joins aproximados Sistemas de recomendación 1 Recomendación por contenido y filtros colaborativos Factorización de matrices y dimensiones latentes Reducción de dimensionalidad: DVS Descomposición en valores singulares Componentes principales Sistemas de recomendación 2 Métodos basados en similitud Mínimos cuadrados alternados Descenso en gradiente estocástico Retroalimentación implícita Recuperación de información Índices invertidos Modelo de espacio vectorial Normalización y similitud Indexado semántico latente Análisis de redes 1 Medidas de centralidad y pagerank Análisis de redes 2 Clustering y comunidades Modelos de lenguaje 1 N-gramas y conteos Aplicaciones Modelos de lenguaje 2 Inmersiones de palabras Modelos básicos de redes neuronales Aplicaciones de modelos de lenguaje Corrección de ortografía, reconocimiento de idiomas Clasificación de textos Métodos generales de clustering Las notas del curso están R, y en algunos casos usamos python o línea de comandos. Puedes usar python también para hacer tareas y ejercicios. Nuestro texto básico es (Leskovec, Rajaraman, and Ullman 2014). Referencias básicas adicionales son (Jurafsky and Martin 2000) (para procesamiento de lenguaje natural), y sparklyr para utlizar la interfaz de R a Spark. Evaluación Tareas semanales (30%) Examen teórico parcial (35%) Trabajo final (35%) Referencias "],["intro.html", "1 Introducción Tipos de soluciones", " 1 Introducción En este curso principalmente consideraremos técnicas para procesar, analizar y entender conjuntos de datos en dimensión alta (es decir, existen muchos atributos relevantes que describen a cada observación). Tareas básicas en este curso son, por ejemplo: Entender si las observaciones forman grupos útiles o interpretables, Construir o seleccionar atributos importantes para describir los datos de maneras más simples o útiles, En general, descubrir otros tipos de estructuras interesantes en datos de dimensión alta. Veremos aplicaciones relacionadas con este problema, como Análisis de market basket (descubrir características o artículos asociados), Sistemas de recomendación (medir similitud entre usuarios o artículos para hacer recomendaciones), Búsqueda de elementos muy similares o duplicados, Análisis de redes y detección de comunidades, Análisis y modelos para lenguaje natural. En general, consideraremos técnicas que son escalables y se aplican a datos masivos, que son los casos donde la dimensionalidad alta realmente puede explotarse efectivamente. Comenzaremos por describir las dificultades del análisis en dimensión alta. Simlitud en dimensión alta Cuando queremos identificar casos similares, o agruparlos por similitud, generalmente estamos pensando alguna tarea que queremos resolver: por ejemplo, recomendar productos o contenido a usuarios similares, detectar imágenes o textos duplicados o reusados, focalizar programas sociales a distintos tipos de hogares o personas, etc. Muchas veces, identificar los atributos correctos y el tipo de similitud produce herramientas útiles para resolver varios problemas en cada área de interés. Por ejemplo, la medida correcta de similitud de usuarios de Netflix tiene varios usos, igual que la identificación de palabras o pasajes de texto que ocurren en contextos similares. Consideramos por ejemplo perfiles de actividad en un sitio (Netflix), productos seleccionados por un comprador en un súper, imágenes o textos. En todos estos casos, las observaciones individuales tienen número muy grande de atributos (qué artículos están o no en una canasta, qué películas vio alguien, número de pixeles, o qué palabras ocurren y en qué orden ocurren). Hay varios problemas que tenemos que manejar en dimensión alta: Distintos atributos pueden ser importantes y otros ser no ser relevantes en ciertos grupos o en general. Por ejemplo: si queremos predecir la siguiente palabra en un texto (o traducirlo), la ausencia o presencia de algunas palabras es importantes, y muchas otras palabras son irrelevantes. Puede ser que muchos atributos no sean de interés para formar grupos útiles o identificar casos similares. Los atributos generalmente tienen relaciones complejas, y no es claro qué medidas de similitud son apropiadas. Medidas de distancia o similitud Consideremos la distancia euclideana en dimensión alta. Si generamos una muestra centrada en el origen, las distancias al origen se ve cómo sigue: library(tidyverse) centro &lt;- rep(0, 50) puntos &lt;- MASS::mvrnorm(n = 5000, mu = centro, Sigma = diag(rep(1, 50))) distancias &lt;- map_dbl(1:nrow(puntos), ~ sqrt(sum((centro - puntos[.x, ])^2))) qplot(distancias, bins = 30) Y vemos que ningún punto está realmente cerca del origen, y las distancias varían alrededor de un valor fijo. Si consideramos todos los posibles pares de puntos, vemos que todos parecen estar más o menos igual de lejos unos de otros: dist_pares &lt;- dist(puntos) |&gt; as.numeric() qplot(dist_pares, bins = 30) En dimensión alta, nuestra intuición muchas veces no funciona muy bien. Por ejemplo, supongamos que en una dimensión tenemos dos grupos claros: library(tidyverse) library(patchwork) library(glue) set.seed(10012) x &lt;- rnorm(100, c( -1,1), c(0.2,0.2)) datos &lt;- tibble(id = 1:100, variable = &quot;x&quot;, x = x) |&gt; mutate(grupo = id %% 2) ggplot(datos, aes(x = x, fill = factor(grupo))) + geom_histogram(bins = 30) Podemos calcular las distancias entre pares: distancias_pares &lt;- datos |&gt; select(-id, -grupo) |&gt; dist() |&gt; as.numeric() ## Warning in dist(select(datos, -id, -grupo)): NAs introduced by coercion g_1 &lt;- qplot(distancias_pares, bins = 30) Ahora agregamos 50 variables adiconales_ datos_aleatorios &lt;- map_df(1:50, function(i){ datos |&gt; select(id, grupo) |&gt; mutate(variable = glue(&quot;x_{i}&quot;), x = rnorm(100, 0, 1)) }) datos_1 &lt;- bind_rows(datos, datos_aleatorios) |&gt; pivot_wider(names_from = variable, values_from = x) Y vemos claramente que hay una estructura de grupos en los datos el la gráfica de la izquierda. Sin embargo, si agregamos variables ruidosas, la estructura no es clara y es difícil de recuperar: distancias_pares_2 &lt;- datos_1 |&gt; select(-id, -grupo) |&gt; dist() |&gt; as.numeric() g_2 &lt;- qplot(distancias_pares_2, bins = 30) g_1 + g_2 Si usamos un método simple de clustering, no recuperamos los grupos originales: grupos_km &lt;- kmeans(datos_1 |&gt; select(contains(&quot;x&quot;)), centers = 2) |&gt; pluck(&quot;cluster&quot;) table(grupos_km, datos_1$grupo) ## ## grupos_km 0 1 ## 1 38 29 ## 2 12 21 Tipos de soluciones Proyección y búsqueda de marginales interesantes En primer lugar, puede ser que aspectos útiles puedan extraerse de algunas marginales particulares \\(P(X_1, X_2)\\). Por ejemplo: Hay muchas variables ruidosas, en el sentido que no presentan estructuras interesantes o no son útiles para la tarea que nos interesa. (estas paso tiende a ser más guiado por teoría). Podemos buscamos regiones \\(P(X_1 = x_1, X_2 = x_2)\\) alrededor de las cuales se acumula alta probabilidad, o de otra manera: podemos buscar modas de marginales con alta densidad. Aplicaciones: análisis de conjuntos frecuentes o canastas, selección de características según varianza. Proyecciones globales Muchas veces podemos reducir dimensionalidad si reexpresamos variables (ya sea linealmente o no), y luego proyectamos (descomposición en valores singulares, PCA, descomposición de matrices) a regiones de alta densidad. Aplicaciones: construcción de índices resumen, sistemas de recomendación, indexado semántico latente en análisis de texto. Descripción de estructura local En algunos casos, los datos pueden ser del tipo donde la estructura local en pequeñas regiones del espacio de entradas es importante, y algunos casos tienden a acumularse en regiones particulares: Duplicados cercanos, búsqueda de vecinos cercanos. Análisis de centralidad en redes, búsqueda de comunidades. Métodos de reducción de dimensionalidad como t-sne y clustering Inmersiones (embeddings) Para algunos tipos de datos, la reducción de dimensionalidad debemos hacerla ad-hoc al problema. Por ejemplo, Redes convolucionales de clasificación de imágenes para obtener representaciones en dimensión baja de imágenes (similitud de imágenes). Construcción de representaciones donde palabras que ocurren en lugares similares son proyectadas a valores similares (inmersiones de palabras, redes neuronales para NLP). "],["frecuentes.html", "2 Análisis de conjuntos frecuentes 2.1 Datos de canastas 2.2 Conjuntos frecuentes 2.3 Monotonicidad de conjuntos frecuentes 2.4 Algoritmo a-priori 2.5 Modelos simples para análisis de canastas 2.6 Soporte teórico y conjuntos frecuentes 2.7 Reglas de asociación 2.8 Dificultades en el análisis de canastas 2.9 Otras medidas de calidad de reglas 2.10 Selección de reglas 2.11 Búsqueda de reglas especializadas 2.12 Visualización de asociaciones 2.13 Otras aplicaciones 2.14 Ejercicios", " 2 Análisis de conjuntos frecuentes Una de las tareas más antiguas de la minería de datos es la búsqueda de conjuntos frecuentes en canastas, o un análisis derivado que se llama análisis de reglas de asociación. Originalmente, pensamos que tenemos una colección grande de tickets de un supermercado. Nos interesa encontrar subconjuntos de artículos (por ejemplo, pan y leche) que ocurren frecuentemente en esos tickets. La idea es que si tenemos estos subconjuntos frecuentes, entonces podemos entender mejor el tipo de compras que hacen los clientes, diseñar mejor promociones y entender potenciales efectos cruzados, reordenar los estantes del supermercado, etc. En general, los conjuntos frecuentes indican asociaciones (y cuantificaciones de la asociación) entre artículos que hay que tomar en cuenta al momento de tomar decisiones. Esto normalmente se llama análisis de market basket. El análisis de subconjuntos frecuentes puede ser utilizado para otros propósitos, como veremos más adelante. 2.1 Datos de canastas Consideremos el siguiente ejemplo chico del paquete arules. Contiene unas \\(10\\) mil canastas observadas en de un supermercado durante un mes, agregadas a \\(169\\) categorías. En muchos casos prácticos, el número de canastas o transacciones puede llegar hasta los miles o millones de millones de transacciones, y el número de artículos puede ser de miles o decenas de miles. data(Groceries) # del paquete arules Groceries ## transactions in sparse format with ## 9835 transactions (rows) and ## 169 items (columns) lista_mb &lt;- as(Groceries, &quot;list&quot;) Estas son tres canastas (tickets) de ejemplo: lista_mb[[2]] ## [1] &quot;tropical fruit&quot; &quot;yogurt&quot; &quot;coffee&quot; lista_mb[[52]] ## [1] &quot;canned beer&quot; lista_mb[[3943]] ## [1] &quot;sausage&quot; &quot;UHT-milk&quot; &quot;flour&quot; &quot;flower (seeds)&quot; Describiremos algunas características típicas de este tipo de datos. En primer lugar, podemos calcular la distribución del número de artículos por canasta, y vemos que es una cantidad relativamente baja en comparación al número total de artículos existentes: sprintf(&quot;Número de canastas: %s&quot;, length(lista_mb)) ## [1] &quot;Número de canastas: 9835&quot; num_items &lt;- sapply(lista_mb, length) sprintf(&quot;Promedio de artículos por canasta: %.3f&quot;, mean(num_items)) ## [1] &quot;Promedio de artículos por canasta: 4.409&quot; qplot(num_items, binwidth = 1) Podemos hacer una tabla con las canastas y examinar los artículos más frecuentes: canastas_tbl &lt;- tibble( canasta_id = 1:length(lista_mb), articulos = lista_mb) canastas_tbl ## # A tibble: 9,835 × 2 ## canasta_id articulos ## &lt;int&gt; &lt;list&gt; ## 1 1 &lt;chr [4]&gt; ## 2 2 &lt;chr [3]&gt; ## 3 3 &lt;chr [1]&gt; ## 4 4 &lt;chr [4]&gt; ## 5 5 &lt;chr [4]&gt; ## 6 6 &lt;chr [5]&gt; ## 7 7 &lt;chr [1]&gt; ## 8 8 &lt;chr [5]&gt; ## 9 9 &lt;chr [1]&gt; ## 10 10 &lt;chr [2]&gt; ## # … with 9,825 more rows canastas_tbl$articulos[[1]] ## [1] &quot;citrus fruit&quot; &quot;semi-finished bread&quot; &quot;margarine&quot; ## [4] &quot;ready soups&quot; num_canastas &lt;- nrow(canastas_tbl) articulos_frec &lt;- canastas_tbl |&gt; unnest(cols = articulos) |&gt; group_by(articulos) |&gt; summarise(n = n()) |&gt; mutate(prop = n / num_canastas) |&gt; arrange(desc(n)) articulos_frec |&gt; mutate(across(where(is_double), ~ round(.x, 3))) |&gt; DT::datatable() ggplot(articulos_frec, aes(x = prop)) + geom_histogram(binwidth = 0.01) + xlab(&quot;Proporción de canastas&quot;) + ylab(&quot;Número de artículos&quot;) Y vemos que hay algunos pocos artículos que ocurren a tasas muy altas en las canastas. La mayoría tiene tasas de ocurrencia baja, y muchos ocurren en una fracción pequeña de las transacciones. Un primer análisis que podríamos considerar es el de canastas completas que ocurren frecuentemente. ¿Qué tan útil crees que puede ser este análisis? colapsar_canasta &lt;- function(x, sep = &quot;-&quot;){ # convierte cada canasta a una cadena x |&gt; as.character() |&gt; sort() |&gt; paste(collapse = &quot;-&quot;) } canastas_conteo &lt;- canastas_tbl |&gt; mutate(canasta_str = map_chr(articulos, colapsar_canasta)) |&gt; group_by(canasta_str) |&gt; summarise(n = n(), .groups = &quot;drop&quot;) |&gt; mutate(prop = round(n /num_canastas, 5)) |&gt; arrange(desc(n)) nrow(canastas_conteo) ## [1] 7011 Y aquí vemos las canastas más frecuentes: DT::datatable(canastas_conteo |&gt; head(n = 100) |&gt; mutate_if(is.numeric, ~ round(.x, 4))) Hay algunas canastas (principalmente canastas que contienen solo un artículo) que aparecen con frecuencia considerable (alrededor de \\(1\\%\\) o \\(2\\%\\)), pero las canastas están bastante dispersas en el espacio de posibles canastas (que es gigantesco: ¿puedes calcularlo?). Debido a esta dispersión este análisis es de utilidad limitada. Datos de canastas El tamaño de las canastas normalmente es chico (por ejemplo de \\(1\\) a \\(30\\) artículos distintos). El número total de artículos típicamente no es muy grande (de cientos a cientos de miles, por ejemplo). El número de canastas puede ser mucho mayor (en algunos casos miles de millones) y quizá no pueden leerse completas en memoria. La mayoría de los artículos ocurre con frecuencias relativamente bajas, aunque unos cuantos tienen frecuencia alta. El número de canastas distintas es alto, y hay pocas canastas frecuentes. El último inciso señala que encontrar canastas frecuentes no será muy informativo. En lugar de eso buscamos conjuntos de artículos (que podríamos llamar subcanastas) que forman parte de muchas canastas. 2.2 Conjuntos frecuentes Un enfoque simple y escalable para analizar estas canastas es el de los conjuntos frecuentes (frequent itemsets). Conjuntos frecuentes Consideramos un conjunto de artículos \\(I = \\{s_1,s_2,\\ldots, s_k\\}\\). El soporte de \\(I\\) lo definimos como la proporción de canastas que contienen (al menos) estos artículos: \\[P(I) = \\frac{n(I)}{n},\\] donde \\(n(I)\\) es el número de canastas que contienen todos los artículos de \\(I\\), y \\(n\\) es el número total de canastas. Sea \\(s\\in (0,1)\\). Para este valor fijo \\(s\\), decimos que un conjunto de artículos \\(I\\) es un conjunto frecuente cuando \\(P(I)\\geq s\\). Ejercicio Considera las canastas {1,2,3}, {1,2}, {2,4}, {2,3}. ¿Cuáles son los itemsets frecuentes de soporte &gt; 0.4? Ejemplo Explicamos más adelante la función apriori de arules, pero por lo pronto podemos examinar algunos conjuntos frecuentes de soporte mínimo \\(0.01\\) (como hay alrededor de \\(10000\\) canastas, esto significa que canastas que aparecieron al menos \\(100\\) veces durante el mes): pars &lt;- list(supp = 0.01, target = &quot;frequent itemsets&quot;) ap &lt;- apriori(lista_mb, parameter = pars) length(ap) ## [1] 333 Veamos algunos conjuntos frecuentes de tamaño \\(1\\): ap_1 &lt;- subset(ap, size(ap) == 1) length(ap_1) ## [1] 88 sort(ap_1, by = &quot;support&quot;) |&gt; DATAFRAME() |&gt; head(10) ## items support count ## 88 {whole milk} 0.25551601 2513 ## 87 {other vegetables} 0.19349263 1903 ## 86 {rolls/buns} 0.18393493 1809 ## 84 {soda} 0.17437722 1715 ## 85 {yogurt} 0.13950178 1372 ## 81 {bottled water} 0.11052364 1087 ## 83 {root vegetables} 0.10899847 1072 ## 82 {tropical fruit} 0.10493137 1032 ## 79 {shopping bags} 0.09852567 969 ## 80 {sausage} 0.09395018 924 Algunas de tamaño \\(2\\) y \\(3\\): ap_2 &lt;- subset(ap, size(ap) == 2) length(ap_2) ## [1] 213 sort(ap_2, by = &quot;support&quot;) |&gt; head(10) |&gt; DATAFRAME() ## items support count ## 301 {other vegetables,whole milk} 0.07483477 736 ## 300 {rolls/buns,whole milk} 0.05663447 557 ## 298 {whole milk,yogurt} 0.05602440 551 ## 291 {root vegetables,whole milk} 0.04890696 481 ## 290 {other vegetables,root vegetables} 0.04738180 466 ## 297 {other vegetables,yogurt} 0.04341637 427 ## 299 {other vegetables,rolls/buns} 0.04260295 419 ## 286 {tropical fruit,whole milk} 0.04229792 416 ## 295 {soda,whole milk} 0.04006101 394 ## 293 {rolls/buns,soda} 0.03833249 377 ap_3 &lt;- subset(ap, size(ap) == 3) length(ap_3) ## [1] 32 sort(ap_3, by=&quot;support&quot;) |&gt; head() |&gt; DATAFRAME() ## items support count ## 327 {other vegetables,root vegetables,whole milk} 0.02318251 228 ## 332 {other vegetables,whole milk,yogurt} 0.02226741 219 ## 333 {other vegetables,rolls/buns,whole milk} 0.01789527 176 ## 322 {other vegetables,tropical fruit,whole milk} 0.01708185 168 ## 331 {rolls/buns,whole milk,yogurt} 0.01555669 153 ## 320 {tropical fruit,whole milk,yogurt} 0.01514997 149 También podemos ver qué itemsets incluyen algún producto particular, por ejemplo ap_berries &lt;- subset(ap, items %pin% &quot;berries&quot;) length(ap_berries) ## [1] 4 sort(ap_berries, by =&quot;support&quot;) |&gt; head() |&gt; DATAFRAME() ## items support count ## 48 {berries} 0.03324860 327 ## 99 {berries,whole milk} 0.01179461 116 ## 97 {berries,yogurt} 0.01057448 104 ## 98 {berries,other vegetables} 0.01026945 101 ap_soda &lt;- subset(ap, items %pin% &quot;soda&quot;) length(ap_soda) ## [1] 28 sort(ap_soda, by =&quot;support&quot;) |&gt; head() |&gt; DATAFRAME() ## items support count ## 84 {soda} 0.17437722 1715 ## 295 {soda,whole milk} 0.04006101 394 ## 293 {rolls/buns,soda} 0.03833249 377 ## 294 {other vegetables,soda} 0.03274021 322 ## 276 {bottled water,soda} 0.02897814 285 ## 292 {soda,yogurt} 0.02735130 269 Observaciones: Si hay \\(m\\) artículos, entonces el número de posibles itemsets es de \\(2^m -1\\). Este es un número típicamente muy grande. En nuestro ejemplo, existen unos \\(7\\times 10^{50}\\) posibles itemsets. El número de itemsets de un tamaño fijo, por ejemplo \\(k=5\\), también puede ser muy grande ( \\(169 \\choose 5\\) es del orden de mil millones). Si existe un gran número canastas, contar todas las posibles subcanastas que ocurren es poco factible si lo hacemos por fuerza bruta: requeríamos usar tablas en disco que son relativamente lentas, y quizá no podremos mantener en memoria todos los conteos. Sin embargo, en el ejemplo de arriba encontramos solamente 333 itemsets frecuentes: este número es relativamente chico comparado con el número de posibles itemsets. Esto nos da indicios que contando de una manera apropiada puede ser posible encontrar todos los itemsets frecuentes de cualquier orden. Ejemplo Si reducimos el soporte a \\(0.0001\\) (que implica prácticamente que queremos contar todos los itemsets que ocurren), obtenemos: pars_2 &lt;- list(supp = 0.0001, target=&quot;frequent itemsets&quot;, maxtime = 0, maxlen = 6) ap_todos &lt;- apriori(lista_mb, parameter = pars_2) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## NA 0.1 1 none FALSE TRUE 0 1e-04 1 ## maxlen target ext ## 6 frequent itemsets TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 0 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s]. ## sorting and recoding items ... [169 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 ## Warning in apriori(lista_mb, parameter = pars_2): Mining stopped (maxlen ## reached). Only patterns up to a length of 6 returned! ## done [1.59s]. ## sorting transactions ... done [0.00s]. ## writing ... [10945131 set(s)] done [1.72s]. ## creating S4 object ... done [2.52s]. Entonces el número de itemsets que obtenemos (longitud menor o igual a \\(5\\)) es length(ap_todos) ## [1] 10945131 que es órdenes de magnitud más grande que el conjunto de todas las transacciones. En este ejemplo chico, el cálculo de esta colección (hasta canastas de tamaño 6) puede requierir menos de unos 2Gb de memoria (8Gb pueden no son ser suficientes para encontrar los de tamaño \\(8\\), \\(9\\) y \\(10\\)). Puedes ver entonces que para conjuntos de transacciones masivos, contar todos los itemsets generalmente será un proceso muy lento si no es que más bien infactible. 2.3 Monotonicidad de conjuntos frecuentes Consideramos el problema de encontrar los conjuntos frecuentes. Como discutimos arriba en las características de los datos de canastas, suponemos que La lista de transacciones es muy grande, y no puede leerse completa en memoria, Sin embargo, para una sola canasta, es posible calcular de manera relativamente rápida todos los subconjuntos de tamaño \\(k\\) (para \\(k=1,2,3,4\\), por ejemplo). Por ejemplo, si una canasta tiene \\(10\\) artículos, hay \\(\\binom{10}{3}\\) subcanastas de tamaño 3, \\(\\binom{10}{3} = 210\\). Calcular estos subconjuntos es relativamente rápido comparado con leer de disco una transacción. Finalmente, suponemos que el número de itemsets frecuentes es relativamente chico, debido a que el número de articulos que son más frecuentes es relativamente bajo (lo cual también depende de que escojamos un soporte suficientemente alto). Bajo estas características, el principio básico que hace posible hacer los conteos de itemsets frecuentes es el siguiente: Monotonicidad de itemsets Sea \\(s\\) un nivel de soporte mínimo fijo. Si un itemset \\(I\\) es frecuente, entonces todos sus subconjuntos son itemsets frecuentes. Equivalentemente, si algún subconjunto de un itemset no es frecuente, entonces el itemset no puede ser frecuente. Así que a priori, no es necesario examinar o contar itemsets que contienen al menos un subconjunto que no sea frecuente. Este hecho, junto con la selección de un soporte mínimo para los itemsets frecuentes, es el que hace que la búsqueda y conteo de itemsets frecuentes sea un problema factible, pues podemos descartar una gran cantidad de artículos o itemsets a priori de manera simple, y no es necesario contar todo. La demostración es como sigue: Sea \\(n(I)\\) el número de canastas que contiene \\(I\\), y supongamos que \\(\\tfrac{n(I)}{n}&gt;s\\) (\\(I\\) es un conjunto frecuente). Sea ahora \\(J\\subset I\\). Entonces cualquier canasta que contiene los artículos de \\(I\\) contiene también los artículos de \\(J\\) (que son menos), de forma que \\(n(J)\\geq n(I)\\). Como \\(\\tfrac{n(I)}{n}&gt;s\\), entonces \\(J\\) es un conjunto frecuente. Ejemplo En nuestro ejemplo anterior, el número total de itemsets de tamaño \\(2\\) es length(subset(ap_todos, size(ap_todos) == 2)) ## [1] 9636 Comparamos con los pares frecuentes cuando el soporte es \\(1\\%\\): length(subset(ap, size(ap) == 2)) ## [1] 213 una diferencia de casi dos ordenes de magnitud. 2.4 Algoritmo a-priori Para entender cómo aplicamos monotonicidad, consideremos cómo calcularíamos los pares frecuentes. Primero calculamos los artículos frecuentes (itemsets de tamaño \\(1\\)), que son los artículos que aparecen en al menos una proporción \\(s\\) de las canastas. (Contar candidatos) Esto requiere recorrer el archivo de transacciones y contar todos los artículos. (Podar) Examinamos los conteos y seleccionamos aquellos artículos que son frecuentes. Por el principio de monotonicidad, ningún par frecuente puede contener un artículo no frecuente. Así que para calcular pares: (Contar candidatos) Recorremos el archivo de transacciones. Para cada transacción, solo contamos pares candidatos cuyos dos artículos son artículos frecuentes (del paso anterior) (Podar) Examinamos los conteos y seleccionamos aquellos pares que son frecuentes. Nótese que este algoritmo requiere dos pasadas sobre el conjunto de transacciones. Ejercicio Aplica este algoritmo para las canastas {1,2,3}, {1,8}, {2,4}, {2,3,6,7,8}, {2,3,8}, {1,7,8}, {1,2,3,5}, {2,3}. (soporte &gt; 0.3) Observaciones En este algoritmo, no es necesario leer todas las transacciones a la vez, podemos procesarlas por bloques, por ejemplo. Usamos una pasada de los datos para cada tamaño de itemset frecuente. En la primera pasada del algoritmo (artículos frecuentes), típicamente no es un problema mantener el conteo de todos los artículos en memoria (hay relativamente pocos artículos). Si en la segunda pasada no usáramos monotonicidad, tendríamos que mantener conteos de todos los posibles pares (que son del orden \\(m^2\\), donde \\(m\\) es el número de artículos). Mantener este conteo en memoria podría ser difícil si el número de artículos es grande. Sin embargo, el número de artículos frecuentes generalmente es considerablemente menor. Para itemsets de tamaño más grande, el algoritmo original a priori (Agrawal and Srikant 1994) es: Algoritmos a-priori Sea \\(L_1\\) el conjunto de itemsets frecuentes de tamaño 1. Para obtener \\(L_k\\), el conjunto de itemsets frecuentes de tamaño \\(k\\): Sea \\(C_k\\) el conjunto de candidatos de tamaño \\(k\\), construido a partir de \\(L_{k-1}\\) (monotonicidad). Para cada transacción \\(t\\), Calculamos \\(S_t\\), que son los candidatos en \\(C_k\\) que están en \\(t\\). Agregamos 1 a cada conteo de los candidatos en \\(S_t\\). Filtramos los elementos de \\(C_k\\) que tengan conteo mayor que el soporte definido para obtener \\(L_k\\) Seguimos hasta que encontramos que algún \\(L_k\\) es vacío (no hay itemsets frecuente), o para alguna \\(k\\) fija. Observaciones: Este algoritmo se puede implementar de distintas maneras, por ejemplo: Hay distintas maneras de generar el conjunto \\(C_{k}\\) de candidatos. El paper original sugiere (suponiendo que los artículos siempre están ordenados en los itemsets) hacer un join de \\(L_{k-1}\\) consigo misma. Por ejemplo, para generar los tríos candidatos \\(C_3\\) a partir de \\(L_2\\) hacemos SELECT A.item1, A.item2, B.item2 FROM L2 AS A, L2 AS B WHERE A.item1 = B.item1, A.item2 &lt; B.item2 donde los itemsets estén ordenados en las canastas (por índice o lexicográficamente). Hay también distintas maneras de calcular \\(C_t\\) para cada transacción. El paper original sugiere una estructura de árbol para encontrar los subconjuntos de \\(t\\) que están en \\(C_k\\), y no es neceasrio calcular \\(C_k\\). Ver también (Borgelt 2004). Más detalles de la implementación de los algoritmos (incluyendo algunos más modernos como FPGrowth, que está implementado en spark) se puede encontrar en (Leskovec, Rajaraman, and Ullman 2014) y en (Borgelt 2004). FPGrowth construye una representación eficiente de árbol para los itemsets frecuentes y con esto evita el paso de construcción de candidatos (aunque tiene que mantener en memoria el árbol, que puede ser una estructura grande). 2.5 Modelos simples para análisis de canastas Podemos entender mejor el comportamiento de este análisis con algunos modelos simples para datos de canastas. En primer lugar, pensamos que los datos están en forma de codificación dummy (aunque no usemos esta representación para los datos reales, podemos considerarlo teóricamente). Una canasta es entonces un renglón de ceros y unos, dependendiendo qué artículos están o no en la canasta: \\[X= (X_1,X_2,\\ldots, X_m)\\] donde \\(X_i = 1\\) si el artículo \\(i\\) está en la canasta, y \\(X_i=0\\) si no está. Podríamos pensar entonces en construir modelos para la conjunta de las canastas \\[P(X_1=x_1,X_2=x_2,\\ldots, X_m=x_m)\\] Ejemplo Por ejemplo, si los items son 1-camisa, 2-pantalones, 3-chamarra, podríamos tener las dos transacciones \\(X = (1,0,0)\\), para alguien que solo compró una camisa \\(X = (1,0,1)\\), para alguien que solo compró camisa y chamarra Y podemos inventar una conjunta para todas las canastas, por ejemplo ## # A tibble: 8 × 4 ## p c ch prob ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 ## 2 0 0 1 0.105 ## 3 0 1 0 0.186 ## 4 0 1 1 0.186 ## 5 1 0 0 0.186 ## 6 1 0 1 0.186 ## 7 1 1 0 0.105 ## 8 1 1 1 0.047 A partir de esta conjunta podemos calcular cualquier probabilidad que nos interese. Por ejemplo, la probabilidad de que alguien compre una camisa dado que compró un pantalón es: prob_cp &lt;- filter(probs, p == 1 &amp; c == 1) |&gt; pull(prob) |&gt; sum() prob_p &lt;- filter(probs, p == 1) |&gt; pull(prob) |&gt; sum() (prob_cp / prob_p) |&gt; round(2) ## [1] 0.29 Como discutimos arriba, intentar estimar esta conjunta usando simples conteos de canastas no funciona, pues hay \\(2^n\\) posibles canastas, e incluso cuando \\(n\\) no es tan grande (por ejemplo \\(200\\)) es un número muy grande. Tenemos dos caminos (o una combinación de ellos): podemos hacer supuestos acerca de esta conjunta (y checar si son apropiados), o concentrarnos en estimar solamente algunas de sus características. Más adelante veremos algunas técnicas basadas en modelos (por ejemplo embeddings de productos y canastas) que pueden ser útiles para el problema de encontrar asociaciones entre artículos. Por el momento, en market basket tomamos el segundo camino: estimar sólo algunas cantidades de la conjunta de ocurrencia de artículos. La simplificación de market basket es concentrarnos en algunas marginales que involucren a pocos artículos de esta distribución, que tienen una forma como \\[P(X_{i}=1,X_{j}=1),\\] que es la probabilidad de que el conjunto \\({i,j}\\) aparezca en una canasta dada, o en los términos de market basket, el soporte del itemset \\({i,j}\\). La búsqueda de itemsets frecuentes se traduce entonces en buscar marginales de este tipo que no involucren muchas variables y que tengan valores altos - buscamos modas en las marginales de la distribución de las canastas. Modelo de artículos independientes Por otro lado, si hacemos supuestos acerca de la conjunta es posible ajustar modelos a los datos de canastas. En primer lugar, podemos considerar el modelo simplista que establece que la aparición o no de cada artículo es independiente del resto: \\[P(X_1=x_1,\\ldots, X_m=x_m) =\\prod_m P(X_j=x_j)\\] Y adicionalmente, suponemos que la probabilidad de cada artículo es fija dada por \\[P(X_j=1)=p_j\\]. Este modelo no es realista, pero podemos usarlo para entender algunos aspectos de nuestros algoritmos de conjuntos frecuentes. El soporte (bajo el modelo teórico) de un conjunto de \\(k\\) artículos es \\[P(X_{s_1}=1,X_{s_2}=1,\\ldots, X_{s_k}=1)=p_{s_1}p_{s_2}\\cdots p_{s_k}\\] Podemos ver qué pasa si simulamos transacciones bajo este modelo simple. Primero definimos una función para simular canastas con probabilidades dadas para los artículos simular_transacciones &lt;- function(n_items, n_trans, prob){ etiquetas &lt;- names(prob) canastas &lt;- map(seq(1, n_trans), function(i){ seleccion &lt;- rbinom(n_items, 1, prob = prob) etiquetas[seleccion == 1] }) canastas } Y ahora simulamos usando las proporciones que encontramos en el conjunto Groceries set.seed(1299) probs_items &lt;- itemFrequency(Groceries) |&gt; sort() trans &lt;- simular_transacciones(n_items = 169, n_trans = 10000, prob = probs_items) ap_indep &lt;- apriori(trans, parameter = list(support = 0.005, target = &quot;frequent itemsets&quot;), control = list(verbose = FALSE)) Por ejemplo, aquí vemos algunos pares frecuentes encontrados por el algoritmo: inspect(subset(ap_indep, support &gt; 0.015 &amp; size(ap_indep)==2)) ## items support count ## [1] {beef, whole milk} 0.0151 151 ## [2] {margarine, whole milk} 0.0160 160 ## [3] {butter, whole milk} 0.0159 159 ## [4] {pork, whole milk} 0.0172 172 ## [5] {domestic eggs, whole milk} 0.0165 165 ## [6] {brown bread, whole milk} 0.0186 186 ## [7] {other vegetables, whipped/sour cream} 0.0159 159 ## [8] {whipped/sour cream, whole milk} 0.0177 177 ## [9] {fruit/vegetable juice, whole milk} 0.0182 182 ## [10] {pip fruit, whole milk} 0.0195 195 ## [11] {canned beer, rolls/buns} 0.0158 158 ## [12] {canned beer, other vegetables} 0.0160 160 ## [13] {canned beer, whole milk} 0.0203 203 ## [14] {newspapers, rolls/buns} 0.0150 150 ## [15] {newspapers, other vegetables} 0.0171 171 ## [16] {newspapers, whole milk} 0.0206 206 ## [17] {bottled beer, rolls/buns} 0.0151 151 ## [18] {bottled beer, other vegetables} 0.0158 158 ## [19] {bottled beer, whole milk} 0.0209 209 ## [20] {other vegetables, pastry} 0.0192 192 ## [21] {pastry, whole milk} 0.0216 216 ## [22] {citrus fruit, soda} 0.0151 151 ## [23] {citrus fruit, rolls/buns} 0.0167 167 ## [24] {citrus fruit, other vegetables} 0.0177 177 ## [25] {citrus fruit, whole milk} 0.0240 240 ## [26] {sausage, soda} 0.0179 179 ## [27] {rolls/buns, sausage} 0.0170 170 ## [28] {other vegetables, sausage} 0.0194 194 ## [29] {sausage, whole milk} 0.0250 250 ## [30] {shopping bags, soda} 0.0172 172 ## [31] {rolls/buns, shopping bags} 0.0150 150 ## [32] {other vegetables, shopping bags} 0.0182 182 ## [33] {shopping bags, whole milk} 0.0258 258 ## [34] {soda, tropical fruit} 0.0180 180 ## [35] {rolls/buns, tropical fruit} 0.0193 193 ## [36] {other vegetables, tropical fruit} 0.0209 209 ## [37] {tropical fruit, whole milk} 0.0266 266 ## [38] {root vegetables, soda} 0.0184 184 ## [39] {rolls/buns, root vegetables} 0.0183 183 ## [40] {other vegetables, root vegetables} 0.0191 191 ## [41] {root vegetables, whole milk} 0.0294 294 ## [42] {bottled water, yogurt} 0.0177 177 ## [43] {bottled water, soda} 0.0202 202 ## [44] {bottled water, rolls/buns} 0.0218 218 ## [45] {bottled water, other vegetables} 0.0227 227 ## [46] {bottled water, whole milk} 0.0287 287 ## [47] {soda, yogurt} 0.0228 228 ## [48] {rolls/buns, yogurt} 0.0245 245 ## [49] {other vegetables, yogurt} 0.0258 258 ## [50] {whole milk, yogurt} 0.0334 334 ## [51] {rolls/buns, soda} 0.0345 345 ## [52] {other vegetables, soda} 0.0344 344 ## [53] {soda, whole milk} 0.0452 452 ## [54] {other vegetables, rolls/buns} 0.0342 342 ## [55] {rolls/buns, whole milk} 0.0462 462 ## [56] {other vegetables, whole milk} 0.0507 507 Estos pares frecuentes no se deben a asociaciones entre los artículos, sino a co-ocurrencia en las canastas. Artículos frecuentes apareceran en pares frecuentes, tríos frecuentes, etc. Comparamos por ejemplo el número de reglas encontradas para los datos reales, contra 20 simulaciones de este modelo: # parámetros de apriori pars &lt;- list(supp = 0.005, target=&quot;frequent itemsets&quot;) # producir 10 simulaciones replicaciones &lt;- map(1:20, function(i){ trans &lt;- simular_transacciones(n_items = 169, n_trans = 10000, prob = probs_items) ap_indep &lt;- apriori(trans, parameter = pars, control = list(verbose = FALSE)) conteos &lt;- table(size(ap_indep)) tibble(rep = i, long = names(conteos), n = as.numeric(conteos)) }) reps_tbl &lt;- bind_rows(replicaciones) Ahora vamos a comparar con el análisis de las canastas reales: # calcular observados ap &lt;- apriori(lista_mb, parameter = pars, control = list(verbose = FALSE)) conteos_obs &lt;- table(size(ap)) obs_tbl &lt;- tibble(long = names(conteos_obs), n = as.numeric(conteos_obs)) ggplot(reps_tbl, aes(x = long, y = n)) + geom_jitter(height = 0) + geom_point(data = obs_tbl, colour = &quot;red&quot;, size = 2) + labs(x = &quot;Tamaño&quot;) Y vemos claramente que el modelo simple está lejos de ajustar los datos que observamos en las canastas de Groceries. Hay muchas más combinaciones frecuentes de tamaño \\(2\\) y \\(3\\) de lo que esperaríamos si los artículos se compraran independientemente, y esto indica asociaciones positivas entre artículos que nos gustaría descubrir. Veremos más adelante cómo identificar este tipo de conjuntos frecuentes. Finalmente, comparamos los itemsets de ambos casos: coinciden &lt;- match(ap, ap_indep) ## Warning in match(x@items, table@items, nomatch = nomatch, incomparables = ## incomparables): Item coding not compatible, recoding item matrices first. coinciden[500:505] ## [1] 323 324 325 NA NA NA inspect(ap[500]) ## items support count ## [1] {pork, rolls/buns} 0.01128622 111 inspect(ap_indep[323]) ## items support count ## [1] {pork, rolls/buns} 0.011 110 sum(!is.na(coinciden)) # contar los matches ## [1] 491 length(ap) ## [1] 1001 length(ap_indep) ## [1] 521 Y vemos que en el análisis de datos reales estamos capturando la mayor parte de los itemsets frecuentes del modelo independiente. Estos itemsets se explican por la frecuencia simple de aparición de cada artículo. 2.6 Soporte teórico y conjuntos frecuentes El tamaño de los datos de transacciones está relacionado con los mínimos soportes que tiene sentido analizar. Supongamos por ejemplo que tenemos un conjunto de \\(n\\) transacciones, y buscamos soporte mínimo de \\(s\\). Consideramos entonces que un conjunto \\(I\\) tiene soporte \\(s_I\\) teórico (el que observaríamos con una muestra muy grande), y nuestro interés es ver qué tan bien podemos identificar como frecuentes aquellos conjuntos que satisfagan \\(s_I &gt; s\\). Notamos primero que el valor esperado de ocurrencias \\(n(I)\\) de un conjunto \\(I\\) en los tiene una distribución binomial con número de pruebas \\(n\\) y probabilidad de observar un éxito de \\(s_I\\). Si suponemos que \\(ns_I \\leq 1\\) y \\(n\\) no es muy chico, entonces la probabilidad de observar \\(I\\) 0 veces es aproximadamente tab_1 &lt;- tibble(ns = seq(0.1, 1, 0.2)) |&gt; mutate(prob_no_obs = ppois(0, ns)) tab_1 ## # A tibble: 5 × 2 ## ns prob_no_obs ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.1 0.905 ## 2 0.3 0.741 ## 3 0.5 0.607 ## 4 0.7 0.497 ## 5 0.9 0.407 De forma que con valores de \\(n\\) y \\(s\\) tales que \\(ns &lt;1\\) , típicamente no observaremos una buena parte de todos los conjuntos frecuentes que buscamos encontrar. Supongamos entonces que \\(ns\\geq 5\\), y queremos aproximar la probabilidad de encontrar una canasta \\(I\\) tal que \\(s_I = (1+\\alpha) s\\) (es decir, canastas con soporte de al menos \\(\\alpha\\)% más que el soporte mínimo elegido). Entonces usando la aproximación normal a la binomial, la probabilidad de capturar a \\(I\\) correctamente como frecuente es aproximadamente \\[P \\left (Z &gt; \\frac{\\sqrt{n}(s - s_I)}{\\sqrt{s_I(1-s_I)}} \\right ) = P \\left (Z &gt; \\frac{\\sqrt{n}(s - (1+\\alpha)s)}{\\sqrt{s_I(1-s_I)}} \\right )\\approx P \\left (Z &gt; -\\frac{\\alpha}{\\sqrt{1+\\alpha}}\\sqrt{ns}\\right )\\] crossing(ns = seq(5, 100, 5), alpha = c(0.10, 0.15, 0.20, 0.25)) |&gt; mutate(prob_capturar = map2_dbl(ns, alpha, \\(ns, alpha) pnorm(sqrt(ns) * alpha/sqrt(1-alpha)))) |&gt; ggplot(aes(x = ns, y = prob_capturar, colour = factor(alpha))) + geom_line() + geom_point() + ylab(&quot;Recall/Sensibilidad&quot;) Como ejercicio, puedes calcular la probabilidad de capturar incorrectamente como frecuente un conjunto \\(I\\) con \\(s_I = s/(1+\\alpha)\\) (la tasa de falsos positivos). En general, vemos que tomar \\(ns\\) menor que 10 no necesariamente es buena idea Así que si queremos capturar con probabilidad al menos 90% las canastas cuyo soporte es 20% mayor al soporte mínimo, necesitamos tomar \\(s\\) tal que \\(ns&gt;30\\), por ejemplo. Usualmente queremos hacer el soporte mínimo \\(s\\) un poco más chico, para que capturemos con alta probabilidad aquellos conjuntos de soporte teórico \\(s\\) (aunque esto implica también que capturaremos más conjuntos de menor soporte que el teórico). La elección de punto de corte es más o menos arbitraria (lo cual es una dificultad del análisis de conjuntos frecuentes), pero es importante recordar que si hacemos \\(s\\) mucho más chico que estas recomendaciones, capturaremos por azar muchos conjuntos cuya frecuencia teórica es mucho más baja de lo que esperábamos. Finalmente, recordamos que cuanto más chico tomemos el soporte, más conjuntos frecuentes encontraremos, y el procesamiento puede tomar mucho tiempo. Ejemplo Podemos experimentar con el modelo de independencia: simular_num_reglas &lt;- function(n_trans, prob, pars){ n_items &lt;- length(prob) etiquetas &lt;- 1:n_items if(!is.null(names(prob))) { etiquetas &lt;- names(prob) } trans &lt;- map(1:n_trans, function(i){ etiquetas[which(rbinom(n_items, 1, prob = prob) == 1)] }) ap_indep &lt;- apriori(trans, parameter = pars, control = list(verbose = FALSE)) conteos &lt;- table(size(ap_indep)) tibble(long = names(conteos), n = as.numeric(conteos)) } # fijamos para el ejemplo probs_items &lt;- sort(itemFrequency(Groceries)) pars_1 &lt;- list(support = 0.005, target = &quot;frequent itemsets&quot;) simular_1 &lt;- function(n_trans){ sim_tbl &lt;- simular_num_reglas(n_trans = n_trans, prob = probs_items, pars = pars_1) sim_tbl } set.seed(125) n_trans &lt;- c(10, 25, 50, 100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600, 51200) reps &lt;- tibble(n_trans = rep(n_trans, each = 5)) |&gt; mutate(conteos = map(n_trans, simular_1)) |&gt; mutate(ns = n_trans * 0.005) reps_long_2 &lt;- reps |&gt; unnest(cols = conteos) |&gt; filter(long == 2) ggplot(reps_long_2, aes(x=factor(ns), y = n)) + geom_boxplot() + ylab(&quot;Número de conjuntos frecuentes detectados (k=2)&quot;) + xlab(&quot;Soporte crudo (ns)&quot;) + labs(subtitle = &quot;Soporte min 0.005&quot;) Observaciones Para conjuntos de transacciones chicos no podemos capturar muchos conjuntos frecuentes teóricos porque no observamos suficientes transacciones. Para niveles medios de número de transacciones, podemos obtener resultados ruidosos: muchos subconjuntos que identificamos como frecuentes no lo son realmente (es variación muestral). Cuanto tenemos un número suficientemente alto de transacciones, el número de subconjuntos frecuentes encontrados se estabiliza en el valor teórico verdadero del número de conjuntos frecuentes que existen. 2.7 Reglas de asociación Aunque algunas veces lo único que nos interesa es la co-ocurrencia de artículos (por ejemplo, para entender qué artículos se podrían ver potencialmente afectados por acciones en otros artículos que están en el mismo itemset frecuente), otras veces nos interesa entender qué artículos están asociados a lo largo de canastas por otros factores, como tipo de cliente, tipo de ocasión o propósito (por ejemplo, hora del día, hacer un pastel, promociones, decisiones de organización de estantes), etc. Con este propósito podemos organizar la información de los itemsets frecuentes en términos de reglas de asociación. Un ejemplo de una regla de asociación es: Entre las personas que compran leche y pan, un \\(40\\%\\) compra también yogurt Una regla de asociación es una relación de la forma \\(I\\to j\\), donde \\(I\\) es un conjunto de artículos (el antecedente) y \\(j\\) es un artículo (el consecuente). Definimos la confianza de esta regla como \\[\\hat{P}(I\\to j) = \\hat{P}(j|I) = \\frac{n(I\\cup {j})}{n(I)} = \\hat{P}(I\\cup j)/\\hat{P}(I), \\] es decir, la proporción de canastas que incluyen al itemset \\(I\\cup {j}\\) entre las canastas que incluyen al itemset \\(I\\). La confianza siempre está entre 0 y 1. Observaciones: Por monotonicidad, si \\(J\\) es un conjunto de artículos más grande que \\(I\\) (es decir \\(I\\subset J\\)), entonces \\(n(J) \\leq n(I)\\): cualquier canasta que contiene a \\(J\\) también contiene a \\(I\\), y puede haber algunas canastas que contienen a \\(I\\) no contienen a \\(J\\). Bajo el modelo de items independientes, todas las confianzas satisfacen \\(P(I\\to j)=P(j)\\) (la confianza simplemente es la probabilidad de observar el artículo \\(j\\), independientemente del antecedente). Confianza alta no necesariamente significa una asociación de los items: si el consecuente \\(j\\) tiene soporte alto, entonces podemos obtener confianza alta aunque no haya asociación. Ejemplo En nuestro ejemplo anterior, el soporte de {whole milk,yogurt} es de \\(0.0560\\), el soporte de {whole milk} es \\(0.2555\\), así que la confianza de la regla \\(whole milk \\to yogurt\\) es \\(\\frac{0.0560}{0.2555}=\\) 0.22 Podemos usar la confianza para filtrar reglas que tienen alta probabilidad de cumplirse: Ejemplo pars &lt;- list(supp = 0.01, confidence = 0.20, target=&quot;rules&quot;, ext = TRUE, minlen = 2) reglas &lt;- apriori(lista_mb, parameter = pars) Podemos examinar algunas de las reglas: inspect(reglas[1:10,]) ## lhs rhs support confidence coverage ## [1] {hard cheese} =&gt; {whole milk} 0.01006609 0.4107884 0.02450432 ## [2] {butter milk} =&gt; {other vegetables} 0.01037112 0.3709091 0.02796136 ## [3] {butter milk} =&gt; {whole milk} 0.01159126 0.4145455 0.02796136 ## [4] {ham} =&gt; {whole milk} 0.01148958 0.4414062 0.02602949 ## [5] {sliced cheese} =&gt; {whole milk} 0.01077783 0.4398340 0.02450432 ## [6] {oil} =&gt; {whole milk} 0.01128622 0.4021739 0.02806304 ## [7] {onions} =&gt; {other vegetables} 0.01423488 0.4590164 0.03101169 ## [8] {onions} =&gt; {whole milk} 0.01209964 0.3901639 0.03101169 ## [9] {berries} =&gt; {yogurt} 0.01057448 0.3180428 0.03324860 ## [10] {berries} =&gt; {other vegetables} 0.01026945 0.3088685 0.03324860 ## lift count ## [1] 1.607682 99 ## [2] 1.916916 102 ## [3] 1.622385 114 ## [4] 1.727509 113 ## [5] 1.721356 106 ## [6] 1.573968 111 ## [7] 2.372268 140 ## [8] 1.526965 119 ## [9] 2.279848 104 ## [10] 1.596280 101 En la siguiente tabla, coverage es el soporte del antecedente (lhs = left hand side). Agregamos también el error estándar de la estimación de confidence (que es una proporción basada en el número de veces que se observa el antecedente): df_1 &lt;- sort(reglas, by = &quot;confidence&quot;) |&gt; DATAFRAME() df_2 &lt;- df_1 |&gt; select(LHS, RHS, coverage, confidence, support) |&gt; head(100) |&gt; mutate(lhs.base = num_canastas * coverage) |&gt; mutate(conf.ee = sqrt(confidence * (1 - confidence) / lhs.base)) |&gt; mutate_if(is.numeric, ~ round(.x, 2)) DT::datatable(df_2 |&gt; select(-lhs.base)) Observaciones: Nota que estas tres cantidades están ligadas en cada canasta por \\(coverage\\times confidence = support\\). Usa un argumento de probabilidad condicional para mostrarlo. Muchas de las reglas con confianza alta tienen como consecuente un artículo de soporte alto (por ejemplo, whole milk), como explicamos arriba. Nótese también que las reglas con confianza más alta tienden a tener soporte bajo. Esto lo discutiremos más adelante. Ejercicio Para un mismo consecuente (por ejemplo whole milk), examina cómo varían los valores de confidence. ¿A qué crees que se deba esto? Es natural que artículos frecuentes ocurran en muchas canastas juntas, es decir, que reglas formadas con ellas tengan confianza relativamente alta. Por ejemplo, la regla pan -&gt; verduras podría tener confianza y soporte alto, pero esto no indica ninguna asociación especial entre estos artículos. La razón puede ser que verduras es un artículo muy común. Podemos refinar las reglas de asociación considerando qué tan diferente es \\(P(j|I)\\) de \\(P(j)\\). La primera cantidad es la probabilidad de observar el item \\(j\\) bajo la información de que la canasta contiene a \\(I\\). Si esta cantidad no es muy diferente a \\(P(j)\\), entonces consideramos que esta regla no tiene mucho interés. El lift o intéres de una regla \\(I\\to j\\) se define como \\[L(I\\to j) = \\frac{\\hat{P}({j}|I)}{\\hat{P}({j})},\\] es decir, la confianza de la regla \\(I\\to j\\) dividida entre la proporción de canastas que contienen \\(j\\). En nuestro ejemplo, veamos dos reglas con interés muy distinto: df_1 &lt;- arrange(df_1, desc(lift)) df_1[c(4, nrow(df_1)),] |&gt; select(LHS, RHS, coverage, confidence, lift) ## LHS RHS coverage confidence lift ## 4 {beef} {root vegetables} 0.05246568 0.3313953 3.0403668 ## 231 {soda} {whole milk} 0.17437722 0.2297376 0.8991124 La primera regla tiene un interés mucho más alto que la segunda, lo que indica una asociación más importante entre los dos artículos. Observaciones Cuando decimos que un grupo de artículos están asociados, generalmente estamos indicando que forma alguna regla de asociación con lift alto. En principio también podría haber reglas con lift muy por debajo de uno, y eso también indica una asociación (por ejemplo coca y pepsi). Pero el método de itemsets frecuentes no es muy apropiado para buscar estas reglas, pues precisamente esas reglas tienden a tener soporte y confianza bajas. El valor del lift también puede escribirse (deméstralo) como \\[ \\frac{\\hat{P}(I\\cup\\{j\\})}{\\hat{P}(I)\\hat{P}({j})},\\] Cuando los artículos son independientes, esta cantidad está cercana a \\(1\\). Es una medida de qué tan lejos de independencia están la ocurrencia de los itemsets \\(I\\) y \\(j\\). Ejemplo df_1 &lt;- sort(reglas, by = &quot;lift&quot;) |&gt; DATAFRAME() En esta tabla, coverage es el soporte del antecedente (lhs = left hand side): df_2 &lt;- df_1 |&gt; select(LHS, RHS, coverage, lift, confidence, support) |&gt; head(100) |&gt; mutate_if(is.numeric, ~ round(.x, 2)) DT::datatable(df_2) Las reglas de asociación se calculan comenzando por calcular los itemsets frecuentes según el algoritmo a priori explicado arriba. Para encontrar las reglas de asociación hacemos: Para cada itemset frecuente \\(f\\), tomamos como candidatos a consecuentes los artículos \\(i\\) de \\(f\\) Si la confianza \\(\\frac{\\hat{P}(I)}{\\hat{P}(I-\\{j\\})}\\) es mayor que la confianza mínima, agregamos la regla de asociación \\(I\\to j\\). Con este proceso encontramos todas las reglas \\(I\\to j\\) tales que \\(I\\cup\\{j\\}\\) es un itemset frecuente. 2.8 Dificultades en el análisis de canastas El análisis de canastas es un método rápido y simple que nos da varias maneras de explorar las relaciones entre los artículos. Sin embargo, hay varias dificultades en su aplicación. Número de reglas y itemsets Muchas veces encontramos un número muy grande de itemsets o reglas. Hay varias maneras de filtrar estas reglas según el propósito. Si filtramos mucho, perdemos reglas que pueden ser interesantes. Si filtramos poco, es difícil entender globalmente los resultados del análisis. Un punto de vista es producir una cantidad de reglas para procesar posteriormente con queries: por ejemplo, si nos interesa entender las relaciones de berries con otros artículos, podemos filtrar las reglas encontradas y examinarlas más fácilmente. Cortes estrictos en el filtrado Cuando seleccionamos valores mínimos de soporte, confianza y/o lift, estas decisiones son más o menos arbitrarias. Distintos analistas pueden llegar a resultados distintos, incluso cuando el propósito del análisis sea similar, y en ocasiones hay que iterar el análisis para encontrar valores adecuados que den conjuntos razonables con resultados interesantes. Este último concepto es subjetivo. Redundancia de reglas Existe alguna redundancia en las reglas que encontramos. Por ejemplo, podríamos tener {yogurt, berries} -&gt; {whipped cream}, pero también {yogurt} -&gt; {whipped cream}. Este traslape de reglas hace también difícil entender conjuntos grandes de reglas. Variabilidad de medidas de calidad Un problema del análisis clásico de soporte-confianza-lift es la variabilidad de las estimaciones de confianza y lift. Cuando comenzamos poniendo valores de soporte y confianza relativamente bajos, encontramos muchas reglas. Muchas de estas reglas son ruidosas (en un número más grande de transacciones las descalificaríamos). Intentamos muchas veces filtrar u ordenar por lift, para considerar las reglas más interesantes Sin embargo, encontramos entonces que muchas reglas de lift y/o confianza altas son aquellas que tienen soporte bajo y consecuentes poco frecuentes. Como veremos más adelante, esto se debe muchas veces a error de estimación. Los valores más grandes de lift generalmente son sobreestimaciones, por la naturaleza del análisis basado en cortes. Si regresamos a incrementar soporte y confianza, potencialmente perdemos reglas interesantes. Veamos cómo se comportan confianza y lift para el modelo donde no hay asociaciones. Utilizamos el modelo de independencia que explicamos arriba. Obsérvese que en este modelo todas las confianzas teóricas son iguales a la frecuencia del consecuente, y todos los valores teóricos de lift son \\(1\\): pars &lt;- list(support=0.002, confidence = 0.0, target=&quot;rules&quot;, ext = TRUE, minlen = 2) sims_reglas &lt;- map(1:10, function(i){ trans &lt;- simular_transacciones(n_items = 169, n_trans = 10000, prob = probs_items) ap_random &lt;- apriori(trans, parameter = pars, control = list(verbose = FALSE)) ap_random }) Y notamos que conforme el soporte de la regla es más bajo, hay más variabilidad en las estimaciones del confianza y lift. En este caso utilizamos plot(subset(sims_reglas[[4]], rhs %pin% &quot;whole milk&quot;), measure=c(&quot;support&quot;,&quot;confidence&quot;), shading = &quot;lift&quot;, engine = &quot;plotly&quot;) ## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter. plot(subset(sims_reglas[[4]], rhs %pin% &quot;whole milk&quot;), measure=c(&quot;support&quot;,&quot;lift&quot;), shading = &quot;confidence&quot;, engine = &quot;plotly&quot;) ## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter. El valor de confianza y de lift puede ser altamente variable para reglas con soporte bajo. Podemos tomar dos caminos: Cuando hagamos el soporte más bajo, incrementamos el valor de lift mínimo. Esto evita que obtengamos demasiadas reglas que no representan interacciones reales entre los artículos. Podemos usar otras medidas que tomen en cuenta la variabilidad de las estimaciones. Por ejemplo, hyper-lift y hyper-confidence están basados en modelos simples (como el que vimos arriba), que filtran aquellos valores de calidad que están en las colas de las distribuciones de los modelos simples. 2.9 Otras medidas de calidad de reglas Hay una gran cantidad de medidas de interés de reglas que se han propuesto desde que se usa el análisis de canasta. Aquí discutimos hyper-lift y hyper-confidence, que toman en cuenta el soporte de las reglas para proponer puntos de corte (Hahsler and Hornik 2008). Explicamos aquí el hyper-lift para una regla \\(i\\to j\\). Consideramos el modelo de independencia (lo pensamos como el modelo nulo), fijando las probabilidades de ocurrencia de los artículos según los datos (como hicimos en los ejemplos de arriba) y el número de transacciones. Bajo este modelo, el número de ocurrencias \\(X_{\\{i,j\\}}\\) de el itemset \\(\\{i,j\\}\\) es una variable aleatoria con distribución conocida (binomial). Esta distribución representa la variación que podemos observar en los conteos de \\(\\{i,j\\}\\) bajo distintas muestras de transacciones del mismo tamaño. La idea básica del hyperlift es comparar el conteo \\(n(\\{i,j\\})\\) con la cola superior de la distribución de \\(X_{i,j}\\) bajo el supuesto de independencia, poniendo \\[HL(I\\to j) = \\frac{n(\\{I,j\\})}{Q_\\delta (X_{I,j})},\\] donde \\(Q_\\delta\\) es tal que \\(P(X_{I,j} &lt; Q_\\delta (X_{I,j}))\\approx \\delta\\). Tomamos por ejemplo \\(\\delta=0.99\\). De esta forma, \\(HL&gt;1\\) sólo cuando el conteo observado \\(n(\\{i,j\\})\\) está en la cola superior del conteo bajo la hipótesis nula de independencia. Esto toma en cuenta la variabilidad de los conteos (que es grande en términos relativos cuando el soporte es bajo). Observaciones: El modelo de independencia que se usa en el paquete arules es una variación del que vimos aquí, ver los detalles en (Hahsler and Hornik 2008). Los valores de hyper-lift no son realmente comparables a los de lift, son dos medidas de calidad de asociación diferentes, pero similares en cuanto a lo que quieren capturar. Hyper-lift bajo hipótesis de independencia Veamos cómo se comporta el hyper-lift simulando datos con el modelo de independencia: trans &lt;- simular_transacciones(n_items = 169, n_trans = 10000, prob = probs_items) ap_random &lt;- apriori(trans, parameter = list(support=0.001, confidence = 0.10, target=&quot;rules&quot;, ext = TRUE, minlen = 2), control = list(verbose = FALSE)) agregar_hyperlift &lt;- function(reglas, trans){ quality(reglas) &lt;- cbind(quality(reglas), hyper_lift = interestMeasure(reglas, measure = &quot;hyperLift&quot;, transactions = trans)) reglas } ap_random &lt;- agregar_hyperlift(ap_random, trans) Vemos claramente que la gran mayoría de reglas obtenidas ahora tienen hyper-lift menor que uno plot(ap_random, measure=c(&quot;lift&quot;,&quot;hyper_lift&quot;), shading = &quot;support&quot;) ## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter. Cortando en un valor relativamente bajo de hyper-lift, vemos que nos deshacemos correctamente de casi todas las reglas: length(ap_random) ## [1] 3591 length(subset(ap_random, lift &gt; 1)) ## [1] 2643 length(subset(ap_random, hyper_lift &gt; 1)) ## [1] 130 Hyper-lift para datos de canastas Ahora aplicamos a los datos reales pars &lt;- list(supp = 0.002, confidence = 0.10, target=&quot;rules&quot;, ext = TRUE, minlen = 2) reglas &lt;- apriori(lista_mb, parameter = pars, control = list(verbose=FALSE)) reglas &lt;- agregar_hyperlift(reglas, Groceries) length(reglas) ## [1] 8332 Vemos que podemos cortar niveles de hyper-lift donde obtenemos reglas de soporte relativamente alto. plot(reglas, measure=c(&quot;lift&quot;,&quot;hyper_lift&quot;), shading = &quot;support&quot;) ## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter. Si cortamos en valores que dan un número similar de reglas, por ejemplo: filtradas_hl &lt;- subset(reglas, hyper_lift &gt; 2) filtradas_lift &lt;- subset(reglas, lift &gt; 3.7) length(filtradas_hl) ## [1] 439 length(filtradas_lift) ## [1] 480 Vemos que las reglas cortadas con hyper-lift tienen mejores valores de soporte: library(patchwork) soportes &lt;- qqplot( quality(filtradas_hl)$support, quality(filtradas_lift)$support, plot.it = FALSE) |&gt; as_tibble() g_soporte &lt;- ggplot(soportes, aes(x, y)) + geom_abline(colour = &quot;red&quot;) + geom_point(alpha = 0.5) + xlab(&quot;Soporte (filtro Hyperlift)&quot;) + ylab(&quot;Soporte (filtro Lift)&quot;) + coord_fixed(xlim = c(0, 0.03), ylim = c(0, 0.03)) + ggtitle(&quot;Soporte&quot;) soportes &lt;- qqplot( quality(filtradas_hl)$lift, quality(filtradas_lift)$lift, plot.it = FALSE) |&gt; as_tibble() g_lift &lt;- ggplot(soportes, aes(x, y)) + geom_abline(colour = &quot;red&quot;) + geom_point(alpha = 0.5) + xlab(&quot;Lift (filtro Hyperlift)&quot;) + ylab(&quot;Lift (filtro Lift)&quot;) + coord_fixed() + ggtitle(&quot;Lift&quot;) g_soporte + g_lift + plot_annotation( subtitle = &quot;Soporte y Lift con filtro de reglas por lift vs. hyperlift&quot;) La distribución de valores de lift no es tan diferente, de forma que esta medida de calidad no se degrada en el conjunto de reglas que encontramos: En resumen, al utilizar hyper-lift para filtrar reglas en lugar de lift obtenemos reglas de mejor calidad: Descartamos más reglas de soporte bajo que tienen lift alto por azar. Los valores de soporte de las reglas tienden a ser más altos. Los valores de lift son comparables 2.10 Selección de reglas Ahora discutiremos cómo seleccionar itemsets frecuentes y reglas. Filtrar con todos estos criterios (soporte, confianza, soporte del antecedente, lift) no es simple, y depende de los objetivos del análisis. Recordemos también que estos análisis están basados justamente en cortes “duros” de los datos, más o menos arbitrarios, y por lo tanto pueden los resultados son variables. Hay varias maneras de conducir el análisis. Dos tipos útiles son: Itemsets de alta frecuencia: en este enfoque buscamos reglas con soporte y confianza relativamente altos. Generalmente están asociados a productos muy frecuentes, y nos indica potencial de interacción entre los artículos. Este análisis es más una reexpresión de la información contenida en los itemsets frecuentes. En este caso, podemos filtrar con soporte alto, para evitar estimaciones ruidosas (por ejemplo, soporte mínimo de 300 canastas). Interacciones altas: en este enfoque donde buscamos entender nichos. Consideramos valores de soporte y confianza más bajos, pero con valores de lift/hyper-lift alto. Este análisis es más útil para entender, por ejemplo, propósitos de compras, convivencia de artículos, tipos de comprador, etc. Colección de reglas para hacer querys: la colección de reglas puede ser más grande, e incluir por ejemplo resultados de distintas corridas de market basket (incluyendo los dos enfoques de arriba). Las reglas se examinan seleccionando antecedentes o consecuentes, valores altos de soporte, etc, según la pregunta particular que se quiera explorar. Ejemplo: canastas grandes Para entender las canastas grandes, podemos variar valores de soporte y confianza para encontrar un número manejable de reglas. pars &lt;- list(support = 0.02, confidence = 0.20, minlen = 2, target=&quot;rules&quot;, ext = TRUE) reglas_1 &lt;- apriori(lista_mb, parameter = pars) Esta elección de parámetros resulta en 72. Podemos ordenar por hyper-lift: plot(reglas_1, colors=c(&quot;red&quot;, &quot;gray&quot;), engine = &quot;plotly&quot;) reglas_1 &lt;- agregar_hyperlift(reglas_1, lista_mb) DT::datatable(DATAFRAME(sort(reglas_1, by = &quot;hyper_lift&quot;)) |&gt; select(-count, -coverage) |&gt; mutate_if(is.numeric, ~ round(.x, 2))) Observaciones: conforme bajamos en esta tabla (ordenada por soporte), las estimaciones de confianza y lift son menos precisas. 2.11 Búsqueda de reglas especializadas Otra manera de usar este análisis es intenando buscar asociaciones más fuertes (lift o hyper-lift más alto), aún cuando sacrificamos soporte. Por su naturaleza, este tipo de análisis puede resultar en reglas más ruidosas (malas estimaciones de confianza y lift), pero es posible filtrar valores más altos de estas cantidades para encontrar reglas útiles. Comenzamos con un soporte y confianza más bajas pars &lt;- list(support = 0.001, confidence = 0.1, minlen = 2, target=&quot;rules&quot;, ext = TRUE) b_reglas &lt;- apriori(lista_mb, parameter = pars) b_reglas &lt;- agregar_hyperlift(b_reglas, lista_mb) Y ahora filtramos con valores más grandes de hyper-lift. Podemos filtrar adicionalmente con coverage para obtener reglas que aplican con más frecuencia: b_reglas ## set of 32783 rules b_reglas_lift &lt;- subset(b_reglas, hyper_lift &gt; 2.5 &amp; size(b_reglas) &lt; 4 &amp; coverage &gt; 0.01) b_reglas_lift &lt;- sort(b_reglas_lift, by = &quot;hyper_lift&quot;) DT::datatable(DATAFRAME(b_reglas_lift) |&gt; select(-count, -coverage) |&gt; mutate_if(is.numeric, ~ round(.x, 3))) 2.12 Visualización de asociaciones Tener una visión amplia del market basket analysis es difícil (típicamente, funciona mejor como un resultado al que se le hacen querys, o uno donde filtramos cuidadosamente algunas reglas que puedan ser útiles). Así que muchas veces ayuda visualizar los pares con asociación alta: Construimos todas las reglas con un antecedente y un consecuente. Filtramos las reglas con hyper-lift relativamente alto (por ejemplo &gt; \\(1.5\\), pero hay que experimentar). Representamos como una gráfica donde los nodos son artículos, y las aristas son relaciones de lift alto. Usamos algún algoritmo para representar gráficas basado en fuerza, usando como peso el lift. 2.12.1 Ejemplo En nuestro caso, podríamos tomar (ajustando parámetros para no obtener demasiadas reglas o demasiado pocas) b_reglas_lift &lt;- subset(b_reglas, hyper_lift &gt; 1.75 &amp; confidence &gt; 0.05) reglas_f &lt;- subset(b_reglas_lift, size(b_reglas_lift) == 2) library(tidygraph) library(ggraph) df_reglas &lt;- reglas_f |&gt; DATAFRAME() |&gt; rename(from = LHS, to = RHS) df_reglas$weight &lt;- log(df_reglas$hyper_lift) nrow(df_reglas) ## [1] 94 graph_1 &lt;- as_tbl_graph(df_reglas) |&gt; mutate(centrality = centrality_degree(mode = &quot;all&quot;)) ggraph(graph_1, layout = &quot;fr&quot;, start.temp=100) + geom_edge_link(aes(alpha=lift), colour = &quot;red&quot;, arrow = arrow(length = unit(4, &quot;mm&quot;))) + geom_node_point(aes(size = centrality, colour = centrality)) + geom_node_text(aes(label = name), size=4, colour = &quot;gray20&quot;, repel=TRUE) + theme_graph(base_family = &quot;sans&quot;) Para gráficas más grandes, es mejor usar software especializado para investigar las redes que obtenemos (como Gephi): b_reglas_lift &lt;- subset(b_reglas, hyper_lift &gt; 1.5 &amp; coverage &gt; 0.01) reglas_f &lt;- subset(b_reglas_lift, size(b_reglas_lift) == 2) length(reglas_f) ## [1] 251 reglas_f |&gt; DATAFRAME() |&gt; rename(source = LHS, target = RHS) |&gt; select(-count) |&gt; write_csv(file = &quot;./salidas/reglas.csv&quot;) Para el análisis de canastas grandes: reglas_f2 &lt;- subset(reglas_1, hyper_lift &gt; 1.3, confidence &gt; 0.4) df_reglas &lt;- reglas_f2 |&gt; DATAFRAME() |&gt; rename(from = LHS, to = RHS) df_reglas$weight &lt;- log(df_reglas$hyper_lift) graph_1 &lt;- as_tbl_graph(df_reglas) |&gt; mutate(centrality = centrality_degree(mode = &quot;all&quot;)) ggraph(graph_1, layout = &quot;fr&quot;, start.temp=100) + geom_edge_link(aes(alpha=hyper_lift), colour = &quot;red&quot;, arrow = arrow(length = unit(4, &quot;mm&quot;))) + geom_node_point(aes(size = centrality, colour = centrality)) + geom_node_text(aes(label = name), size=4, colour = &quot;gray20&quot;, repel=TRUE) + theme_graph(base_family = &quot;sans&quot;) 2.13 Otras aplicaciones Análisis de tablas de variables categóricas: podemos considerar una tabla con varias variables categóricas. Una canasta son los valores que toman las variables. Por ejemplo, podríamos encontrar reglas como {hogar = propio, ocupación=profesional} -&gt; ingreso = alto. Puedes ver más de este análisis en (Hastie, Tibshirani, and Friedman 2017), por ejemplo, sección 14.2. Conceptos relacionados: si los artículos son palabras y las canastas documentos (tweets, por ejemplo), este tipo de análisis (una vez que eliminamos las palabras más frecuentes, que no tienen significado como artículos, preposiciones, etc.), puede mostrar palabras que co-ocurren para formar conceptos relacionados. Plagiarismo: si los artículos son documentos y los canastas oraciones, el análisis de canastas puede encontrar documentos que contienen las mismas oraciones. Si varias canastas (oraciones) “contienen” los mismos artículos (documentos), entonces esas oraciones son indicadores de plagio 2.14 Ejercicios Considera los datos de datos/recetas. Lee los datos, asegúrate que puedes filtrar por tipo de cocina, y que puedes aplicarles la función apriori de arules (o cualquier otra herramienta que estés utilizando). Calcula la frecuencia de todos los artículos (ingredientes). El resto de este ejercicio lo haremos a principio de la siguiente clase. Acerca de los datos: Cada receta es una canasta, y los artículos son los ingredientes que usan. Puedes consultar el artículo original aquí. Haz algunos experimentos el ejemplo 2.12.1 que vimos en clase: incrementa/decrementa hyperlift, incrementa/decrementa soporte. ¿Qué pasa con las gráficas resultantes y el número de reglas? (Opcional) Muchas veces el análisis de canastas puede hacerse con una muestra de transacciones. Leer secciones 6.4.1 a 6.4.4 de (Leskovec, Rajaraman, and Ullman 2014). Referencias "],["similitud.html", "3 Similitud y vecinos cercanos 3.1 Similitud de conjuntos 3.2 Representación de documentos como conjuntos 3.3 Representación matricial 3.4 Minhash y reducción probabilística de dimensionalidad 3.5 Agrupando textos de similitud alta 3.6 Ejemplo: tweets 3.7 Verificar si un nuevo elemento es duplicado 3.8 Controlando la sensibilidad y umbral de similitud 3.9 Distancia euclideana y LSH 3.10 Locality Sensitive Hashing (LSH) 3.11 LSH para imágenes 3.12 Joins por similitud 3.13 Ejemplo: entity matching", " 3 Similitud y vecinos cercanos En esta parte consideraremos la tarea de agrupar eficientemente elementos muy similares en conjuntos datos masivos. Algunos ejemplos de esta tarea son: Encontrar documentos similares en una colección de documentos. Esto puede servir para detectar plagio, deduplicar noticias o páginas web, hacer matching de datos de dos fuentes (por ejemplo, nombres completos de personas), etc. Ver por ejemplo Google News. Encontrar usuarios similares (Netflix), en el sentido de que tienen gustos similares, o películas similares, en el sentido de qe le gustan a las mismas personas. Encontrar imágenes similares en una colección grande, ver por ejemplo Pinterest. Uber: rutas similares que indican (fraude o abusos)[https://eng.uber.com/lsh/]. Deduplicar registros de usuarios de algún servicio (por ejemplo, beneficiarios de programas sociales). Estos problemas no son triviales por dos razones: Los elementos que queremos comparar muchas veces están naturalmente representados en espacios de dimensión alta, y es relativamente costoso comparar un par (documentos, imágenes, usuarios, rutas). Muchas veces es preferible construir una representación más compacta y hacer comparaciones con las versiones comprimidas. Si la colección de elementos es grande (\\(N\\)), entonces el número de pares posibles es del orden de \\(N^2\\), y es muy costoso hacer todas las posibles comparaciones para encontrar los elementos similares (por ejemplo, comparar \\(100\\) mil documentos, con unas \\(10\\) mil comparaciones por segundo, tardaría alrededor de \\(5\\) días). Si tenemos que calcular todas las similitudes, no hay mucho qué hacer. Pero muchas veces nos interesa encontrar pares de similitud alta, o completar tareas más específicas como contar duplicados, etc. En estos casos, veremos que es posible construir soluciones probabilísticas aproximadas para resolver estos problemas de forma escalable. Aunque veremos más adelante métricas de similitud comunes como la dada por la distancia euclideana o distancia coseno, por ejemplo, en esta primera parte nos concentramos en discutir similitud entre pares de textos. Los textos los podemos ver como colecciones de palabras, o de manera más general, como colecciones de cadenas. 3.1 Similitud de conjuntos Muchos de estos problemas de similitud se pueden pensar como problemas de similitud entre conjuntos. Por ejemplo, los documentos son conjuntos de palabras, conjuntos de pares de palabras, sucesiones de caracteres, una película se puede ver como el conjunto de personas a las que les gustó, o una ruta como un conjunto de tramos, etc. Hay muchas medidas que son útiles para cuantificar la similitud entre conjuntos. Una que es popular, y que explotaremos por sus propiedades, es la similitud de Jaccard: La similitud de Jaccard de los conjuntos \\(A\\) y \\(B\\) está dada por \\[sim(A,B) = \\frac{|A\\cap B|}{|A\\cup B|}\\] Esta medida cuantifica qué tan cerca está la unión de \\(A\\) y \\(B\\) de su intersección. Cuanto más parecidos sean \\(A\\cup B\\) y \\(A\\cap B\\), más similares son los conjuntos. En términos geométricos, es el área de la intersección entre el área de la unión. Ejercicio Calcula la similitud de Jaccard entre los conjuntos \\(A=\\{5,2,34,1,20,3,4\\}\\) y \\(B=\\{19,1,2,5\\}\\) library(tidyverse) options(digits = 3) sim_jaccard &lt;- \\(a, b) length(intersect(a, b)) / length(union(a, b)) sim_jaccard(c(0,1,2,5,8), c(1,2,5,8,9)) ## [1] 0.667 sim_jaccard(c(2,3,5,8,10), c(1,8,9,10)) ## [1] 0.286 sim_jaccard(c(3,2,5), c(8,9,1,10)) ## [1] 0 3.2 Representación de documentos como conjuntos Hay varias maneras de representar documentos como conjuntos. Las más simples son: Los documentos son colecciones de palabras, o conjuntos de sucesiones de palabras de tamaño \\(n\\). Los documentos son colecciones de caracteres, o conjuntos de sucesiones de caracteres (cadenas) de tamaño \\(k\\). La primera representación se llama representación de n-gramas, y la segunda representación de k-tejas, o \\(k\\)-shingles. Nótese que en ambos casos, representaciones de dos documentos con secciones parecidas acomodadas en distintos lugares tienden a ser similares. Consideremos una colección de textos cortos: textos &lt;- c(&quot;el perro persigue al gato pero no lo alcanza&quot;, &quot;el gato persigue al perro, pero no lo alcanza&quot;, &quot;este es el documento de ejemplo&quot;, &quot;este no es el documento de los ejemplos&quot;, &quot;documento más corto&quot;, &quot;otros animales pueden ser mascotas&quot;) Abajo mostramos la representacion en bolsa de palabras (1-gramas) y la representación en bigramas (2-gramas) de los primeros dos documentos: # Bolsa de palabras (1-gramas) tokenizers::tokenize_ngrams(textos[1:2], n = 1) |&gt; map(unique) ## [[1]] ## [1] &quot;el&quot; &quot;perro&quot; &quot;persigue&quot; &quot;al&quot; &quot;gato&quot; &quot;pero&quot; &quot;no&quot; ## [8] &quot;lo&quot; &quot;alcanza&quot; ## ## [[2]] ## [1] &quot;el&quot; &quot;gato&quot; &quot;persigue&quot; &quot;al&quot; &quot;perro&quot; &quot;pero&quot; &quot;no&quot; ## [8] &quot;lo&quot; &quot;alcanza&quot; # bigramas tokenizers::tokenize_ngrams(textos[1:2], n = 2) |&gt; map(unique) ## [[1]] ## [1] &quot;el perro&quot; &quot;perro persigue&quot; &quot;persigue al&quot; &quot;al gato&quot; ## [5] &quot;gato pero&quot; &quot;pero no&quot; &quot;no lo&quot; &quot;lo alcanza&quot; ## ## [[2]] ## [1] &quot;el gato&quot; &quot;gato persigue&quot; &quot;persigue al&quot; &quot;al perro&quot; ## [5] &quot;perro pero&quot; &quot;pero no&quot; &quot;no lo&quot; &quot;lo alcanza&quot; La representación en k-tejas es otra posibilidad: calcular_tejas &lt;- function(x, k = 2){ tokenizers::tokenize_character_shingles(x, n = k, lowercase = FALSE, simplify = TRUE, strip_non_alpha = FALSE) } # 2-tejas calcular_tejas(textos[1:2], k = 2) |&gt; map(unique) ## [[1]] ## [1] &quot;el&quot; &quot;l &quot; &quot; p&quot; &quot;pe&quot; &quot;er&quot; &quot;rr&quot; &quot;ro&quot; &quot;o &quot; &quot;rs&quot; &quot;si&quot; &quot;ig&quot; &quot;gu&quot; &quot;ue&quot; &quot;e &quot; &quot; a&quot; ## [16] &quot;al&quot; &quot; g&quot; &quot;ga&quot; &quot;at&quot; &quot;to&quot; &quot; n&quot; &quot;no&quot; &quot; l&quot; &quot;lo&quot; &quot;lc&quot; &quot;ca&quot; &quot;an&quot; &quot;nz&quot; &quot;za&quot; ## ## [[2]] ## [1] &quot;el&quot; &quot;l &quot; &quot; g&quot; &quot;ga&quot; &quot;at&quot; &quot;to&quot; &quot;o &quot; &quot; p&quot; &quot;pe&quot; &quot;er&quot; &quot;rs&quot; &quot;si&quot; &quot;ig&quot; &quot;gu&quot; &quot;ue&quot; ## [16] &quot;e &quot; &quot; a&quot; &quot;al&quot; &quot;rr&quot; &quot;ro&quot; &quot;o,&quot; &quot;, &quot; &quot; n&quot; &quot;no&quot; &quot; l&quot; &quot;lo&quot; &quot;lc&quot; &quot;ca&quot; &quot;an&quot; &quot;nz&quot; ## [31] &quot;za&quot; # 4-tejas:&quot; calcular_tejas(textos[1:2], k = 4) |&gt; map(unique) ## [[1]] ## [1] &quot;el p&quot; &quot;l pe&quot; &quot; per&quot; &quot;perr&quot; &quot;erro&quot; &quot;rro &quot; &quot;ro p&quot; &quot;o pe&quot; &quot;pers&quot; &quot;ersi&quot; ## [11] &quot;rsig&quot; &quot;sigu&quot; &quot;igue&quot; &quot;gue &quot; &quot;ue a&quot; &quot;e al&quot; &quot; al &quot; &quot;al g&quot; &quot;l ga&quot; &quot; gat&quot; ## [21] &quot;gato&quot; &quot;ato &quot; &quot;to p&quot; &quot;pero&quot; &quot;ero &quot; &quot;ro n&quot; &quot;o no&quot; &quot; no &quot; &quot;no l&quot; &quot;o lo&quot; ## [31] &quot; lo &quot; &quot;lo a&quot; &quot;o al&quot; &quot; alc&quot; &quot;alca&quot; &quot;lcan&quot; &quot;canz&quot; &quot;anza&quot; ## ## [[2]] ## [1] &quot;el g&quot; &quot;l ga&quot; &quot; gat&quot; &quot;gato&quot; &quot;ato &quot; &quot;to p&quot; &quot;o pe&quot; &quot; per&quot; &quot;pers&quot; &quot;ersi&quot; ## [11] &quot;rsig&quot; &quot;sigu&quot; &quot;igue&quot; &quot;gue &quot; &quot;ue a&quot; &quot;e al&quot; &quot; al &quot; &quot;al p&quot; &quot;l pe&quot; &quot;perr&quot; ## [21] &quot;erro&quot; &quot;rro,&quot; &quot;ro, &quot; &quot;o, p&quot; &quot;, pe&quot; &quot;pero&quot; &quot;ero &quot; &quot;ro n&quot; &quot;o no&quot; &quot; no &quot; ## [31] &quot;no l&quot; &quot;o lo&quot; &quot; lo &quot; &quot;lo a&quot; &quot;o al&quot; &quot; alc&quot; &quot;alca&quot; &quot;lcan&quot; &quot;canz&quot; &quot;anza&quot; Observaciones: Los tokens son las unidades básicas de análisis. Los tokens son palabras para los n-gramas (cuya definición no es del todo simple) y caracteres para las k-tejas. Podrían ser también oraciones, por ejemplo. Nótese que en ambos casos es posible hacer algo de preprocesamiento para obtener la representación. Transformaciones usuales son: Eliminar puntuación y/o espacios. Convertir los textos a minúsculas. Esto incluye decisiones acerca de qué hacer con palabras compuestas (por ejemplo, con un guión), palabras que denotan un concepto (Reino Unido, por ejemplo) y otros detalles. Si lo que nos interesa principalmente similitud textual (no significado, o polaridad, etc.) entre documentos, entonces podemos usar \\(k\\)-tejas, con un mínimo de preprocesamiento. Esta representación es simple y flexible en el sentido de que se puede adaptar para documentos muy cortos (mensajes o tweets, por ejemplo), pero también para documentos más grandes. Por estas razones, no concentramos por el momento en \\(k\\)-tejas Tejas (shingles) Sea \\(k&gt;0\\) un entero. Las \\(k\\)-tejas (\\(k\\)-shingles) de un documento d es el conjunto de todas las corridas (distintas) de \\(k\\) caracteres sucesivos. Escogemos \\(k\\) suficientemente grande, de forma que la probabilidad de que una teja particular ocurra en un texto dado sea relativamente baja. Ejemplo Documentos textualmente similares tienen tejas similares: # calcular tejas textos ## [1] &quot;el perro persigue al gato pero no lo alcanza&quot; ## [2] &quot;el gato persigue al perro, pero no lo alcanza&quot; ## [3] &quot;este es el documento de ejemplo&quot; ## [4] &quot;este no es el documento de los ejemplos&quot; ## [5] &quot;documento más corto&quot; ## [6] &quot;otros animales pueden ser mascotas&quot; tejas_doc &lt;- calcular_tejas(textos, k = 4) # calcular similitud de jaccard entre algunos pares sim_jaccard(tejas_doc[[1]], tejas_doc[[2]]) ## [1] 0.773 sim_jaccard(tejas_doc[[1]], tejas_doc[[3]]) ## [1] 0 sim_jaccard(tejas_doc[[4]], tejas_doc[[5]]) ## [1] 0.156 Podemos calcular todas las similitudes: tejas_tbl &lt;- crossing(id_1 = 1:length(textos), id_2 = 1:length(textos)) |&gt; filter(id_1 &lt; id_2) |&gt; mutate(tejas_1 = tejas_doc[id_1], tejas_2 = tejas_doc[id_2]) |&gt; mutate(sim = map2_dbl(tejas_1, tejas_2, ~sim_jaccard(.x, .y))) |&gt; select(id_1, id_2, sim) tejas_tbl ## # A tibble: 15 × 3 ## id_1 id_2 sim ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 2 0.773 ## 2 1 3 0 ## 3 1 4 0.0137 ## 4 1 5 0 ## 5 1 6 0 ## 6 2 3 0 ## 7 2 4 0.0133 ## 8 2 5 0 ## 9 2 6 0 ## 10 3 4 0.6 ## 11 3 5 0.189 ## 12 3 6 0 ## 13 4 5 0.156 ## 14 4 6 0 ## 15 5 6 0 pero nótese que, como señalamos arriba, esta operación será muy costosa incluso si la colección de textos es de tamaño moderado. Si los textos son cortos, entonces basta tomar valores como \\(k=4,5\\), pues hay un total de \\(27^4\\) tejas de tamaño \\(4\\), y el número de tejas de un documento corto (mensajes, tweets) es mucho más bajo que \\(27^4\\) (nota: ¿puedes explicar por qué este argumento no es exactamente correcto?) Para documentos grandes, como noticias o artículos, es mejor escoger un tamaño más grande, como \\(k=9,10\\), pues en documentos largos puede haber cientos de miles de caracteres. Si \\(k\\) fuera más chica entonces una gran parte de las tejas aparecerá en muchos de los documentos, y todos los documentos tendrían similitud alta. Evitamos escoger \\(k\\) demasiado grande, pues entonces los únicos documentos similares tendrían que tener subcadenas largas exactamente iguales. Por ejemplo: “Batman y Robin” y “Robin y Batman” son algo similares si usamos tejas de tamaño 3, pero son muy distintas si usamos tejas de tamaño 8: Ejemplo tejas_1 &lt;- calcular_tejas(&quot;Batman y Robin&quot;, k = 3) tejas_2 &lt;- calcular_tejas(&quot;Robin y Batman&quot;, k = 3) sim_jaccard(tejas_1, tejas_2) ## [1] 0.6 tejas_1 &lt;- calcular_tejas(&quot;Batman y Robin&quot;, k = 8) tejas_2 &lt;- calcular_tejas(&quot;Robin y Batman&quot;, k = 8) sim_jaccard(tejas_1, tejas_2) ## [1] 0 3.3 Representación matricial Podemos usar una matriz binaria para guardar todas las representaciones en k-tejas de nuestra colección de documentos. Puede usarse una representación rala (sparse) si es necesario: dtejas_tbl &lt;- tibble(id = paste0(&quot;doc_&quot;, 1:length(textos)), tejas = tejas_doc) |&gt; unnest(cols = tejas) |&gt; unique() |&gt; mutate(val = 1) |&gt; pivot_wider(names_from = id, values_from = val, values_fill = list(val = 0)) |&gt; arrange(tejas) # opcionalmente ordenamos tejas dtejas_tbl ## # A tibble: 123 × 7 ## tejas doc_1 doc_2 doc_3 doc_4 doc_5 doc_6 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot; al &quot; 1 1 0 0 0 0 ## 2 &quot; alc&quot; 1 1 0 0 0 0 ## 3 &quot; ani&quot; 0 0 0 0 0 1 ## 4 &quot; cor&quot; 0 0 0 0 1 0 ## 5 &quot; de &quot; 0 0 1 1 0 0 ## 6 &quot; doc&quot; 0 0 1 1 0 0 ## 7 &quot; eje&quot; 0 0 1 1 0 0 ## 8 &quot; el &quot; 0 0 1 1 0 0 ## 9 &quot; es &quot; 0 0 1 1 0 0 ## 10 &quot; gat&quot; 1 1 0 0 0 0 ## # … with 113 more rows ¿Cómo calculamos la similitud de Jaccard usando estos datos? Calcular la unión e intersección se puede hacer haciendo OR y AND de las columnas, y entonces podemos calcular la similitud inter_12 &lt;- sum(dtejas_tbl$doc_1 &amp; dtejas_tbl$doc_2) union_12 &lt;- sum(dtejas_tbl$doc_1 | dtejas_tbl$doc_2) similitud &lt;- inter_12/union_12 similitud # comparar con el número que obtuvimos arriba. ## [1] 0.773 El cálculo para todos los documentos podríamos hacerlo (aunque veremos que normalmente no haremos esto si no necesitamos calcular todas las similitudes) con: mat_td &lt;- dtejas_tbl |&gt; select(-tejas) |&gt; as.matrix() |&gt; t() 1 - dist(mat_td, method = &quot;binary&quot;) ## doc_1 doc_2 doc_3 doc_4 doc_5 ## doc_2 0.7727 ## doc_3 0.0000 0.0000 ## doc_4 0.0137 0.0133 0.6000 ## doc_5 0.0000 0.0000 0.1892 0.1556 ## doc_6 0.0000 0.0000 0.0000 0.0000 0.0000 3.4 Minhash y reducción probabilística de dimensionalidad Para una colección grande de documentos la representación binaria de la colección de documentos puede tener un número muy grande de renglones. Puede ser posible crear un número más chico de nuevos features (ojo: aquí los renglones son las “variables,” y los casos son las columnas) con los que sea posible obtener una buena aproximación de la similitud. La idea básica es la siguiente: Escogemos una función al azar (una función hash) que mapea cadenas cortas a un número grande de enteros, de manera existe muy baja probabilidad de colisiones, y no hay correlación entre las cadenas y el valor al que son mapeados. Si un documento tiene tejas \\(T\\), aplicamos la función hash a cada teja de \\(T\\), y calculamos el mínimo de estos valores hash. Repetimos este proceso para varias funciones hash fijas, por ejemplo \\(k= 5\\) Los valores mínimos obtenidos nos dan una representación en dimensión baja de cada documento. Ejemplo textos_tbl &lt;- tibble(doc_id = 1:length(textos), texto = textos) tejas_tbl &lt;- tibble(doc_id = 1:length(textos), tejas = tejas_doc) tejas_tbl ## # A tibble: 6 × 2 ## doc_id tejas ## &lt;int&gt; &lt;list&gt; ## 1 1 &lt;chr [41]&gt; ## 2 2 &lt;chr [42]&gt; ## 3 3 &lt;chr [28]&gt; ## 4 4 &lt;chr [36]&gt; ## 5 5 &lt;chr [16]&gt; ## 6 6 &lt;chr [31]&gt; Creamos una función hash: set.seed(813) generar_hash &lt;- function(){ r &lt;- as.integer(stats::runif(1, 1, 2147483647)) funcion_hash &lt;- function(tejas){ digest::digest2int(tejas, seed = r) } funcion_hash } h_1 &lt;- generar_hash() Y aplicamos la función a cada teja del documento 1, y tomamos el mínimo: hashes_1 &lt;- h_1(tejas_tbl$tejas[[1]]) hashes_1 ## [1] -1318809190 -1534091290 -1401861150 -1601894665 781339434 -519860631 ## [7] 2116727945 -1824301917 -1401861150 -1371561364 -1385084918 -821046029 ## [13] 1766711521 1952075680 569109680 -1412107908 1059544482 -1866546594 ## [19] 2090868926 -455965089 784118133 1421354549 -1397404756 -481987742 ## [25] -1824301917 -1401861150 1157451749 698137483 -623448608 544014704 ## [31] -587926856 1455454405 950121497 -165719415 2119479774 2115904977 ## [37] 1519310299 1652203243 -667865842 1219377605 -1500087628 minhash_1 &lt;- min(hashes_1) minhash_1 ## [1] -1866546594 Consideramos este minhash como un descriptor del documento. Generalmente usamos más de un descriptor. En el siguiente ejemplo usamos 4 funciones hash creadas de manera independiente: hashes &lt;- map(1:4, ~ generar_hash()) docs_firmas &lt;- tejas_tbl |&gt; mutate(firma = map(tejas, \\(lista) map_int(hashes, \\(h) min(h(lista))))) |&gt; select(doc_id, firma) |&gt; unnest_wider(firma, names_sep = &quot;_&quot;) docs_firmas ## # A tibble: 6 × 5 ## doc_id firma_1 firma_2 firma_3 firma_4 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 -2129712961 -2124594172 -2073157333 -1982715048 ## 2 2 -2129712961 -2124594172 -2073157333 -1982715048 ## 3 3 -2139075502 -2093452194 -1959662839 -2048484934 ## 4 4 -2139075502 -2093452194 -1959662839 -2048484934 ## 5 5 -2106131386 -2093452194 -2127642881 -1984764210 ## 6 6 -2115397946 -2087727654 -2074217109 -1986993146 Nótese ahora que en documentos muy similares, varios de los minhashes coinciden. Esto es porque la teja donde ocurrió el mínimo está en los dos documentos. Entonces cuando los las tejas de dos documentos son muy similares, es muy probable que sus minhashes coincidan. ¿Cuál es la probabilidad de que la firma coincida para un documento? Sea \\(h\\) una función hash escogida escogida al azar, y \\(a\\) y \\(b\\) dos documentos dados dadas. Denotamos como \\(f_h\\) la función minhash asociada a \\(h\\). Entonces \\[P(f_h(a) = f_h(b)) = sim(a, b)\\] donde \\(sim\\) es la similitud de Jaccard basada en las tejas usadas. Sean \\(h_1, h_2, \\ldots h_n\\) funciones hash escogidas al azar de manera independiente. Si \\(n\\) es grande, entonces por la ley de los grandes números \\[sim(a,b) \\approx \\frac{|h_j : f_{h_j}{\\pi_j}(a) = f_{h_j}(b)|}{n},\\] es decir, la similitud de Jaccard es aproximadamente la proporción de elementos de las firmas que coinciden. Ahora damos un argumento para demostrar este resultado: Supongamos que el total de tejas de los dos documentos es \\(|A\\cup B|\\), y el número de tejas que tienen en común es \\(|A\\cap B|\\). Sea \\(h\\) la función hash que escogimos al azar. Para fijar ideas, puedes suponer que las tejas están numeradas \\(1,\\ldots, M\\), y la función hash es una permutación aleatoria de estos números. Entonces: El mínimo de \\(h\\) puede ocurrir en cualquier elemento de \\(|A\\cup B|\\) con la misma probabilidad. Los minhashes de \\(a\\) y \\(b\\) coinciden si y sólo si el mínimo de \\(h\\) ocurre en un elemento de \\(|A\\cap B|\\) Por 1 y 2, la probabilidad de que esto ocurra es \\[\\frac{|A\\cap B|}{|A\\cup B|},\\] que es la similitud de Jaccard. Nótese que esto requiere que la familia de donde escogemos nuestra función hash cumple, al menos aproximadamente, las propiedades 1 y 2. Para que 1 ocurra, la familia debe ser suficientemente grande y variada: por ejemplo, esto fallaría si todas las cadenas que comienzan con “a” se mapean a números chicos. Para que ocurra 2, no debe haber colisiones (cadenas distintas que se mapean al mismo valor). Observaciónes: Una familia que cumple de manera exacta estas dos propiedades es la familia de permutaciones que mencionamos arriba: numeramos las tejas, construimos una permutación al azar, y luego aplicamos esta función de permutaciones a los índices de las tejas. La razón por la que esta familia no es utiliza típicamente es porque es costosa si el número de tejas es grande: primero hay que escoger un ordenamiento al azar, y luego es necesario almacenarlo. Muchas veces, se utiliza una función hash con aritmética modular como sigue: sea \\(M\\) el número total de tejas, y sea \\(p\\) un número primo fijo grande (al menos \\(p &gt; M\\)). Numeramos las tejas. Ahora escogemos dos enteros \\(a\\) y \\(b\\) al azar, y hacemos \\[h(x) = (ax + b\\mod p) \\mod M\\] Estas funciones se pueden seleccionar y aplicar rápidamente, y sólo tenemos que almacenar los coeficientes \\(a\\) y \\(b\\). En el enfoque que vimos arriba, utilizamos directamente una función hash de cadenas que está diseñada para cumplir 1 y 2 de manera aproximada. Resumen. Con el método de minhash, representamos a los documentos con un número relativamente chico de atributos numéricos (reducción de dimensionalidad). Esta respresentación tiene la propiedad de que textos muy similares con probabilidad alta coinciden en uno o más de los descriptores. 3.5 Agrupando textos de similitud alta Nuestro siguiente paso es evitar hacer la comparación de todos los pares de descriptores. Para esto hacemos un clustering no exhaustivo basado en los descriptores que acabamos de construir. Recordemos que tenemos docs_firmas ## # A tibble: 6 × 5 ## doc_id firma_1 firma_2 firma_3 firma_4 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 -2129712961 -2124594172 -2073157333 -1982715048 ## 2 2 -2129712961 -2124594172 -2073157333 -1982715048 ## 3 3 -2139075502 -2093452194 -1959662839 -2048484934 ## 4 4 -2139075502 -2093452194 -1959662839 -2048484934 ## 5 5 -2106131386 -2093452194 -2127642881 -1984764210 ## 6 6 -2115397946 -2087727654 -2074217109 -1986993146 Ahora agrupamos documentos que comparten alguna firma. A los grupos que coinciden en cada firma les llamamos cubetas: cubetas_tbl &lt;- docs_firmas |&gt; pivot_longer(contains(&quot;firma_&quot;), &quot;n_firma&quot;) |&gt; mutate(cubeta = paste(n_firma, value)) |&gt; group_by(cubeta) |&gt; summarise(documentos = list(doc_id)) |&gt; mutate(num_docs = map_int(documentos, length)) cubetas_tbl ## # A tibble: 15 × 3 ## cubeta documentos num_docs ## &lt;chr&gt; &lt;list&gt; &lt;int&gt; ## 1 firma_1 -2106131386 &lt;int [1]&gt; 1 ## 2 firma_1 -2115397946 &lt;int [1]&gt; 1 ## 3 firma_1 -2129712961 &lt;int [2]&gt; 2 ## 4 firma_1 -2139075502 &lt;int [2]&gt; 2 ## 5 firma_2 -2087727654 &lt;int [1]&gt; 1 ## 6 firma_2 -2093452194 &lt;int [3]&gt; 3 ## 7 firma_2 -2124594172 &lt;int [2]&gt; 2 ## 8 firma_3 -1959662839 &lt;int [2]&gt; 2 ## 9 firma_3 -2073157333 &lt;int [2]&gt; 2 ## 10 firma_3 -2074217109 &lt;int [1]&gt; 1 ## 11 firma_3 -2127642881 &lt;int [1]&gt; 1 ## 12 firma_4 -1982715048 &lt;int [2]&gt; 2 ## 13 firma_4 -1984764210 &lt;int [1]&gt; 1 ## 14 firma_4 -1986993146 &lt;int [1]&gt; 1 ## 15 firma_4 -2048484934 &lt;int [2]&gt; 2 Ahora filtramos las cubetas que tienen más de un elemento: cubetas_tbl &lt;- cubetas_tbl |&gt; filter(num_docs &gt; 1) cubetas_tbl ## # A tibble: 8 × 3 ## cubeta documentos num_docs ## &lt;chr&gt; &lt;list&gt; &lt;int&gt; ## 1 firma_1 -2129712961 &lt;int [2]&gt; 2 ## 2 firma_1 -2139075502 &lt;int [2]&gt; 2 ## 3 firma_2 -2093452194 &lt;int [3]&gt; 3 ## 4 firma_2 -2124594172 &lt;int [2]&gt; 2 ## 5 firma_3 -1959662839 &lt;int [2]&gt; 2 ## 6 firma_3 -2073157333 &lt;int [2]&gt; 2 ## 7 firma_4 -1982715048 &lt;int [2]&gt; 2 ## 8 firma_4 -2048484934 &lt;int [2]&gt; 2 Y de aquí extraemos pares candidatos que tienen alta probabilidad de ser muy similares: pares_tbl &lt;- cubetas_tbl |&gt; mutate(pares_cand = map(documentos, ~ combn(.x, 2, simplify = FALSE))) |&gt; select(cubeta, pares_cand) |&gt; unnest(pares_cand) |&gt; unnest_wider(pares_cand, names_sep = &quot;_&quot;) pares_tbl ## # A tibble: 10 × 3 ## cubeta pares_cand_1 pares_cand_2 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 firma_1 -2129712961 1 2 ## 2 firma_1 -2139075502 3 4 ## 3 firma_2 -2093452194 3 4 ## 4 firma_2 -2093452194 3 5 ## 5 firma_2 -2093452194 4 5 ## 6 firma_2 -2124594172 1 2 ## 7 firma_3 -1959662839 3 4 ## 8 firma_3 -2073157333 1 2 ## 9 firma_4 -1982715048 1 2 ## 10 firma_4 -2048484934 3 4 pares_tbl &lt;- pares_tbl |&gt; select(-cubeta) |&gt; unique() pares_tbl ## # A tibble: 4 × 2 ## pares_cand_1 pares_cand_2 ## &lt;int&gt; &lt;int&gt; ## 1 1 2 ## 2 3 4 ## 3 3 5 ## 4 4 5 Nótese que con este proceso evitamos hacer todas las comparaciones, y el método tiene complejidad lineal en el tamaño de la colección de documentos. Una vez que tenemos los pares, podemos calcular la similitud exacta de solamente esos documentos: pares_tbl |&gt; left_join(tejas_tbl |&gt; rename(pares_cand_1 = doc_id, texto_1 = tejas)) |&gt; left_join(tejas_tbl |&gt; rename(pares_cand_2 = doc_id, texto_2 = tejas)) |&gt; mutate(score = map2_dbl(texto_1, texto_2, ~ sim_jaccard(.x, .y))) |&gt; select(-contains(&quot;texto&quot;)) ## Joining, by = &quot;pares_cand_1&quot; ## Joining, by = &quot;pares_cand_2&quot; ## # A tibble: 4 × 3 ## pares_cand_1 pares_cand_2 score ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 2 0.773 ## 2 3 4 0.6 ## 3 3 5 0.189 ## 4 4 5 0.156 Si queremos capturar solamente aquellos pares de similitud muy alta, podemos también combinar firmas para formar cubetas donde las dos firmas coinciden: cubetas_tbl &lt;- docs_firmas |&gt; mutate(cubeta = paste(firma_1, firma_2)) |&gt; group_by(cubeta) |&gt; summarise(documentos = list(doc_id)) |&gt; mutate(num_docs = map_int(documentos, length)) cubetas_tbl ## # A tibble: 4 × 3 ## cubeta documentos num_docs ## &lt;chr&gt; &lt;list&gt; &lt;int&gt; ## 1 -2106131386 -2093452194 &lt;int [1]&gt; 1 ## 2 -2115397946 -2087727654 &lt;int [1]&gt; 1 ## 3 -2129712961 -2124594172 &lt;int [2]&gt; 2 ## 4 -2139075502 -2093452194 &lt;int [2]&gt; 2 pares_tbl &lt;- cubetas_tbl |&gt; filter(num_docs &gt; 1) |&gt; mutate(pares_cand = map(documentos, ~ combn(.x, 2, simplify = FALSE))) |&gt; select(cubeta, pares_cand) |&gt; unnest(pares_cand) |&gt; unnest_wider(pares_cand, names_sep = &quot;_&quot;) |&gt; left_join(tejas_tbl |&gt; rename(pares_cand_1 = doc_id, texto_1 = tejas)) |&gt; left_join(tejas_tbl |&gt; rename(pares_cand_2 = doc_id, texto_2 = tejas)) |&gt; mutate(score = map2_dbl(texto_1, texto_2, ~ sim_jaccard(.x, .y))) |&gt; select(-contains(&quot;texto&quot;)) ## Joining, by = &quot;pares_cand_1&quot; ## Joining, by = &quot;pares_cand_2&quot; pares_tbl ## # A tibble: 2 × 4 ## cubeta pares_cand_1 pares_cand_2 score ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 -2129712961 -2124594172 1 2 0.773 ## 2 -2139075502 -2093452194 3 4 0.6 3.6 Ejemplo: tweets Ahora buscaremos tweets similares en una colección de un dataset de kaggle. ruta &lt;- &quot;../datos/FIFA.csv&quot; if(!file.exists(ruta)){ fifa &lt;- read_csv(&quot;https://fifatweets.s3.amazonaws.com/FIFA.csv&quot;) write_csv(fifa, &quot;../datos/FIFA.csv&quot;) } else { fifa &lt;- read_csv(ruta) } ## Rows: 530000 Columns: 16 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (9): lang, Source, Orig_Tweet, Tweet, Hashtags, UserMentionNames, UserM... ## dbl (6): ID, len, Likes, RTs, Followers, Friends ## dttm (1): Date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. tw &lt;- fifa$Tweet tw[1:10] ## [1] &quot;Only two goalkeepers have saved three penalties in penalty shoot out Ricardo vs&quot; ## [2] &quot;scores the winning penalty to send into the quarter finals where they will face Russia&quot; ## [3] &quot;Tonight we have big game&quot; ## [4] &quot;We get stronger Turn the music up now We got that power power&quot; ## [5] &quot;Only two goalkeepers have saved three penalties in penalty shoot out Ricardo vs&quot; ## [6] &quot;We re looking strong going into the knockout stage We caught up with ahead of&quot; ## [7] &quot;am happy for winning Especially since you know we colluded and all Russia eliminates Spain after penalty&quot; ## [8] &quot;When you see me When we feel the same feeling Power power&quot; ## [9] &quot;Kasper Schmeichel takes the final award of the day&quot; ## [10] &quot;After Years Global Puma Ambassador LG Mobile Ambassador CocaCola WorldCup Kookmin Bank UNICEF&quot; set.seed(9192) num_tweets &lt;- 100000 system.time(tejas_doc &lt;- calcular_tejas(tw[1:num_tweets], k = 5)) ## user system elapsed ## 2.559 0.049 2.608 tejas_tbl &lt;- tibble(doc_id = 1:num_tweets, tejas = tejas_doc) hash_f &lt;- map(1:50, ~ generar_hash()) system.time( docs_firmas &lt;- tejas_tbl |&gt; mutate(firma = map(tejas, \\(lista) map_int(hash_f, \\(h) min(h(lista))))) |&gt; select(doc_id, firma)) ## user system elapsed ## 24.720 0.006 24.728 docs_firmas ## # A tibble: 100,000 × 2 ## doc_id firma ## &lt;int&gt; &lt;list&gt; ## 1 1 &lt;int [50]&gt; ## 2 2 &lt;int [50]&gt; ## 3 3 &lt;int [50]&gt; ## 4 4 &lt;int [50]&gt; ## 5 5 &lt;int [50]&gt; ## 6 6 &lt;int [50]&gt; ## 7 7 &lt;int [50]&gt; ## 8 8 &lt;int [50]&gt; ## 9 9 &lt;int [50]&gt; ## 10 10 &lt;int [50]&gt; ## # … with 99,990 more rows La firma de minhashes del primer documento es por ejemplo: docs_firmas$firma[1] ## [[1]] ## [1] -1953455759 -2112001395 -2031432865 -2108080560 -2095602723 -2118858341 ## [7] -2099337985 -1978298401 -2092751862 -2097330616 -1897306653 -2139645671 ## [13] -1981364640 -2081406116 -2127003388 -2138613184 -2146084782 -2082312909 ## [19] -2083329993 -2001255852 -2091106849 -2091293600 -2120806067 -2095725804 ## [25] -2143199361 -2091159512 -2124095195 -2038461037 -2109684884 -2073791594 ## [31] -2129542593 -1904599938 -2118234795 -2056298394 -2105031889 -2113931139 ## [37] -2134167293 -2140440957 -1717523835 -2144768923 -2138027225 -2084388635 ## [43] -2110278901 -2135431955 -2139423769 -2130397390 -2097742891 -2105787557 ## [49] -2101425996 -1941363605 Y probaremos primero hacer cubetas con algunas las firmas (las cuatro primeras por ejemplo): docs_firmas &lt;- docs_firmas |&gt; mutate(cubeta_nombre = map_chr(firma, \\(x) paste(x[1:3], collapse = &quot;-&quot;))) docs_cubetas_tbl &lt;- docs_firmas |&gt; group_by(cubeta_nombre) |&gt; summarise(docs = list(doc_id)) |&gt; mutate(num_docs = map_int(docs, length)) docs_cubetas_filt_tbl &lt;- docs_cubetas_tbl |&gt; filter(num_docs &gt; 1) Y examinamos ahora algunas de las cubetas: docs_ejemplo &lt;- docs_cubetas_filt_tbl$docs[[125]] tw[docs_ejemplo] ## [1] &quot;We got That power power&quot; &quot;We got That power power&quot; ## [3] &quot;We got That power power&quot; docs_ejemplo &lt;- docs_cubetas_filt_tbl$docs[[1658]] length(docs_ejemplo) ## [1] 406 tw[docs_ejemplo][1:2] ## [1] &quot;short story involving Nigeria Argentina and France&quot; ## [2] &quot;short story involving Nigeria Argentina and France&quot; docs_ejemplo &lt;- docs_cubetas_filt_tbl$docs[[4958]] tw[docs_ejemplo] ## [1] &quot;The keeps on serving us more and more drama If you missed any of the great moments from todays fixtures yo&quot; ## [2] &quot;The keeps on serving us more and more drama If you missed any of the great moments from todays fixtures yo&quot; ## [3] &quot;The keeps on serving us more and more drama If you missed any of the great moments from todays fixtures yo&quot; ## [4] &quot;The keeps on serving us more and more drama If you missed any of the great moments from todays fixtures you can watch the highlights on&quot; ## [5] &quot;The keeps on serving us more and more drama If you missed any of the great moments from todays fixtures you can watch the highlights on&quot; Con este método, podemos extraer pares de muy alta similitud (cercano a duplicados) de forma eficiente en colecciones grandes de texto. 3.7 Verificar si un nuevo elemento es duplicado Si tenemos un nuevo entrada, podemos checar si es duplicado calculando su firma, formando la cubetas, y revisando si cae en alguna de las cubetas conocidas. Nótese que tenemos que guardar las funciones hash que usamos para aplicar a los nuevos datos, y repetir el proceso exactamente como procesamos los datos originales. Por ejemplo: nuevos_tweets &lt;- 150000:150005 tw[nuevos_tweets] ## [1] &quot;It should sound the song that was on the Show of the great fountain of Dubai Please play of&quot; ## [2] &quot;PARK CHANYEOL Please play of&quot; ## [3] &quot;What Game Congratulations to on reaching the quarter finals with but commiserations to Marcos&quot; ## [4] &quot;For the fans of the vs rivalry Mbappe Cristiano Ronaldo fan has ended Messis dream of winning the&quot; ## [5] &quot;Messi How far Suarez could not make it to the quarterfinals you know what to do Suarez Roger&quot; ## [6] &quot;Got few daubes all right had to put him in when was painting low Thankfully painting finished another bit later after the&quot; system.time(tejas_nuevas_doc &lt;- calcular_tejas(tw[nuevos_tweets], k = 5)) ## user system elapsed ## 0.001 0.000 0.001 tejas_nuevas_tbl &lt;- tibble(doc_id = nuevos_tweets, tejas = tejas_nuevas_doc) system.time( docs_nuevas_firmas &lt;- tejas_nuevas_tbl |&gt; mutate(firma = map(tejas, \\(lista) map_int(hash_f, \\(h) min(h(lista))))) |&gt; select(doc_id, firma)) ## user system elapsed ## 0.006 0.000 0.007 docs_nuevas_firmas ## # A tibble: 6 × 2 ## doc_id firma ## &lt;int&gt; &lt;list&gt; ## 1 150000 &lt;int [50]&gt; ## 2 150001 &lt;int [50]&gt; ## 3 150002 &lt;int [50]&gt; ## 4 150003 &lt;int [50]&gt; ## 5 150004 &lt;int [50]&gt; ## 6 150005 &lt;int [50]&gt; docs_nuevas_firmas &lt;- docs_nuevas_firmas |&gt; mutate(cubeta_nombre = map_chr(firma, \\(x) paste(x[1:3], collapse = &quot;-&quot;))) docs_cubetas_nuevas_tbl &lt;- docs_nuevas_firmas |&gt; group_by(cubeta_nombre) |&gt; summarise(docs = list(doc_id)) |&gt; mutate(num_docs = map_int(docs, length)) docs_cubetas_nuevas_tbl ## # A tibble: 6 × 3 ## cubeta_nombre docs num_docs ## &lt;chr&gt; &lt;list&gt; &lt;int&gt; ## 1 -2041983150--2147447475--2129777399 &lt;int [1]&gt; 1 ## 2 -2051683951--2053274679--2125528528 &lt;int [1]&gt; 1 ## 3 -2108824716--2032083428--2137254712 &lt;int [1]&gt; 1 ## 4 -2113147634--2140751096--2058622652 &lt;int [1]&gt; 1 ## 5 -2139603446--2140751096--2090996001 &lt;int [1]&gt; 1 ## 6 -2142045021--2054826468--2129777399 &lt;int [1]&gt; 1 Y ahora podemos hacer un semi-join con las cubetas: cand_duplicados_tbl &lt;- docs_cubetas_nuevas_tbl |&gt; semi_join(docs_cubetas_tbl |&gt; select(cubeta_nombre)) ## Joining, by = &quot;cubeta_nombre&quot; cand_duplicados_tbl ## # A tibble: 3 × 3 ## cubeta_nombre docs num_docs ## &lt;chr&gt; &lt;list&gt; &lt;int&gt; ## 1 -2041983150--2147447475--2129777399 &lt;int [1]&gt; 1 ## 2 -2113147634--2140751096--2058622652 &lt;int [1]&gt; 1 ## 3 -2142045021--2054826468--2129777399 &lt;int [1]&gt; 1 Y vemos que tenemos tres candidatos a duplicados. Checamos el primer tweet: cubeta_1 &lt;- cand_duplicados_tbl |&gt; pull(cubeta_nombre) |&gt; pluck(1) doc_1 &lt;- cand_duplicados_tbl |&gt; pull(docs) |&gt; pluck(1) cubeta_1 ## [1] &quot;-2041983150--2147447475--2129777399&quot; doc_1 ## [1] 150004 # en la tabla original de todos los tweets doc_2 &lt;- filter(docs_cubetas_tbl, cubeta_nombre == cubeta_1) |&gt; pull(docs) |&gt; pluck(1) doc_2[1] ## [1] 36839 Checamos los tweets: tw[doc_2[1]] ## [1] &quot;Messi How far Suarez could not make it to the quarterfinals you know what to do Suarez Roger&quot; tw[doc_1] ## [1] &quot;Messi How far Suarez could not make it to the quarterfinals you know what to do Suarez Roger&quot; Y efectivamente detectamos que el nuevo tweet es duplicado. Otra vez, no fue necesario hacer una comparación exhaustiva del nuevo tweet contra nuestra colección inicial grande. Ejercicio: checa los otros candidatos a duplicados que encontramos en este ejemplo. 3.8 Controlando la sensibilidad y umbral de similitud En algunos casos nos pueden interesar encontrar duplicados muy cercanos, y en otros problemas quisiéramos capturar pares con un umbral más bajo de similitud. Usando distinto número de hashes podemos hacer esto. En primer lugar, sabemos que un la probabilidad de que dos minhashes coincidan es igual a la similitud de jaccard de los dos textos correspondientes. Si \\(s\\) es la similitud de Jaccard entre \\(a\\) y \\(b\\), entonces: \\[P(f_h(a) = f_h(b)) = s(a,b) = s\\] Si escogemos \\(k\\) hashes al azar, y definimos candidatos solamente cuando coincidan todos los hashes (construcción AND), la probabilidad de hacer un par candidatos es: \\[P(f_1(a) = f_1(b), \\ldots, f_k(a)=f_k(b)) = s^k\\] Por otro lado, si consideramos un par candidato cuando al menos alguno de los minhashes coinciden (construcción OR), la probabilidad que \\(a\\) y \\(b\\) sean candidatos es: \\[P(f_j(a) = f_j(b) \\, \\textrm{ para alguna } j) = 1-(1-s)^k\\] Podemos graficar estas probabilidades con distintos valores de \\(k\\) k &lt;- 3 prob_captura &lt;- tibble(s = seq(0, 1, 0.01)) |&gt; crossing(tipo = c(&quot;k_hashes_AND&quot;, &quot;k_hashes_OR&quot;)) |&gt; crossing(k = c(1, 3, 5, 10)) |&gt; mutate(prob_captura = ifelse(tipo == &quot;k_hashes_AND&quot;, s^k, 1 - (1 - s)^k)) ggplot(prob_captura, aes(x = s, y = prob_captura, colour = factor(k))) + geom_point() + facet_wrap(~ tipo) Nótese que: Si queremos tener muy alta probabilidad de capturar todos los pares similares, podemos usar la construcción OR con k = 3 para similitud mayor a 0.75 o k = 10 para similitud mayor a 0.3, por ejemplo La desventaja de esto, es que es posible que obtengamos muchos candidatos que realmente no tienen la similitud deseada (falsos positivos). Esto implica más procesamiento. Si solo queremos capturar pares de muy alta similitud (por ejemplo &gt; 0.98), y no es grave que tengamos algunos falsos negativos, podemos usar la construcción AND con 1 o 3 hashes por ejemplo, y obtendremos un número más manejable de pares candidatos para procesar. La desventaja de esto es que es posible obtener algunos falsos negativos. Depende de la aplicación esto puede ser aceptable o no. A partir de un umbral \\(s\\) de similitud para los pares que queremos capturar, podemos usar varios minhashes para afinar el método: Usamos la construcción OR con varios hashes para capturar pares de alta o mediana similitud con mucha confianza. Generalmente es necesario filtrar falsos positivos. Usamos la construcción AND con uno o varios hashes para pares de muy alta similitud con confianza alta. Tendremos menos falsos positivos, pero también es posible tener más falsos negativos (se nos escapan algunos pares muy similares). Nota: es posible combinar estas técnicas para refinar la captura de pares a un nivel de similitud dado haciendo bandas de hashes: por ejemplo, si tenemos 20 hashes podemos hacer 4 bandas de 5 hashes, usamos AND para cada grupo de 5 hashes y OR para las 4 bandas. Para más de esto, revisa nuestro texto (Leskovec, Rajaraman, and Ullman 2014). 3.9 Distancia euclideana y LSH Ahora aplicamos estas ideas para otro tipo de similitud. En este caso, consideramos la distancia euclideana usual en dimensión \\(p\\): \\[d(x,y) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 +\\cdots (x_p - y_p)^2}\\] Y nuestra tarea es encontrar todos los pares tales que su distancia euclideana es muy cercana a 0. Es decir, queremos hacer “clusters” pero sólo de datos muy cercanos, igual que en los ejemplos de similitud de jaccard. Para distancia euclideana nuestros hashes resultan de proyecciones aleatorias rectas fijas en cubetas. La idea general es que tomamos una línea al azar en el espacio de entradas, y la dividimos en cubetas de manera uniforme. El valor hash de un punto \\(x\\) es el número de cubeta donde cae la proyección de \\(x\\). Más especificamente, si escogemos un ancho \\(r\\) de cubeta: Escogemos una dirección al azar \\(v\\) El hash de un punto \\(x\\) se calcula como sigue: Calculamos el tamaño de la proyección \\(x\\) sobre \\(v\\) Dividimos este tamaño entre \\(r\\) el hash la parte entera de este último número Es decir, \\[h(x) = \\left\\lfloor{ \\frac{x\\cdot v}{r}}\\right\\rfloor\\] Ejercicio: haz un dibujo de este proceso, y cómo se calcula el hash de un punto \\(x\\) una vez que tienes \\(r\\) y \\(v\\). Por ejemplo, si \\(v=(1,2)\\), y \\(r = 1\\): library(tidyverse) norma &lt;- function(x) sqrt(sum(x^2)) v &lt;- c(1,2) / norma(c(1,2)) v ## [1] 0.447 0.894 hash_1 &lt;- function(x) floor(sum(x * v) / 1) hash_1(c(5,0)) ## [1] 2 hash_1(c(0,-1)) ## [1] -1 Construimos ahora nuestra función generadora de hashes: gen_hash &lt;- function(p, r){ v &lt;- rnorm(p) v &lt;- v / norma(v) # devolvemos una función que calcula la cubeta: function(x){ floor(sum(x * v) / r) |&gt; as.integer() } } set.seed(823) hash_1 &lt;- gen_hash(2, 1) # los hashes de dos puntos: hash_1(c(4, 7)) ## [1] -7 hash_1(c(-4, 7)) ## [1] 0 # el vector que escogimos es environment(hash_1)$v ## [1] -0.914 -0.405 Ejemplo La siguiente función genera dos clusters de puntos mezclados con puntos distribuidos normales con desviación estándar relativamente grande set.seed(1021) simular_puntos &lt;- function(d = 2, n = 200){ #puntos muy cercanos a (3,3,..., 3): mat_1 &lt;- matrix(rnorm(10 * d, sd = 0.01) + 3, ncol = d) #puntos muy cercanos a (-3,-3,..., -3): mat_2 &lt;- matrix(rnorm(10 * d, sd = 0.01) - 3, ncol = d) # puntos distribuidos alrededor del origen: mat_3 &lt;- matrix(rnorm(n * d, sd = 10), ncol = d) datos_tbl_vars &lt;- rbind(mat_3, mat_1, mat_2) |&gt; as_tibble() |&gt; mutate(id_1 = row_number()) datos_tbl_vars } # diez puntos en cluster 1, diez en cluster 2, y 100 sin cluster: datos_tbl_vars &lt;- simular_puntos(d = 2, n = 100) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ggplot(datos_tbl_vars, aes(x = V1, y= V2)) + geom_jitter(width = 0.5, height = 0.5, alpha = 0.3) Para este ejemplo calculamos las distancias reales: dist_e &lt;- function(x, y){ norma(x - y) } datos_tbl &lt;- datos_tbl_vars |&gt; pivot_longer(-id_1, names_to = &quot;variable&quot;, values_to = &quot;valor&quot;) |&gt; group_by(id_1) |&gt; arrange(variable) |&gt; summarise(vec_1 = list(valor)) system.time( pares_tbl &lt;- datos_tbl |&gt; crossing(datos_tbl |&gt; rename(id_2 = id_1, vec_2 = vec_1)) |&gt; filter(id_1 &lt; id_2) |&gt; mutate(dist = map2_dbl(vec_1, vec_2, dist_e)) ) ## user system elapsed ## 0.024 0.000 0.024 pares_tbl |&gt; head() ## # A tibble: 6 × 5 ## id_1 vec_1 id_2 vec_2 dist ## &lt;int&gt; &lt;list&gt; &lt;int&gt; &lt;list&gt; &lt;dbl&gt; ## 1 1 &lt;dbl [2]&gt; 2 &lt;dbl [2]&gt; 7.07 ## 2 1 &lt;dbl [2]&gt; 3 &lt;dbl [2]&gt; 24.2 ## 3 1 &lt;dbl [2]&gt; 4 &lt;dbl [2]&gt; 29.0 ## 4 1 &lt;dbl [2]&gt; 5 &lt;dbl [2]&gt; 24.9 ## 5 1 &lt;dbl [2]&gt; 6 &lt;dbl [2]&gt; 3.31 ## 6 1 &lt;dbl [2]&gt; 7 &lt;dbl [2]&gt; 16.2 nrow(pares_tbl) ## [1] 7140 Supongamos que queremos encontrar los puntos que están a distancia menor a 1: pares_sim &lt;- pares_tbl |&gt; filter(dist &lt; 1) nrow(pares_sim) ## [1] 103 Ahora veremos cómo encontrar estos 103 pares de puntos cercanos. Cálculo de firmas Usaremos 4 hashes con tamaño de cubeta = 0.2: #generar hashes hash_f &lt;- map(1:4, ~ gen_hash(p = 2, r = 0.2)) # esta es una función de conveniencia: calculador_hashes &lt;- function(hash_f){ function(z) { map_int(hash_f, ~ .x(z)) } } calc_hashes &lt;- calculador_hashes(hash_f) Calculamos las firmas: firmas_tbl &lt;- datos_tbl_vars |&gt; pivot_longer(cols = -id_1, names_to = &quot;variable&quot;, values_to = &quot;valor&quot;) |&gt; group_by(id_1) |&gt; summarise(vec_1 = list(valor)) |&gt; mutate(firma = map(vec_1, ~ calc_hashes(.x))) |&gt; select(id_1, firma) firmas_tbl ## # A tibble: 120 × 2 ## id_1 firma ## &lt;int&gt; &lt;list&gt; ## 1 1 &lt;int [4]&gt; ## 2 2 &lt;int [4]&gt; ## 3 3 &lt;int [4]&gt; ## 4 4 &lt;int [4]&gt; ## 5 5 &lt;int [4]&gt; ## 6 6 &lt;int [4]&gt; ## 7 7 &lt;int [4]&gt; ## 8 8 &lt;int [4]&gt; ## 9 9 &lt;int [4]&gt; ## 10 10 &lt;int [4]&gt; ## # … with 110 more rows firmas_tbl$firma[[1]] ## [1] -88 14 14 -87 firmas_tbl$firma[[2]] ## [1] -91 48 48 -92 Para este ejemplo, consideraremos todos los pares que coinciden en al menos una cubeta (hacemos disyunción o construcción OR de los 4 hashes): cubetas_tbl &lt;- firmas_tbl |&gt; unnest(firma) |&gt; group_by(id_1) |&gt; mutate(hash_no = 1:4) |&gt; mutate(cubeta = paste(hash_no, firma, sep = &quot;/&quot;)) cubetas_tbl ## # A tibble: 480 × 4 ## # Groups: id_1 [120] ## id_1 firma hash_no cubeta ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 -88 1 1/-88 ## 2 1 14 2 2/14 ## 3 1 14 3 3/14 ## 4 1 -87 4 4/-87 ## 5 2 -91 1 1/-91 ## 6 2 48 2 2/48 ## 7 2 48 3 3/48 ## 8 2 -92 4 4/-92 ## 9 3 -24 1 1/-24 ## 10 3 88 2 2/88 ## # … with 470 more rows Ahora agrupamos cubetas y filtramos las que tienen más de un elemento cubetas_tbl &lt;- cubetas_tbl |&gt; group_by(cubeta) |&gt; summarise(ids = list(id_1), n = length(id_1)) |&gt; filter(n &gt; 1) cubetas_tbl ## # A tibble: 79 × 3 ## cubeta ids n ## &lt;chr&gt; &lt;list&gt; &lt;int&gt; ## 1 1/-1 &lt;int [2]&gt; 2 ## 2 1/-13 &lt;int [2]&gt; 2 ## 3 1/-16 &lt;int [4]&gt; 4 ## 4 1/-21 &lt;int [11]&gt; 11 ## 5 1/-28 &lt;int [2]&gt; 2 ## 6 1/-32 &lt;int [2]&gt; 2 ## 7 1/-41 &lt;int [2]&gt; 2 ## 8 1/-44 &lt;int [2]&gt; 2 ## 9 1/-54 &lt;int [2]&gt; 2 ## 10 1/-55 &lt;int [2]&gt; 2 ## # … with 69 more rows Y finalmente, extraemos los pares candidatos: candidatos_tbl &lt;- cubetas_tbl |&gt; mutate(pares_cand = map(ids, ~ combn(.x, 2, simplify = FALSE))) |&gt; select(cubeta, pares_cand) |&gt; unnest(pares_cand) |&gt; unnest_wider(pares_cand, names_sep = &quot;_&quot;) |&gt; select(-cubeta) |&gt; unique() candidatos_tbl ## # A tibble: 219 × 2 ## pares_cand_1 pares_cand_2 ## &lt;int&gt; &lt;int&gt; ## 1 25 34 ## 2 30 85 ## 3 8 33 ## 4 8 40 ## 5 8 80 ## 6 33 40 ## 7 33 80 ## 8 40 80 ## 9 100 111 ## 10 100 112 ## # … with 209 more rows nrow(candidatos_tbl) ## [1] 219 En este caso, seguramente tenemos algunos falsos positivos que tenemos que filtrar, y quizá algunos falsos negativos. Calculamos distancias para todos los pares candidatos (es una lista mucho más corta generalmente): puntos_tbl &lt;- datos_tbl_vars |&gt; pivot_longer(V1:V2) |&gt; group_by(id_1) |&gt; select(-name) |&gt; summarise(punto = list(value)) candidatos_tbl &lt;- candidatos_tbl |&gt; left_join(puntos_tbl |&gt; rename(pares_cand_1 = id_1, punto_1 = punto)) |&gt; left_join(puntos_tbl |&gt; rename(pares_cand_2 = id_1, punto_2 = punto)) ## Joining, by = &quot;pares_cand_1&quot; ## Joining, by = &quot;pares_cand_2&quot; candidatos_tbl ## # A tibble: 219 × 4 ## pares_cand_1 pares_cand_2 punto_1 punto_2 ## &lt;int&gt; &lt;int&gt; &lt;list&gt; &lt;list&gt; ## 1 25 34 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 2 30 85 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 3 8 33 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 4 8 40 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 5 8 80 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 6 33 40 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 7 33 80 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 8 40 80 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 9 100 111 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 10 100 112 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## # … with 209 more rows pares_similares_tbl &lt;- candidatos_tbl |&gt; mutate(dist = map2_dbl(punto_1, punto_2, dist_e)) |&gt; filter(dist &lt; 1) nrow(pares_similares_tbl) ## [1] 96 Probando con datos de “gold standard” En este caso, sabemos cuáles son los pares que buscamos, así que podemos evaluar nuestro método: verdadero_pos &lt;- nrow(inner_join(pares_similares_tbl, pares_sim)) ## Joining, by = &quot;dist&quot; verdadero_pos ## [1] 96 sensibilidad &lt;- verdadero_pos / nrow(pares_sim) sensibilidad ## [1] 0.932 precision &lt;- verdadero_pos / nrow(pares_similares_tbl) precision ## [1] 1 Como vemos, la precisión es 1 y la sensibilidad es alta. Nos faltó encontrar una pequeña parte de los pares similares (alrededor de 7 de 103). Si queremos ser mas exhaustivos (con el mayor cómputo que implica), podemos hacer más anchas las cubetas (cambiar \\(r\\) a 1 por ejemplo), y podemos incluir más hashes. ¿Qué pasa si ponemos \\(r = 0.8\\) por ejemplo? Proyección aleatoria en cubetas. Podemos encontrar pares muy similares en datos numéricos usando como hashes proyecciones aleatorias discretizadas en cubetas. Para afinar el umbral de captura y el balance de falsos positivos y falsos negativos, usamos las mismas técnicas mencionadas arriba para minhashing (construcciones OR o AND y combinaciones). 3.10 Locality Sensitive Hashing (LSH) Las dos técnicas que acabamos de ver arriba son tipos de Locality Sensitive Hashing (LSH), donde usamos hashes que tienden a coincidir cuando dos elementos son similares. Esto requiere de dos partes: Una definición de distancia en el espacio original Una familia de hashes que se seleccionan al azar que son sensibles a la localidad en relación a la distancia seleccionada, que formalmente podemos escribir como: Sean \\(d_1&lt;d_2\\) dos valores (que interpretamos como distancias). Una familia \\({\\cal F}\\) es una familia \\(d_1,d_2,p_1,p_2\\), sensible a localidad (con \\(p_1&gt;p_2\\)) cuando para cualquier par de elementos \\(x,y\\), 1. Si \\(d(x,y)\\leq d_1\\), entonces la probabilidad \\(P(f(x)=f(y))\\geq p_1\\). 2. Si \\(d(x,y)\\geq d_2\\), entonces \\(P(f(x)=f(y))\\leq p_2\\) Nótese que las probabilidades están dadas sobre la selección de \\(f\\). Estas condiciones se interpretan como sigue: cuando \\(x\\) y \\(y\\) están suficientemente cerca (\\(d_1\\)), la probabilidad de que sean mapeados al mismo valor por una función \\(f\\) de la familia es alta. Cuando \\(x\\) y \\(y\\) están lejos \\(d_2\\), entonces, la probabilidad de que sean mapeados al mismo valor es baja. Podemos ver una gráfica: En nuestros ejemplos, vimos (puedes intentar demostrarlas): Para documentos, utilizamos distancia de Jaccard de tejas. Las funciones minhash dan una familia sensible a la localidad. Para datos numéricos con distancia euclideana, la familia de proyecciones aleatorias en cubetas das una familia sensible a la localidad (Adicional) La similitud coseno para datos numéricos (donde no nos importa la magnitud sino solo la dirección de lso puntos), que se utiliza a veces en procesamiento de texto, también puede tratarse utilizando proyecciones aleatorias con 2 cubetas (derecha e izquierda) De esta última puedes ver más en Leskovec, Rajaraman, and Ullman (2014). 3.11 LSH para imágenes Consideramos tres imágenes para probar: En espacios de dimensión muy alta, como en imágenes, conviene hacer reducción de dimensionalidad para definir la métrica de distancia y utilizar estos métodos para encontrar vecinos cercanos. library(keras) modelo &lt;- application_vgg16(weights = &#39;imagenet&#39;) ## Loaded Tensorflow version 2.4.0 # obtener la penúltima embed_modelo &lt;- keras_model(inputs = modelo$input, outputs = get_layer(modelo, &quot;fc2&quot;)$output) obtener_pixeles &lt;- function(imagen_ruta){ img &lt;- image_load(imagen_ruta, target_size = c(224,224)) x &lt;- image_to_array(img) array_reshape(x, c(1, dim(x))) } calcular_capa &lt;- function(imagen_ruta){ x &lt;- obtener_pixeles(imagen_ruta) |&gt; imagenet_preprocess_input() embed_modelo |&gt; predict(x) |&gt; as.numeric() } pixeles_1 &lt;- obtener_pixeles(&quot;../datos/imagenes/elefante_1.jpg&quot;) |&gt; as.numeric() pixeles_2 &lt;- obtener_pixeles(&quot;../datos/imagenes/elefante_3.jpg&quot;) |&gt; as.numeric() pixeles_3 &lt;- obtener_pixeles(&quot;../datos/imagenes/leon_1.jpg&quot;) |&gt; as.numeric() Calculamos la distancia pixel a pixel: mean((pixeles_2 - pixeles_1)^2) ## [1] 7040 mean((pixeles_1 - pixeles_3)^2) ## [1] 7064 Calculamos la penúltima capa de nuestro modelo para las imágenes de prueba: features_1 &lt;- calcular_capa(&quot;../datos/imagenes/elefante_1.jpg&quot;) features_2 &lt;- calcular_capa(&quot;../datos/imagenes/elefante_3.jpg&quot;) features_3 &lt;- calcular_capa(&quot;../datos/imagenes/leon_1.jpg&quot;) length(features_1) ## [1] 4096 Nótese ahora que la distancia en nuestro nuevo espacio de imágenes es mucho más chica para los elefantes que entre el león y los elefantes: mean((features_2 - features_1)^2) ## [1] 0.889 mean((features_1 - features_3)^2) ## [1] 3.2 Podemos usar entonces el siguiente proceso: Calculamos para cada imagen la representación dada por la última capa de una red nueronal de clasificación para imagen. Definimos como nuestra medida de distancia entre imagenes la distancia euclideana en la representación del inciso anterior Definimos funciones hash con proyecciones en cubetas como vimos arriba Con estos hashes, podemos encontrar imagenes duplicadas o muy similares. 3.12 Joins por similitud Otro uso de las técnicas del LSH nos permita hacer uniones (joins) por similitud. La idea es la siguiente: Tenemos una tabla A, con una columna A.x que es un texto, por ejemplo, o un vector de números, etc. Tenemos una tabla B, con una columna B.x que es del mismo tipo que A.x Queremos hacer una unión de A con B con la llave x, de forma que queden pareados todos los elementos tales que \\(sim(A.x_i, A.y_j)\\) es chica. Un ejemplo es pegar dos tablas de datos de películas de fuentes distintas mediante el título (que a veces varía en cómo está escrito, de manera que no podemos hacer un join usual), o títulos de pláticas en diferentes conferencias, o juntar registros de personas que pueden tener escrito su nombre de manera un poco diferente o con errores, netc. Usando LSH podemos hacer un join aproximado por similitud. La idea es la misma que antes: Calculamos cubetas de la misma forma para cada tabla (mismos hashes y bandas) Unimos las cubetas de las dos fuentes Los pares candidatos son todos los pares (uno de A y uno de B) que caen en la misma cubeta. Usando criterios adicionales, podemos filtrar falsos positivos. 3.13 Ejemplo: entity matching Ver tarea 4 en el repositorio del curso. Referencias "],["dvs-y-reducción-de-dimensionalidad.html", "4 DVS y reducción de dimensionalidad 4.1 Descomposición aditiva en matrices de rango 1 4.2 Aproximación con matrices de rango 1. 4.3 Aproximación con matrices de rango bajo 4.4 Interpetación de vectores singulares 4.5 Descomposición en valores singulares (SVD o DVS) 4.6 Más de interpretación geométrica 4.7 SVD para películas de netflix 4.8 Componentes principales", " 4 DVS y reducción de dimensionalidad En esta parte veremos técnicas de reducción de dimensionalidad por medio de proyecciones lineales, que buscan preservar la mayor parte de la información contenida en los datos originales. En particular veremos la descomposición en valores singulares, que es una de las más útiles. La descomposición en valores singulares puede verse como un tipo de descomposición aditiva en matrices de rango uno, así que comenzaremos explicando estos conceptos. 4.1 Descomposición aditiva en matrices de rango 1 Supongamos que tenemos una matriz de datos \\(X\\) de tamaño \\(n\\times p\\) (todas son variables numéricas). En los renglones tenemos los casos (\\(n\\)) y las columnas son las variables \\(p\\). Típicamente pensamos que las columnas o variables están todas definidas en una misma escala: por ejemplo, cantidades de dinero, poblaciones, número de eventos, etc. Cuando no es así, entonces normalizamos las variables de alguna forma para no tener unidades. 4.1.1 Matrices de rango 1 Una de las estructuras de datos más simples que podemos imaginar (que sea interesante) para un tabla de este tipo es que se trata de una matriz de datos de rango 1. Es generada por un score de individuos que determina mediante un peso el valor de una variable. Es decir, el individuo \\(i\\) en la variable \\(j\\) es \\[X_{ij} = \\sigma u_i v_j\\] Donde \\(u=(u_1,u_2,\\ldots, u_n)\\) son los scores de los individuos y \\(v = (v_1, v_2, \\ldots, v_p)\\) son los pesos de las variables. Tanto \\(u\\) como \\(v\\) son vectores normalizados, es decir \\(||u||=||v||=1\\). La constante \\(\\sigma\\) nos permite pensar que los vectores \\(u\\) y \\(v\\) están normalizados. Esto se puede escribir, en notación matricial, como \\[X = \\sigma u v^t\\] donde consideramos a \\(u\\) y \\(v\\) como matrices columna. Una matriz de rango uno (o en general de rango bajo) es más simple de analizar. En rango 1, tenemos que entender la variación de \\(n+p\\) datos (componentes de \\(u\\) y \\(v\\)), mientras que en una matriz general tenemos que entender \\(n\\times p\\) datos. Cada variable \\(j\\) de las observaciones es un reescalamiento del índice o score \\(u\\) de las personas por el factor \\(\\sigma v_j\\). Igualmente, cada caso \\(i\\) de las observaciones es un reescalamiento del índice o peso \\(v\\) de las variables por el factor \\(\\sigma u_i\\). \\(u\\) y \\(v\\) representan una dimensión (dimensión latente, componente) de estos datos. Ejemplo: una matriz de rango 1 de preferencias Supongamos que las columnas de \\(X\\) son películas (\\(p\\)), los renglones (\\(n\\)) personas, y la entrada \\(X_{ij}\\) es la afinidad de la persona \\(i\\) por la película \\(j\\). Vamos a suponer que estos datos tienen una estructura ficticia de rango 1, basada en las preferencias de las personas por películas de ciencia ficción. Construimos los pesos de las películas que refleja qué tanto son de ciencia ficción o no. Podemos pensar que cada uno de estos valores el el peso de la película en la dimensión de ciencia ficción. library(tidyverse) peliculas_nom &lt;- c(&#39;Gladiator&#39;,&#39;Memento&#39;,&#39;X-Men&#39;,&#39;Scream&#39;,&#39;Amores Perros&#39;, &#39;Billy Elliot&#39;, &#39;Lord of the Rings&#39;,&#39;Mulholland drive&#39;, &#39;Amelie&#39;,&#39;Planet of the Apes&#39;) # variable latente que describe el contenido de ciencia ficción de cada v &lt;- c(-1.5, -0.5, 4, -1,-3, -3, 0, 1, -0.5, 3.5) normalizar &lt;- function(x){ norma &lt;- sqrt(sum(x^2)) if(norma &gt; 0){ x_norm &lt;- x/norma } else { x_norm &lt;- x } x_norm } v &lt;- normalizar(v) peliculas &lt;- tibble(pelicula = peliculas_nom, v = v) |&gt; arrange(v) peliculas ## # A tibble: 10 × 2 ## pelicula v ## &lt;chr&gt; &lt;dbl&gt; ## 1 Amores Perros -0.420 ## 2 Billy Elliot -0.420 ## 3 Gladiator -0.210 ## 4 Scream -0.140 ## 5 Memento -0.0700 ## 6 Amelie -0.0700 ## 7 Lord of the Rings 0 ## 8 Mulholland drive 0.140 ## 9 Planet of the Apes 0.490 ## 10 X-Men 0.560 Ahora pensamos que tenemos con individuos con scores de qué tanto les gusta la ciencia ficción set.seed(102) u &lt;- rnorm(15, 0, 1) u &lt;- normalizar(u) personas &lt;- data_frame(persona = 1:15, u = u) ## Warning: `data_frame()` was deprecated in tibble 1.1.0. ## Please use `tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. head(personas) ## # A tibble: 6 × 2 ## persona u ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.0422 ## 2 2 0.183 ## 3 3 -0.316 ## 4 4 0.463 ## 5 5 0.289 ## 6 6 0.280 Podemos entonces construir la afinidad de cada persona por cada película (matriz \\(n\\times p\\) ) multiplicando el score de cada persona (en la dimensión ciencia ficción) por el peso de la película (en la dimensión ciencia ficción). Por ejemplo, para una persona, tenemos que su índice es personas$u[2] ## [1] 0.1832537 Esta persona tiene afinidad por la ciencia ficción, así que sus niveles de gusto por las películas son (multiplicando por \\(\\sigma = 100\\), que en este caso es una constante arbitraria seleccionada para el ejemplo): tibble(pelicula=peliculas$pelicula, afinidad= 100*personas$u[2]*peliculas$v) |&gt; arrange(desc(afinidad)) ## # A tibble: 10 × 2 ## pelicula afinidad ## &lt;chr&gt; &lt;dbl&gt; ## 1 X-Men 10.3 ## 2 Planet of the Apes 8.98 ## 3 Mulholland drive 2.57 ## 4 Lord of the Rings 0 ## 5 Memento -1.28 ## 6 Amelie -1.28 ## 7 Scream -2.57 ## 8 Gladiator -3.85 ## 9 Amores Perros -7.70 ## 10 Billy Elliot -7.70 Consideremos otra persona personas$u[15] ## [1] -0.05320133 Esta persona tiene disgusto ligero por la ciencia ficción, y sus scores de las películas son: tibble(pelicula = peliculas$pelicula, afinidad = 100 * personas$u[15] * peliculas$v) ## # A tibble: 10 × 2 ## pelicula afinidad ## &lt;chr&gt; &lt;dbl&gt; ## 1 Amores Perros 2.23 ## 2 Billy Elliot 2.23 ## 3 Gladiator 1.12 ## 4 Scream 0.745 ## 5 Memento 0.372 ## 6 Amelie 0.372 ## 7 Lord of the Rings 0 ## 8 Mulholland drive -0.745 ## 9 Planet of the Apes -2.61 ## 10 X-Men -2.98 Si fuera tan simple el gusto por las películas (simplemente depende si contienen ciencia ficción o no, y si a la persona le gusta o no), la matriz \\(X\\) de observaciones sería \\[X_1 = \\sigma uv^t\\] donde consideramos a \\(u\\) y \\(v\\) como vectores columna. El producto es de una matriz de \\(n\\times 1\\) contra una de \\(1\\times p\\), lo cual da una matriz de \\(n\\times p\\). Podemos calcular como: X = 100 * tcrossprod(personas$u, peliculas$v) # tcrossprod(x,y) da x %*% t(y) colnames(X) &lt;- peliculas$pelicula head(round(X, 1)) ## Amores Perros Billy Elliot Gladiator Scream Memento Amelie ## [1,] -1.8 -1.8 -0.9 -0.6 -0.3 -0.3 ## [2,] -7.7 -7.7 -3.8 -2.6 -1.3 -1.3 ## [3,] 13.3 13.3 6.6 4.4 2.2 2.2 ## [4,] -19.5 -19.5 -9.7 -6.5 -3.2 -3.2 ## [5,] -12.1 -12.1 -6.1 -4.0 -2.0 -2.0 ## [6,] -11.8 -11.8 -5.9 -3.9 -2.0 -2.0 ## Lord of the Rings Mulholland drive Planet of the Apes X-Men ## [1,] 0 0.6 2.1 2.4 ## [2,] 0 2.6 9.0 10.3 ## [3,] 0 -4.4 -15.5 -17.7 ## [4,] 0 6.5 22.7 25.9 ## [5,] 0 4.0 14.2 16.2 ## [6,] 0 3.9 13.7 15.7 O usando una tabla peliculas |&gt; crossing(personas) |&gt; mutate(afinidad = round(100 * u * v, 2)) |&gt; select(persona, pelicula, afinidad) |&gt; pivot_wider(names_from = pelicula, values_from = afinidad) ## # A tibble: 15 × 11 ## persona Amelie `Amores Perros` `Billy Elliot` Gladiator `Lord of the Rings` ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -0.3 -1.77 -1.77 -0.89 0 ## 2 2 -1.28 -7.7 -7.7 -3.85 0 ## 3 3 2.21 13.3 13.3 6.64 0 ## 4 4 -3.24 -19.5 -19.5 -9.73 0 ## 5 5 -2.02 -12.2 -12.2 -6.07 0 ## 6 6 -1.96 -11.8 -11.8 -5.89 0 ## 7 7 -1.47 -8.79 -8.79 -4.4 0 ## 8 8 -0.41 -2.49 -2.49 -1.24 0 ## 9 9 -0.9 -5.39 -5.39 -2.7 0 ## 10 10 -3.11 -18.7 -18.7 -9.34 0 ## 11 11 -2.36 -14.2 -14.2 -7.09 0 ## 12 12 -0.19 -1.13 -1.13 -0.57 0 ## 13 13 1.03 6.15 6.15 3.08 0 ## 14 14 2.08 12.4 12.4 6.23 0 ## 15 15 0.37 2.23 2.23 1.12 0 ## # … with 5 more variables: Memento &lt;dbl&gt;, Mulholland drive &lt;dbl&gt;, ## # Planet of the Apes &lt;dbl&gt;, Scream &lt;dbl&gt;, X-Men &lt;dbl&gt; Nótese que en este ejemplo podemos simplificar mucho el análisis: en lugar de ver la tabla completa, podemos simplemente considerar los dos vectores de índices (pesos y scores), y trabajar como si fuera un problema de una sola dimensión. 4.2 Aproximación con matrices de rango 1. En general, las matrices de datos reales no son de rango 1. Más bien nos interesa saber si se puede hacer una buena aproximación de rango 1. El problema que nos interesa es el inverso: si tenemos la tabla \\(X\\), ¿cómo sabemos si se puede escribir aproximadamente en la forma simple de una matriz de rango uno? Nótese que si lo pudiéramos hacer, esto simplificaría mucho nuestro análisis de estos datos, y obtendríamos información valiosa. Medimos la diferencia entre una matriz de datos general \\(X\\) y una matriz de rango 1 \\(\\sigma uv^t\\) mediante la norma Frobenius: \\[ ||X-\\sigma uv^t||^2_F = \\sum_{i,j} (X_{i,j} - \\sigma u_iv_j)^2\\] Nos interesa resolver \\[\\min_{\\sigma, u,v} || X - \\sigma uv^t ||_F^2\\] donde \\(\\sigma\\) es un escalar, \\(u\\) es un vector columna de tamaño \\(n\\) y \\(v\\) es un vector columna de tamaño \\(p\\). Suponemos que los vectores \\(u\\) y \\(v\\) tienen norma uno. Esto no es necesario - podemos absorber constantes en \\(\\sigma\\). Ejemplo Por ejemplo, la siguiente tabla tiene gastos personales en distintos rubros en distintos años para todo Estados Unidos (en dólares nominales). library(tidyverse) X_arr &lt;- USPersonalExpenditure[, c(1,3,5)] X_arr ## 1940 1950 1960 ## Food and Tobacco 22.200 59.60 86.80 ## Household Operation 10.500 29.00 46.20 ## Medical and Health 3.530 9.71 21.10 ## Personal Care 1.040 2.45 5.40 ## Private Education 0.341 1.80 3.64 En este ejemplo podríamos tener la intuición de que las proporciones de gasto se han mantenido aproximadamente constante en cada año, y que todos los rubros han aumentado debido a la inflación. Podríamos intentar hacer varias normalizaciones para probar esta idea, pero quisiéramos idear una estrategia general. Digamos que el vector \\(u\\) denota los niveles generales de cada rubro (es un vector de longitud 5), y el vector \\(v\\) denota los niveles generales de cada año (un vector de longitud 3). Queremos ver si es razonable aproximar \\[X\\approx uv^t\\] Observación: En este caso, la ecuación de arriba \\(X_{i,j} = u_iv_j\\) expresa que hay niveles generales para cada rubro \\(i\\) a lo largo de todos los años, y para obtener una aproximación ajustamos con un factor \\(v_j\\) de inflación el año \\(j\\) La mejor manera de entender este problema es con álgebra lineal, como veremos más adelante. Por el momento intentemos aproximar directamente, intentando resolver (podemos normalizar \\(u\\) y \\(v\\) más tarde y encontrar la \\(\\sigma\\)): \\[\\min_{u,v} \\sum_{i,j} (X_{i,j} - u_iv_j)^2 = \\min_{u,v} ||X-uv^t||^2_F\\] Observación:Este problema tiene varios mínimos, pues podemos mover constantes de \\(u\\) a \\(v\\) (tiene múltiples soluciones). Hay varias maneras de lidiar con esto (por ejemplo, normalizando). Por el momento, corremos la optimización para encontrar una solución: error &lt;- function(pars){ v &lt;- pars[1:3] u &lt;- pars[4:8] mean((X_arr - tcrossprod(u, v))^2) #tcrossprod da x %*% t(y) } optim_decomp &lt;- optim(rep(0.1, 5 + 3), error, method =&#39;BFGS&#39;) v_años &lt;- optim_decomp$par[1:3] u_rubros &lt;- optim_decomp$par[4:8] La matriz \\(X_1=uv^t\\) que obtuvimos es: X_1 &lt;- tcrossprod(u_rubros, v_años) round(X_1, 1) ## [,1] [,2] [,3] ## [1,] 21.6 58.4 87.8 ## [2,] 11.1 30.1 45.3 ## [3,] 4.7 12.6 18.9 ## [4,] 1.2 3.2 4.8 ## [5,] 0.8 2.2 3.3 Podemos ver qué tan buena es la aproximación: R &lt;- X_arr - X_1 qplot(as.numeric(X_1), as.numeric(as.matrix(X_arr))) + geom_abline(colour=&#39;red&#39;) round(R,2) ## 1940 1950 1960 ## Food and Tobacco 0.60 1.25 -0.98 ## Household Operation -0.65 -1.11 0.90 ## Medical and Health -1.12 -2.87 2.18 ## Personal Care -0.15 -0.77 0.55 ## Private Education -0.46 -0.38 0.36 donde vemos que nuestra aproximación es muy cercana a los datos en la tabla \\(X\\). La descomposición que obtuvimos es de la forma \\[X = uv^t + R\\] donde \\(R\\) tiene norma Frobenius relativamente chica. Observaciones: Este método nos da un ordenamiento de rubros de gasto según su nivel general, y un ordenamiento de años según su nivel general de gasto. tibble(rubro = rownames(X_arr), nivel = u_rubros) |&gt; arrange(desc(nivel)) ## # A tibble: 5 × 2 ## rubro nivel ## &lt;chr&gt; &lt;dbl&gt; ## 1 Food and Tobacco 9.74 ## 2 Household Operation 5.03 ## 3 Medical and Health 2.10 ## 4 Personal Care 0.538 ## 5 Private Education 0.363 tibble(año = colnames(X_arr), nivel = v_años) ## # A tibble: 3 × 2 ## año nivel ## &lt;chr&gt; &lt;dbl&gt; ## 1 1940 2.22 ## 2 1950 5.99 ## 3 1960 9.01 Pudimos explicar estos datos usando esos dos índices (5+3=7 números) en lugar de toda la tabla(5(3)=15 números). Una vez explicado esto, podemos concentrarnos en los patrones que hemos aislado en la matriz \\(R\\). Podríamos repetir buscando una aproximación igual a la que acabomos de hacer para la matriz \\(X\\), o podríamos hacer distintos tipos de análisis. 4.2.1 Suma de matrices de rango 1. La matriz de datos \\(X\\) muchas veces no puede aproximarse bien con una sola matriz de rango 1. Podríamos entonces buscar descomponer los datos en más de una dimensión latente: \\[X = \\sigma_1 u_1v_1^t + \\sigma_2 u_2v_2^t+\\ldots+ \\sigma_k u_kv_k^t\\] Interpretamos cada término del lado derecho de la misma manera (cada uno de ellos es una matriz de rango uno), y los valores finales son una suma ponderada de estos términos. Ejemplo: películas En nuestro ejemplo anterior, claramente debe haber otras dimensiones latentes que expliquen la afinidad por una película. Por ejemplo, quizá podríamos considerar el gusto por películas mainstream vs películas independientes. peliculas_nom &lt;- c(&#39;Gladiator&#39;,&#39;Memento&#39;,&#39;X-Men&#39;,&#39;Scream&#39;,&#39;Amores Perros&#39;, &#39;Billy Elliot&#39;, &#39;Lord of the Rings&#39;,&#39;Mulholland drive&#39;, &#39;Amelie&#39;,&#39;Planet of the Apes&#39;) # variable latente que describe el contenido de ciencia ficción de cada v_1 &lt;- c(-1.5, -0.5, 4, -1,-3, -3, 0, 1, -0.5, 3.5) v_2 &lt;- c(4.1, 0.2, 3.5, 1.5, -3.0, -2.5, 2.0, -4.5, -1.0, 2.6) #mainstream o no v_1 &lt;- normalizar(v_1) v_2 &lt;- normalizar(v_2) peliculas &lt;- tibble(pelicula = peliculas_nom, v_1 = v_1, v_2 = v_2) |&gt; arrange(v_2) peliculas ## # A tibble: 10 × 3 ## pelicula v_1 v_2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mulholland drive 0.140 -0.508 ## 2 Amores Perros -0.420 -0.338 ## 3 Billy Elliot -0.420 -0.282 ## 4 Amelie -0.0700 -0.113 ## 5 Memento -0.0700 0.0226 ## 6 Scream -0.140 0.169 ## 7 Lord of the Rings 0 0.226 ## 8 Planet of the Apes 0.490 0.293 ## 9 X-Men 0.560 0.395 ## 10 Gladiator -0.210 0.462 Y las personas tienen también scores en esta nueva dimensión, que aquí simulamos al azar personas &lt;- personas |&gt; mutate(u_1 = u, u_2 = normalizar(rnorm(15, 0, 1))) |&gt; select(-u) head(personas) ## # A tibble: 6 × 3 ## persona u_1 u_2 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.0422 -0.0623 ## 2 2 0.183 -0.377 ## 3 3 -0.316 0.420 ## 4 4 0.463 -0.401 ## 5 5 0.289 0.127 ## 6 6 0.280 0.180 Por ejemplo, la segunda persona persona le gusta la ciencia ficción, y pero prefiere fuertemente películas independientes. Podemos graficar a las personas según su interés en ciencia ficción y mainstream: ggplot(personas, aes(x = u_1, y=u_2)) + geom_point() + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) + xlab(&#39;Ciencia ficción&#39;)+ ylab(&#39;Mainstream&#39;) Y también podemos graficar las películas ggplot(peliculas, aes(x = v_1, y=v_2, label = pelicula)) + geom_point() + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;)+ xlab(&#39;Ciencia ficción&#39;)+ ylab(&#39;Mainstream&#39;) + geom_text() ¿Cómo calculariamos ahora la afinidad de una persona por una película? Necesitamos calcular (dando el mismo peso a las dos dimensiones) \\[X_{i,j} = \\sigma_1 u_{1,i} v_{1,j} + \\sigma_2 u_{2,i} v_{2,j}\\] Usamos la notación \\(u_{k,i}\\) para denotar la componente \\(i\\) del vector \\(u_k\\). Antes pusimos \\(\\sigma_1=100\\). Supongamos que la siguiente componente es un poco menos importante que la primera. Podriamos escoger \\(\\sigma_2=70\\), por ejemplo. Podríamos hacer library(purrr) library(stringr) personas_larga &lt;- personas |&gt; gather(dimension, u, u_1:u_2) |&gt; separate(dimension, c(&#39;x&#39;,&#39;dim&#39;), &#39;_&#39;) |&gt; select(-x) head(personas_larga) ## # A tibble: 6 × 3 ## persona dim u ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 1 0.0422 ## 2 2 1 0.183 ## 3 3 1 -0.316 ## 4 4 1 0.463 ## 5 5 1 0.289 ## 6 6 1 0.280 peliculas_larga &lt;- peliculas |&gt; gather(dimension, v, v_1:v_2) |&gt; separate(dimension, c(&#39;x&#39;,&#39;dim&#39;), &#39;_&#39;) |&gt; select(-x) head(peliculas_larga) ## # A tibble: 6 × 3 ## pelicula dim v ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Mulholland drive 1 0.140 ## 2 Amores Perros 1 -0.420 ## 3 Billy Elliot 1 -0.420 ## 4 Amelie 1 -0.0700 ## 5 Memento 1 -0.0700 ## 6 Scream 1 -0.140 sigma_df &lt;- data_frame(dim = c(&#39;1&#39;,&#39;2&#39;), sigma = c(100,70)) df_dim &lt;- personas_larga |&gt; left_join(peliculas_larga) |&gt; left_join(sigma_df) |&gt; mutate(afinidad = sigma*u*v) ## Joining, by = &quot;dim&quot; ## Joining, by = &quot;dim&quot; df_agg &lt;- df_dim |&gt; group_by(persona, pelicula) |&gt; summarise(afinidad = round(sum(afinidad),2)) ## `summarise()` has grouped output by &#39;persona&#39;. You can override using the `.groups` argument. df_agg |&gt; spread(pelicula, afinidad) ## # A tibble: 15 × 11 ## # Groups: persona [15] ## persona Amelie `Amores Perros` `Billy Elliot` Gladiator `Lord of the Rings` ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.2 -0.3 -0.54 -2.9 -0.98 ## 2 2 1.69 1.22 -0.27 -16.0 -5.95 ## 3 3 -1.1 3.33 4.99 20.2 6.63 ## 4 4 -0.07 -9.95 -11.5 -22.7 -6.34 ## 5 5 -3.03 -15.2 -14.7 -1.96 2.01 ## 6 6 -3.38 -16.0 -15.3 -0.06 2.84 ## 7 7 -1.07 -7.6 -7.8 -6.02 -0.79 ## 8 8 2.08 4.99 3.74 -11.5 -4.98 ## 9 9 0.95 0.17 -0.76 -10.3 -3.71 ## 10 10 -2.44 -16.6 -17.0 -12.1 -1.35 ## 11 11 -2.01 -13.1 -13.3 -8.52 -0.7 ## 12 12 -0.34 -1.6 -1.52 0.07 0.31 ## 13 13 0.7 5.17 5.33 4.42 0.66 ## 14 14 0.01 6.27 7.3 14.7 4.12 ## 15 15 -3.43 -9.17 -7.27 16.7 7.6 ## # … with 5 more variables: Memento &lt;dbl&gt;, Mulholland drive &lt;dbl&gt;, ## # Planet of the Apes &lt;dbl&gt;, Scream &lt;dbl&gt;, X-Men &lt;dbl&gt; Observación: Piensa qué harías si vieras esta tabla directamente, e imagina cómo simplificaría la comprensión y análisis si conocieras las matrices de rango 1 con las que se construyó este ejemplo. Consideremos la persona 2: filter(personas, persona == 2) ## # A tibble: 1 × 3 ## persona u_1 u_2 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 0.183 -0.377 Que tiene gusto por la ciencia ficción y le gustan películas independientes. Sus afinidades son: filter(df_agg, persona==2) |&gt; arrange(desc(afinidad)) ## # A tibble: 10 × 3 ## # Groups: persona [1] ## persona pelicula afinidad ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Mulholland drive 15.9 ## 2 2 Amelie 1.69 ## 3 2 Planet of the Apes 1.25 ## 4 2 Amores Perros 1.22 ## 5 2 X-Men -0.14 ## 6 2 Billy Elliot -0.27 ## 7 2 Memento -1.88 ## 8 2 Lord of the Rings -5.95 ## 9 2 Scream -7.03 ## 10 2 Gladiator -16.0 Explicaríamos así esta descomposición: Cada persona \\(i\\) tiene un nivel de gusto por ciencia ficción (\\(u_{1,i}\\)) y otro nivel de gusto por películas independientes (\\(u_{2,i}\\)). Cada película \\(j\\) tiene una calificación o peso en la dimensión de ciencia ficción (\\(v_{1,i}\\)) y un peso en la dimensión de independiente (\\(v_{2,i}\\)) La afinidad de una persona \\(i\\) por una película \\(j\\) se calcula como \\[ \\sigma_1 u_{1,i}v_{1,j} + \\sigma_2 u_{2,i}v_{2,j}\\] Una matriz de rango 2 es una suma (o suma ponderada) de matrices de rango 1 Las explicaciones de matrices de rango aplican para cada sumando (ver arriba) En este caso, hay dos dimensiones latentes que explican los datos: preferencia por independientes y preferencia por ciencia ficción. En este ejemplo ficticio estas componentes explica del todo a los datos. 4.3 Aproximación con matrices de rango bajo Nuestro problema generalmente es el inverso: si tenemos la matriz de datos \\(X\\), ¿podemos encontrar un número bajo \\(k\\) de dimensiones de forma que \\(X\\) se escribe (o aproxima) como suma de matrices de \\(k\\) matrices rango 1? Lograr esto sería muy bueno, pues otra vez simplificamos el análisis a solo un número de dimensiones \\(k\\) (muy) menor a \\(p\\), el número de variables, sin perder mucha información (con buen grado de aproximación). Adicionalmente, las dimensiones encontradas pueden mostrar patrones interesantes que iluminan los datos, esqpecialmente en términos de aquellas dimensiones que aportan mucho a la aproximación. En general, buscamos encontrar una aproximación de la matriz \\(X\\) mediante una suma de matrices de rango 1 \\[X \\approx \\sigma_1 u_1v_1^t + \\sigma_2 v_2v_2^t+\\ldots+ \\sigma_k u_kv_k^t.\\] A esta aproximación le llamamos una aproximación de rango \\(k\\). Hay muchas maneras de hacer esto, y probablemente la mayoría de ellas no son muy interesantes. Podemos más concretamente preguntar, ¿cuál es la mejor aproximación de rango \\(k\\) que hay? \\[\\min_{X_k} || X - X_k ||_F^2\\] donde consideramos la distancia entre \\(X\\) y \\(X_k\\) con la norma de Frobenius, que está definida por: \\[|| A ||_F^2 = \\sum_{i,j} a_{i,j}^2\\] y es una medida de qué tan cerca están las dos matrices \\(A\\) y \\(B\\), componente a componente. 4.3.1 Discusión: aproximación de rango 1. Empecemos resolviendo el problema más simple, que es \\[\\min_{\\sigma,u,v} || X - \\sigma uv^t ||_F^2\\] donde \\(\\sigma\\) es un escalar, \\(u\\) es un vector columna de tamaño \\(n\\) y \\(v\\) es un vector columna de tamaño \\(p\\). Suponemos que los vectores \\(u\\) y \\(v\\) tienen norma uno. El objetivo que queremos minimizar es \\[\\sum_{i,j} (X_{i,j} - \\sigma u_iv_j)^2\\] Derivando con respecto a \\(u_i\\) y \\(v_j\\), e igualando a cero, obtenemos (la sigma podemos quitarla en la derivada, pues multiplica todo el lado derecho): \\[\\frac{\\partial}{\\partial u_i} = -2\\sigma\\sum_{j} (X_{i,j} - \\sigma u_iv_j)v_j = 0\\] \\[\\frac{\\partial}{\\partial v_j} = -2\\sigma\\sum_{i} (X_{i,j} - \\sigma u_iv_j)u_i = 0\\] Que simplificando (y usando que la norma de \\(u\\) y \\(v\\) es igual a 1: \\(\\sum_iu_i^2 = \\sum_j v_j^2=1\\)) quedan: \\[\\sum_j X_{i,j}v_j = \\sigma u_i,\\] \\[\\sum_i X_{i,j}u_i =\\sigma v_j,\\] O en forma matricial \\[\\begin{equation} Xv = \\sigma u \\tag{4.1} \\end{equation}\\] \\[\\begin{equation} u^t X= \\sigma v^t. \\tag{4.2} \\end{equation}\\] Podemos resolver este par de ecuaciones para encontrar la solución al problema de optimización de arriba. Este problema tiene varias soluciones (con distintas \\(\\sigma\\)), pero veremos cómo podemos escoger la que de mejor la aproximación (adelanto: escoger las solución con \\(\\sigma^2\\) más grande). A un par de vectores \\((u,v)\\) que cumplen esta propiedad les llamamos vector propio izquierdo (\\(u\\)) y vector propio derecho (\\(v\\)), con valor singular asociado \\(\\sigma\\). Por convención, tomamos \\(\\sigma \\geq 0\\) (si no, podemos multiplicar a \\(u\\) por menos, por ejemplo). Y tenemos un resultado importante que nos será útil, y que explica el nombre de estos vectores: Si \\((u,v)\\) son vectores propios de \\(X\\) asociados a \\(\\sigma\\), entonces \\(v\\) es un vector propio de la matriz cuadrada \\(X^tX\\) (\\(p\\times p\\)) con valor propio \\(\\sigma^2\\). \\(u\\) es un vector propio de la matrix cuadrada \\(XX^t\\) (\\(n\\times n\\)) con valor propio \\(\\sigma^2\\). Observaciones: La demostración es fácil pues aplicando \\(X^t\\) a ambos lados de (4.1), obtenemos \\(X^t X v= \\sigma X^t u\\), que implica \\((X^t X) v= \\sigma (u^tX)^t = \\sigma^2 v\\). Podemos hacer lo mismo para (4.2). Nótese que \\(X^tX\\) es una matriz simétrica. Por el teorema espectral, existe una base ortogonal de vectores propios (usual) \\(v_1, v_2, \\ldots, v_p\\) con valores propios reales. Adicionalmente, como \\(X^tX\\) es positivo-definida, entonces todos estos vectores propios tienen valor propio no negativos. Ejemplo Verifiquemos en el ejemplo del gasto en rubros. Si comparamos \\(Xv\\) con \\(u\\), vemos que son colineales (es decir, \\(Xv=\\sigma u\\)): # qplot(Xv, u), si Xv=sigma*u entonces Xv y u deben ser proporcionales qplot(as.matrix(X_arr) %*% v_años, u_rubros) + geom_smooth(method=&#39;lm&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Y también # qplot(u^tX, v^t), si u^tXv=sigma*v entonces Xv y u deben ser proporcionales qplot(t(as.matrix(X_arr)) %*% u_rubros, (v_años) ) + geom_smooth(method=&#39;lm&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Ahora normalizamos \\(u\\) y \\(v\\) para encontrar \\(\\sigma\\): u_rubros_norm &lt;- normalizar(u_rubros) v_años_norm &lt;- normalizar(v_años) (as.matrix(X_arr) %*% v_años_norm)/u_rubros_norm ## [,1] ## Food and Tobacco 123.4858 ## Household Operation 123.4855 ## Medical and Health 123.4864 ## Personal Care 123.4891 ## Private Education 123.4799 Y efectivamente vemos que \\((u,v)\\) (normalizados) forman satisfacen las ecuaciones mostradas arriba, con \\(\\sigma\\) igual a: first((as.matrix(X_arr) %*% v_años_norm)/u_rubros_norm) ## [1] 123.4858 Si hay varias soluciones, ¿cuál \\(\\sigma\\) escogemos? Supongamos que encontramos dos vectores propios \\((u,v)\\) (izquierdo y derecho) con valor propio asociado \\(\\sigma\\). Podemos evaluar la calidad de la aproximación usando la igualdad \\[\\||A||_F^2 = traza (AA^t)\\] que es fácil de demostrar, pues la componente \\((i,i)\\) de \\(AA^t\\) está dada por el producto punto del renglon \\(i\\) de A por el renglón \\(i\\) de \\(A\\), que es \\(\\sum_{i,j}a_{i,j}^2.\\) Entonces tenemos que \\[||X-\\sigma uv^t||_F^2 = \\mathrm{Tr} ((X-\\sigma uv^t)(X-\\sigma uv^t)^t)\\] que es igual a \\[ \\mathrm{Tr} (XX^t) - 2\\sigma \\mathrm{Tr} ( X(vu^t)) + \\sigma^2\\mathrm{Tr}(uv^tvu^t)\\] Como \\(u\\) y \\(v\\) tienen norma 1, tenemos que \\(v^tv=1\\), y \\(\\textrm{Tr(uu^t)} = \\sum_i u_i^2 = 1\\). Adicionalmente, usando el hecho de que \\(Xv=\\sigma u\\) obtenemos \\[ ||X-\\sigma uv^t||_F^2 = \\mathrm{Tr} (XX^t) - \\sigma^2\\] que es una igualdad interesante: quiere decir que la mejor aproximación se encuentra encontrando el par de valores propios tal que el valor propio asociado \\(\\sigma\\) tiene el valor \\(\\sigma^2\\) más grande posible. La cantidad a la cantidad \\(\\mathrm{Tr} (XX^t)\\) está dada por \\[\\mathrm{Tr} (XX^t) = ||X||_F^2 = \\sum_{i,j} X_{i,j}^2,\\] que es una medida del “tamaño” de la matriz \\(X\\). 4.4 Interpetación de vectores singulares El resultado de arriba nos da una primera forma de interpretar las aproximaciones de rango uno. Nótese que por el argumento de arriba, tenemos que \\[Xv = \\sigma u\\] Y consideremos el lado izquierdo de esta ecuación: \\(Xv\\) son las variables originales de \\(X\\) ponderadas por los pesos del vector \\(v\\), de forma que \\(Xv\\) nos da una nueva variable derivada que combina linealmente las variables originales. Las proyecciones de los reglones o casos \\(x_i\\) de \\(X\\) podemos escribirlas también como (suponiendo \\(v\\) de norma 1): \\[(x_i\\cdot v) v= \\sigma u_iv\\] y su componente \\(j\\) es \\[(x_i\\cdot v) v_j= \\sigma u_iv_j\\] De aquí, vemos que la matriz de proyecciones de los puntos originales sobre \\(v\\) es la matriz \\[\\sigma uv^t\\] Si \\(u\\) y \\(v\\) son vectores singulares de \\(X\\), entonces El vector \\(\\sigma u\\) nos da las variables ponderadas de \\(X\\) por los pesos de las variables \\(v\\) La matriz de proyecciones de los renglones de \\(X\\) sobre el vector \\(v\\) es \\(\\sigma uv^t\\) Si escogemos el par \\(u,v\\) de vectores propios con la \\(\\sigma\\) más grande posible, entonces el vector \\(v\\) es la dirección tal que las proyecciones de los reglones de \\(X\\) sobre \\(v\\) son lo más cercanas posibles a los renglones originales \\(x_i\\) Ejemplo Consideremos unos datos simulados set.seed(3221) x_1 &lt;- rnorm(200,2, 1) x_2 &lt;- rnorm(200,0,1) + x_1 datos &lt;- data_frame(x_1, x_2) ggplot(datos, aes(x=x_1, y=x_2)) + geom_point() + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) Hacemos descomposición en valores singulares y extraemos la primera dimensión: svd_x &lt;- svd(datos) v &lt;- svd_x$v[,1] u &lt;- svd_x$u[,1] d &lt;- svd_x$d[1] #Nota: podemos mover signos para hacer las gráficas y la interpetación # más simples v &lt;- - v u &lt;- - u Graficamos ahora el vector \\(v\\), escalándolo para ver mejor cómo quedan en relación a los datos (esto no es necesario hacerlo): ggplot(datos) + geom_point(aes(x=x_1, y=x_2)) + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) + geom_segment(aes(xend= 4*v[1], yend=4*v[2], x=0, y=0), col=&#39;red&#39;, size=1.1, arrow = arrow(length = unit(0.3,&quot;cm&quot;))) El primer vector \\(v\\) es el “que pasa más cercano a los puntos,” en el sentido de que la distancia ortogonal entre los datos proyectados al vector \\(v\\) y los datos originales es lo más chica posible (mejor aproximación). La proyección de los datos sobre \\(v\\) es igual a \\(Xv=\\sigma_1 u\\), es decir, está dada por \\(\\sigma u\\), y la matriz de aproximaciones es \\(\\sigma uv^t\\) Podemos utilizar \\(u\\) como nuevas coordenadas de los reglones en el nuevo espacio a lo largo de \\(v\\). 4.4.1 Discusión: aproximaciones de rango más alto Vamos a repetir el análisis para dimensión 2, repitiendo el proceso que hicimos arriba. Denotamos como \\(u_1\\) y \\(v_1\\) los vectores \\(u\\) y \\(v\\) que encontramos en el paso anterior. Ahora buscamos minimizar \\[\\min_{u_2,v_2} || X - \\sigma_1 u_1 v_1^t - \\sigma_2 u_2 v_2^{t} ||_F^2\\] Repetimos el argumento de arriba y derivando respecto a las componentes de \\(u_2,v_2\\), y usando el hecho de que \\((u_1, v_1)\\) son vectores propios derecho e izquierdo asociados a \\(\\sigma_1\\), obtenemos: \\(v_2\\) es ortogonal a \\(v_1\\). \\(u_2\\) es ortogonal a \\(u_1\\). \\((u_2, v_2)\\) tienen que ser vectores propios derecho e izquierdo asociados a \\(\\sigma_2\\geq 0\\). Usando el hecho de que \\(v_1\\) y \\(v_2\\) son ortogonales, podemos podemos demostrar igual que arriba que \\[|| X - \\sigma_1 u_1 v_1^t - \\sigma_2 u_2 v_2^{t} ||_F^2 = \\textrm{Tr} (XX^t) - (\\sigma_1^2 + \\sigma_2^2) = ||X||_F^2 - (\\sigma_1^2 + \\sigma_2^2)\\] De modo que obtenemos la mejor aproximación escogiendo los dos valores de \\(\\sigma_1^2\\) y \\(\\sigma_2^2\\) más grandes para los que hay solución de (4.1) y (4.2) y Observaciones: Aunque aquí usamos un argumento incremental o greedy (comenzando con la mejor aproximación de rango 1), es posible demostrar que la mejor aproximación de rango 2 se puede construir de este modo. Ver por ejemplo estas notas. Vemos que la solución es incremental: \\(\\sigma_1, u_1, v_1\\) son los mismos que para la solución de dimensión 1. En dimensión 2, tenemos que buscar el siguiente valor singular más grande después de \\(\\sigma_1\\), de forma que tenemos \\(\\sigma_1^2 \\geq \\sigma_2^2\\). La solución entonces es agregar \\(\\sigma_2 u_2 v_2^t\\), donde \\((u_2,v_2)\\) es el par de vectores propios izquierdo y derecho. Ahora podemos enunciar nuestro teorema: Aproximación de matrices mediante valores singulares Sea \\(X\\) una matriz \\(n\\times p\\), y supongamos que \\(p\\leq n\\). Entonces, para cada \\(k \\leq p\\), la mejor aproximación de rango \\(k\\) a la matriz \\(X\\) se puede escribir como una suma \\(X_k\\) de \\(k\\) matrices de rango 1: \\[X_k = \\sigma_1 u_1v_1^t + \\sigma_2 u_2v_2^t + \\ldots \\sigma_k u_kv_k^t,\\] donde La calidad de la aproximación está dada por \\[||X-X_k||^2_F = ||X||^2_F - (\\sigma_1^2+ \\sigma_2^2 + \\cdots + \\sigma_k^2),\\] de forma que cada aproximación es sucesivamente mejor. \\(\\sigma_1^2 \\geq \\sigma_2^2 \\geq \\cdots \\geq \\sigma_k^2\\geq 0\\) Los vectores \\((u_i,v_i)\\) son un par de vectores propios izquierdo y derechos para \\(X\\) con valor singular \\(\\sigma_i\\). \\(v_1,\\ldots, v_k\\) son vectores ortogonales de norma 1 \\(u_1,\\ldots, u_k\\) son vectores ortogonales de norma 1 Observaciones: Normalmente no optimizamos como hicimos en el ejemplo de la matriz de gastos para encontrar las aproximación de rango bajo, sino que se usan algoritmos para encontrar vectores propios de \\(X^tX\\) (que son las \\(v\\)’s), o más generalmente algoritmos basados en álgebra lineal que intentan encontrar directamente los pares de vectores (u_i, v_i), y otros algoritmos numéricos (por ejemplo, basados en iteraciones). Un resultado interesante (que faltaría por demostrar) es que si tomamos la aproximación de rango \\(p\\) (cuando \\(p\\leq n\\)), obtenemos que \\[X= \\sigma_1 u_1v_1^t + \\sigma_2 u_2v_2^t + \\ldots \\sigma_p u_pv_p^t\\] es decir, la aproximación es exacta. Esto es un fraseo del teorema de descomposición en valores singulares, que normalmente se expresa de otra forma (ver más adelante). Ejemplo Consideremos el ejemplo de los gastos. Podemos usar la función svd de R svd_gasto &lt;- svd(X_arr) El objeto de salida contiene los valores singulares (en d). Nótese que ya habíamos calculado por fuerza bruta el primer valor singular: sigma &lt;- svd_gasto$d sigma ## [1] 123.4857584 4.5673718 0.3762533 Los vectores \\(v_1,v_2,v_3\\) (pesos de las variables) en nuestras tres nuevas dimensiones, que son las columnas de v &lt;- svd_gasto$v rownames(v) &lt;- colnames(X_arr) v ## [,1] [,2] [,3] ## 1940 -0.2007388 -0.3220495 -0.92519623 ## 1950 -0.5423269 -0.7499672 0.37872247 ## 1960 -0.8158342 0.5777831 -0.02410854 y los vectores \\(u_1,u_2,u_3\\), que son los scores de los rubros en cada dimensión dim(svd_gasto$u) ## [1] 5 3 u &lt;- (svd_gasto$u) rownames(u) &lt;- rownames(X_arr) u ## [,1] [,2] [,3] ## Food and Tobacco -0.87130286 -0.3713244 -0.1597823 ## Household Operation -0.44966139 0.3422116 0.4108311 ## Medical and Health -0.18778444 0.8259030 -0.2584369 ## Personal Care -0.04812680 0.2074885 -0.4372590 ## Private Education -0.03250802 0.1408623 0.7400691 Podemos considerar ahora la segunda dimensión que encontramos. En los scores: \\(u_2\\) tiene valores altos en el rubro 3 (salud), y valores negativos en rubro 1. Es un patrón de gasto más alto en todo menos en comida (que es el rubro 1), especialmente en salud. Ahora vemos \\(v_2\\): tiene un valor alto en el año 60 (3a entrada), y valores más negativos para los dos primeros años (40 y 50) Así que decimos que en los 60, el ingreso se desplazó hacia salud (y otros rubros en general), reduciéndose el de comida. Si multiplicamos podemos ver la contribución de esta matriz de rango 1 (en billones (US) de dólares): d &lt;- svd_gasto$d (d[2]*tcrossprod(svd_gasto$u[,2], svd_gasto$v[,2])) |&gt; round(1) ## [,1] [,2] [,3] ## [1,] 0.5 1.3 -1.0 ## [2,] -0.5 -1.2 0.9 ## [3,] -1.2 -2.8 2.2 ## [4,] -0.3 -0.7 0.5 ## [5,] -0.2 -0.5 0.4 Este es un efecto relativamente chico (comparado con el patrón estable de la primera dimensión), pero ilumina todavía un aspecto adicional de esta tabla. La norma de la diferencia entre la matriz \\(X\\) y la aproximación de rango 2 podemos calcularla de dos maneras: sum(X_arr^2) - sum(d[1:2]^2) ## [1] 0.1415665 O calculando la aproximación y la diferencia directamente. Podemos hacerlo de la siguiente forma X_arr_2 &lt;- d[1]*tcrossprod(u[,1], v[,1]) + d[2]*tcrossprod(u[,2], v[,2]) sum((X_arr - X_arr_2)^2) ## [1] 0.1415665 Pero podemos calcular la aproximación \\(X_2\\) en forma matricial, haciendo X_arr_2 &lt;- u[,1:2] %*% diag(d[1:2]) %*% t(v[,1:2]) sum((X_arr - X_arr_2)^2) ## [1] 0.1415665 4.5 Descomposición en valores singulares (SVD o DVS) Aunque ya hemos enunciado los resultados, podemos enunciar el teorema de descomposición en valores singulares en términos matriciales. Supongamos entonces que tenemos una aproximación de rango \\(k\\) \\[X_k = \\sigma_1 u_1v_1^t + \\sigma_2 u_2v_2^t + \\ldots \\sigma_k u_kv_k^t\\] Se puede ver que esta aproximación se escribe como (considera todos los vectores como vectores columna) \\[ X_k = (u_1,u_2, \\ldots, u_k) \\left( {\\begin{array}{ccccc} \\sigma_1 &amp; 0 &amp; \\cdots &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2 &amp; 0 &amp;\\cdots &amp; 0 \\\\ \\vdots &amp; &amp; &amp; \\vdots\\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\sigma_k \\\\ \\end{array} } \\right) \\left ( \\begin{array}{c} v_1^t \\\\ v_2^t \\\\ \\vdots \\\\ v_k^t \\end{array} \\right)\\] o más simplemente, como \\[X_k = U_k \\Sigma_k V_k^t\\] donde \\(U_k\\) (\\(n\\times k\\)) contiene los vectores \\(u_i\\) en sus columnas, \\(V_k\\) (\\(k\\times p\\)) contiene los vectores \\(v_j\\) en sus columnas, y la matriz \\(\\Sigma_k\\) es la matriz diagonal con los primeros \\(\\sigma_1\\geq \\sigma_2\\geq\\cdots \\sigma_k\\) valores singulares. Ver el ejemplo anterior para ver cómo los cálculos son iguales. Descomposición en valores singulares Sea \\(X\\) una matriz de \\(n\\times p\\) con \\(p\\leq n\\). Entonces existe una factorización \\[X=U\\Sigma V^t,\\] \\(\\Sigma\\) es una matriz diagonal con valores no-negativos (valores singulares). Los valores singulares de \\(\\Sigma\\) estan ordenados en orden decreciente. Las columnas de U y V son vectores ortogonales unitarios. La i-ésima columna \\(u_i\\) de \\(V\\) y la i-ésima columna \\(v_i\\) de \\(V\\) son pares de vectores propios \\((u_i, v_i)\\) izquierdo y derecho de \\(X\\) con valor singular \\(\\sigma_i = \\Sigma_{i,i}\\) Una vez que tenemos esta descomposición, podemos extraer la aproximación que nos sea útil: una aproximación \\(X_k\\) de orden \\(k\\) se escribe como \\[X_k = U_k\\Sigma_k V_k^t\\] donde \\(U_k\\) contiene las primeras \\(k\\) columnas de \\(U\\), \\(V_k\\) las primeras \\(k\\) columnas de \\(V\\), y \\(\\Sigma_k\\) es la submatriz cuadrada \\(k\\times k\\) de los primeros \\(k\\) renglones y columnas de \\(\\Sigma\\) : knitr::include_graphics(&quot;images/svd.png&quot;) Frecuenta el teorema de aproximación óptima (teorema de Ekhart-Young) se deriva de la descomposición en valores singulares, que se demuestra antes usando técnicas de álgebra lineal. 4.6 Más de interpretación geométrica Proyecciones Los vectores \\(v_1,v_2, \\ldots, v_p\\) están en el espacio de variables o columnas (son de dimensión \\(p\\)). La componente de la proyección (ver proyección de vectores ) de la matriz de datos sobre una de estas dimensiones está dada por \\[Xv_j,\\] que son iguales a los scores de los casos escalados por \\(\\sigma\\): \\[\\sigma_j u_j\\]. Las proyecciones \\(d_j = \\sigma_j u_j\\) son las variables que normalmente se usan para hacer análisis posterior, aunque cuando la escala de las proyecciones no es importante, también se pueden usar simplemente las \\(u_j\\). Por ejemplo, la projeccion del rengón \\(x_i\\) de la matriz \\(X\\) es \\((x_i\\cdot v_j) v_j\\) (nótese que \\(x_i \\cdot v_j\\) es un escalar, la componente de la proyección). Consideremos nuestro ejemplo anterior: set.seed(3221) x_1 &lt;- rnorm(200,2, 1) x_2 &lt;- rnorm(200,0,1) + x_1 datos &lt;- data_frame(x_1, x_2) ggplot(datos, aes(x=x_1, y=x_2)) + geom_point() + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) Hacemos descomposición en valores singulares y graficamos svd_x &lt;- svd(datos) v &lt;- svd_x$v |&gt; t() |&gt; as_tibble(.name_repair = NULL) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. u &lt;- svd_x$u |&gt; as_tibble(.name_repair = NULL) colnames(v) &lt;- c(&#39;x_1&#39;,&#39;x_2&#39;) colnames(u) &lt;- c(&#39;x_1&#39;,&#39;x_2&#39;) d &lt;- svd_x$d #Nota: podemos mover signos para hacer las gráficas y la interpetación # más simples v[,1] &lt;- - v[,1] u[,1] &lt;- - u[,1] v ## # A tibble: 2 × 2 ## x_1 x_2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.673 -0.740 ## 2 0.740 0.673 Graficamos ahora los dos vectores \\(v_1\\) y \\(v_2\\), escalándolos para ver mejor cómo quedan en relación a los datos (esto no es necesario hacerlo): ggplot(datos) + geom_point(aes(x=x_1, y=x_2)) + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) + geom_segment(data = v, aes(xend= 4*x_1, yend=4*x_2, x=0, y=0), col=&#39;red&#39;, size=1.1, arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + coord_equal() El primer vector es el “que pasa más cercano a los puntos,” en el sentido de que la distancia entre los datos proyectados al vector y los datos es lo más chica posible (mejor aproximación). La proyección de los datos sobre \\(v\\) es igual a \\(Xv_1=\\sigma_1 u_1\\), es decir, está dada por \\(\\sigma u_1\\) Las proyecciones de los datos sobre el segundo vector \\(v_2\\) están dadas igualmente por \\(\\sigma_2 u_2\\). Sumamos esta proyección a la de la primera dimensión para obtener una mejor aproximación a los datos (en este caso, exacta). Por ejemplo, seleccionemos el primer punto y obtengamos sus proyecciones: proy_1 &lt;- (d[1])*as.numeric(u[1,1])*v$x_1 #v_1 por el score en la dimensión 1 u[1,1] proy_2 &lt;- (d[2])*as.numeric(u[1,2])*v$x_2 #v_2 por el score en la dimensión 1 u[1,1] proy_2 + proy_1 ## [1] 3.030313 1.883698 datos[1,] ## # A tibble: 1 × 2 ## x_1 x_2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3.03 1.88 Podemos graficar la aproximación sucesiva: datos$selec &lt;- c(&#39;seleccionado&#39;, rep(&#39;no_seleccionado&#39;, nrow(datos)-1)) ggplot(datos) + geom_point(aes(x=x_1, y=x_2, colour=selec, size=selec)) + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) + geom_segment(aes(xend= proy_1[1], yend=proy_1[2], x=0, y=0), col=&#39;red&#39;, size=1.1, arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + geom_segment(aes(xend= proy_2[1] + proy_1[1], yend=proy_2[2] + proy_1[2], x=proy_1[1], y=proy_1[2]), col=&#39;red&#39;, size=1.1, arrow = arrow(length = unit(0.2,&quot;cm&quot;))) + coord_equal() Las aproximaciones de la descomposión en valores singulares mediante matrices de rango 1 puede entenderse como la búsqueda sucesiva de subespacios de dimensión baja, donde al proyectar los datos perdemos poca información. Las proyecciones sucesivas se hacen sobre vectores ortogonales, y en este sentido la DVS separa la información en partes que no tienen contenido común (desde el punto de vista lineal). Finalmente, muchas veces graficamos las proyecciones en el nuevo espacio creado por las dimensiones de la DVS (nótese la escala distinta de los ejes). proyecciones &lt;- data_frame(dim_1 = d[1]*u$x_1, dim_2 = d[2]*u$x_2, selec = datos$selec) ggplot(proyecciones, aes(x = dim_1, y = dim_2, size=selec, colour=selec)) + geom_point() ## Warning: Using size for a discrete variable is not advised. 4.7 SVD para películas de netflix Vamos a intentar encontrar dimensiones latentes para los datos del concurso de predicción de Netflix (una de las componentes de las soluciones ganadoras fue descomposición en valores singulares). ruta &lt;- &quot;../datos/netflix/dat_muestra_nflix.csv&quot; url &lt;- &quot;https://s3.amazonaws.com/ma-netflix/dat_muestra_nflix.csv&quot; if(!file.exists(ruta)){ evals_tbl &lt;- read_csv(url, col_types = &quot;iii&quot;) write_csv(evals_tbl, ruta) } else { evals_tbl &lt;- read_csv(ruta, col_types = &quot;iii&quot;) } evals_tbl ## # A tibble: 20,968,941 × 5 ## peli_id usuario_id_orig calif fecha usuario_id ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;dbl&gt; ## 1 1 2442 3 2004-04-14 1 ## 2 1 1086807 3 2004-12-28 2 ## 3 1 2165002 4 2004-04-06 3 ## 4 1 1133214 4 2004-03-07 4 ## 5 1 1537427 4 2004-03-29 5 ## 6 1 525356 2 2004-07-11 6 ## 7 1 1910569 4 2004-04-12 7 ## 8 1 2421815 2 2004-02-26 8 ## 9 1 2508819 3 2004-05-18 9 ## 10 1 1342007 3 2004-07-16 10 ## # … with 20,968,931 more rows peliculas_nombres &lt;- read_csv(&quot;../datos/netflix/movies_title_fix.csv&quot;, col_names = c(&quot;peli_id&quot;, &quot;year&quot;, &quot;name&quot;) ) ## Rows: 17770 Columns: 3 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): year, name ## dbl (1): peli_id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. peliculas_nombres ## # A tibble: 17,770 × 3 ## peli_id year name ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 2003 Dinosaur Planet ## 2 2 2004 Isle of Man TT 2004 Review ## 3 3 1997 Character ## 4 4 1994 Paula Abdul&#39;s Get Up &amp; Dance ## 5 5 2004 The Rise and Fall of ECW ## 6 6 1997 Sick ## 7 7 1992 8 Man ## 8 8 2004 What the #$*! Do We Know!? ## 9 9 1991 Class of Nuke &#39;Em High 2 ## 10 10 2001 Fighter ## # … with 17,760 more rows Hay muchas peliculas que no son evaluadas por ningún usuario. Aquí tenemos que decidir cómo tratar estos datos: si los rellenamos con 0, la implicación es que un usuario tiene bajo interés en una película que no ha visto. Hay otras opciones (y quizá un método que trate apropiadamente los datos faltantes es mejor). library(Matrix) library(irlba) if(TRUE){ evals_tbl &lt;- evals_tbl |&gt; group_by(usuario_id) |&gt; mutate(calif_centrada = calif - mean(calif)) |&gt; ungroup() #Usamos matriz rala, de otra manera la matriz es demasiado grande evals_mat &lt;- sparseMatrix(i = as.integer(evals_tbl$usuario_id), j = as.integer(evals_tbl$peli_id), x = evals_tbl$calif_centrada) set.seed(81) svd_parcial &lt;- irlba(evals_mat, 6) } svd_parcial$d ## [1] 917.5007 598.4652 392.0046 342.3649 333.8895 327.0080 #no correr en notas V_peliculas &lt;- svd_parcial$v |&gt; as_tibble(.name_repair = NULL) |&gt; mutate(peli_id = row_number()) |&gt; left_join(peliculas_nombres |&gt; mutate(pelicula_id = peli_id)) ## Joining, by = &quot;peli_id&quot; U_usuarios &lt;- svd_parcial$u |&gt; as_tibble(.name_repair = NULL) Examinamos la primera y segunda componente en los pesos de las películas \\(V\\): library(ggrepel) pel_graf &lt;- V_peliculas |&gt; mutate(dist_0 = sqrt(V1^2 + V2^2)) muestra &lt;- pel_graf |&gt; mutate(etiqueta = name) |&gt; mutate(etiqueta = ifelse(dist_0 &gt; 0.05, name, &#39;&#39;)) ggplot(muestra, aes(x = V1, y = V2, label=etiqueta)) + geom_point(alpha = 0.15) + geom_text_repel(size = 2.0, max.overlaps = 12) ¿Qué nombres pondrías a estas dimensiones? Puedes examinar otras. 4.7.1 Calidad de representación de SVD. Podemos hacer varios cálculos para entender qué tan buena es nuestra aproximación de rango bajo \\(X_k\\). Por ejemplo, podríamos calcular las diferencias de \\(X-X_k\\) y presentarlas de distinta forma. Ejemplo En el ejemplo de rubros de gasto, podríamos mostrar las diferencias en billones (us) de dólares, donde vemos que la aproximación es bastante buena qplot(as.numeric(X_arr-X_arr_2)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Que podríamos resumir, por ejemplo, con la media de errores absolutos: mean(abs(as.numeric(X_arr-X_arr_2))) ## [1] 0.06683576 Otra opción es usar la norma Frobenius, calculando para la apoximación de rango 1 1 - (sum(X_arr^2) - sum(svd_gasto$d[1]^2))/sum(X_arr^2) ## [1] 0.9986246 Lo que indica que capturamos 99.8% de la información, y para la de rango 2: d 1-(sum(X_arr^2) - sum(svd_gasto$d[1:2]^2))/sum(X_arr^2) ## [1] 0.9999907 Lo que indica que estos datos (en 3 variables), podemos entenderlos mediante un análisis de dos dimensiones Podemos medir la calidad de la representación de \\(X\\) (\\(n\\times p\\) con \\(p &lt; n\\)) de una aproximación \\(X_k\\) de SVD mediante \\[1-\\frac{||X-X_k||_F^2}{||X||_F^2} = \\frac{\\sigma_1^2 + \\sigma_2^2 + \\cdots \\sigma_k^2}{\\sigma_1^2 + \\sigma_2^2 + \\cdots \\sigma_p^2},\\] que es un valor entre 0 y 1. Cuanto más cercana a 1 está, mejor es la representación. Observaciones: Dependiendo de nuestro objetivo, nos interesa alcanzar distintos niveles de calidad de representación. Por ejemplo, algunas reglas de dedo: Si queremos usar los datos para un proceso posterior, o dar una descripción casi completa de los datos, quizá buscamos calidad \\(&gt;0.9\\) o mayor. Si nos interesa extraer los patrones más importantes, podemos considerar valores de calidad mucho más chicos, entendiendo que hay una buena parte de la información que no se explica por nuestra aproximación. 4.8 Componentes principales Componentes principales es la descomposición en valores singulares aplicada a una matriz de datos centrada por columna. Esta operación convierte el problema de aproximación de matrices de rango bajo en uno de aproximaciones que buscan explicar la mayoría de la varianza (incluyendo covarianza) de las variables de la matriz de datos \\(X\\). Consideremos entonces una matriz de datos \\(X\\) de tamaño \\(n\\times p\\). Definimos la matrix centrada por columna \\(\\tilde{X}\\) , que se calcula como \\[\\tilde{X}_{i,j} = X_{i,j} - \\mu_j\\] donde \\(\\mu_j = \\frac{1}{n} \\sum_j X_{i,j}\\). - La diferencia en construcción entre Svd y Svd con columnas centradas (componentes principales) es que en Svd las proyecciones se hacen pasando por el origen, pero en componentes principales se hacen a partir del centroide de los datos. ### Ejemplo {-} Veamos primero el último ejemplo simulado que hicimos anterioremnte. Primero centramos los datos por columna: datos_c &lt;- scale(datos |&gt; select(-selec), scale = FALSE) |&gt; as_tibble() ggplot(datos_c, aes(x=x_1, y=x_2)) + geom_point() + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) Y ahora calculamos la descomposición en valores singulares svd_x &lt;- svd(datos_c) v &lt;- t(svd_x$v) |&gt; as_tibble() u &lt;- svd_x$u |&gt; as_tibble() colnames(v) &lt;- c(&#39;x_1&#39;,&#39;x_2&#39;) colnames(u) &lt;- c(&#39;x_1&#39;,&#39;x_2&#39;) d &lt;- svd_x$d v ## # A tibble: 2 × 2 ## x_1 x_2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.507 0.862 ## 2 0.862 -0.507 Notemos que los resultados son similares, pero no son los mismos. Graficamos ahora los dos vectores \\(v_1\\) y \\(v_2\\), que en este contexto se llaman direcciones principales ggplot(datos_c) + geom_point(aes(x=x_1, y=x_2)) + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) + geom_segment(data = v, aes(xend= 5*x_1, yend=5*x_2, x=0, y=0), col=&#39;red&#39;, size=1.1, arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + coord_equal() Las componentes de las proyecciones de los datos sobre las direcciones principales dan las componentes principales (nótese que multiplicamos por los valores singulares): head(svd_x$u %*% diag(svd_x$d)) ## [,1] [,2] ## [1,] 0.3230641 0.9257829 ## [2,] 0.4070429 1.5360770 ## [3,] -1.2788977 -0.2762829 ## [4,] 0.8910247 0.4071926 ## [5,] -4.4466993 -0.5743111 ## [6,] -1.2267878 0.3470759 Que podemos graficar comps &lt;- svd_x$u %*% diag(svd_x$d) |&gt; as_tibble() ggplot(comps, aes(x=V1, y=V2)) + geom_point()+ geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) Este resultado lo podemos obtener directamente usando la función princomp comp_principales &lt;- princomp(datos |&gt; select(-selec)) scores &lt;- comp_principales$scores head(scores) ## Comp.1 Comp.2 ## [1,] 0.3230641 0.9257829 ## [2,] 0.4070429 1.5360770 ## [3,] -1.2788977 -0.2762829 ## [4,] 0.8910247 0.4071926 ## [5,] -4.4466993 -0.5743111 ## [6,] -1.2267878 0.3470759 Y verificamos que los resultados son los mismos: qplot(scores[,1], comps$V1) qplot(scores[,2], -comps$V2) Varianza en componentes principales Cuando centramos por columna, la svd es un tipo de análisis de la matriz de varianzas y covarianzas de la matriz \\(X\\), dada por \\[C = \\frac{1}{n} \\tilde{X}^t \\tilde{X}\\] (Nota: asegúrate de que entiendes por qué esta es la matriz de varianzas y covarianzas de \\(X\\)). Nótese que las proyecciones (que se llaman componentes principales) \\(\\tilde{X}v_j = \\sigma_j u_j = d_j\\) satisfacen que 1. La media de las proyecciones \\(d_j\\) es igual a cero Pues \\[\\sigma_j \\sum_k {u_{j,k}} = \\sum_k \\sum_i (\\tilde{X}_{k,i})v_{j,i} = \\sum_i v_{j,i}\\sum_k (\\tilde{X}_{k,i}) = 0,\\] pues las columnas de \\(\\tilde{X}\\) tienen media cero. 2- \\(\\sigma_j^2/n\\) es la varianza de la proyección \\(d_j\\), pues \\[Var(d_j) = \\frac{\\sigma_j^2}{n} \\sum_k (u_{j,k} - 0)^2 = \\frac{\\sigma_j^2}{n},\\] y el vector \\(u_j\\) tienen norma 1. 3. La ortogonalidad de los vectores \\(u_j\\) se interpreta ahora en términos de covarianza: \\[Cov(d_i,d_j) = \\frac{1}{n}\\sum_{k=1}^n (d_{i,k}-0)(d_{j,k}-0) = \\frac{1}{n}\\sum_{k=1}^n \\sigma_j\\sigma_i u_{i,k}u_{j,k} = 0\\] Así que Buscamos sucesivamente direcciones para proyectar que tienen varianza máxima (ver ejemplo anterior), y que sean no correlacionadas de forma que no compartan información lineal entre ellas. Adicionalmente, vimos que podíamos escribir \\[||\\tilde{X}||^2_F = \\sum_{j=1}^p \\sigma_{j}^2\\] Y el lado izquierdo es en este caso una suma de varianzas: \\[\\sum_{j=1}^p Var(X_j) = \\sum_{j=1}^p \\sigma_{j}^2.\\] El lado izquierdo se llama Varianza total de la matriz \\(X\\). Componentes principales particiona la varianza total de la matriz \\(X\\) en componentes. ¿Centrar o no centrar por columna? Típicamente, antes de aplicar SVD hacemos algunos pasos de procesamiento de las variables. En componentes principales, este paso de procesamiento es centrar la tabla por columnas. Conviene hacer esto cuando: Centramos si las medias de las columnas no tienen información importante o interesante para nuestros propósitos - es mejor eliminar esta parte de variación desde el principio para no lidiar con esta información en las dimensiones que obtengamos. En otro caso, quizá es mejor no centrar. Centramos si nos interesa más tener una interpretación en términos de varianzas y covarianzas que hacer una aproximación de los datos originales. Sin embargo, también es importante notar que muchas veces los resultados de ambos análisis son similares en cuanto a interpretación y en cuanto a usos posteriores de las dimensiones obtenidas. Pueden ver análisis detallado en este artículo, que hace comparaciones a lo largo de varios conjuntos de datos. Ejemplo: resultados similares En el ejemplo de gasto en rubros que vimos arriba, los pesos \\(v_j\\) son muy similares: comps_1 &lt;- princomp(USPersonalExpenditure[,c(1,3,5)]) svd_1 &lt;- svd(USPersonalExpenditure[,c(1,3,5)]) comps_1$loadings[,] ## Comp.1 Comp.2 Comp.3 ## 1940 0.2099702 0.2938755 0.93249650 ## 1950 0.5623341 0.7439168 -0.36106546 ## 1960 0.7998081 -0.6001875 0.00905589 svd_1$v ## [,1] [,2] [,3] ## [1,] -0.2007388 -0.3220495 -0.92519623 ## [2,] -0.5423269 -0.7499672 0.37872247 ## [3,] -0.8158342 0.5777831 -0.02410854 comps_1$scores ## Comp.1 Comp.2 Comp.3 ## Food and Tobacco 68.38962 0.8783065 0.06424607 ## Household Operation 16.25334 -0.9562769 -0.16502902 ## Medical and Health -16.13276 -2.2900370 0.07312028 ## Personal Care -33.29512 1.0003211 0.23036177 ## Private Education -35.21507 1.3676863 -0.20269910 svd_1$u %*% diag(svd_1$d) ## [,1] [,2] [,3] ## [1,] -107.593494 -1.6959766 -0.06011861 ## [2,] -55.526778 1.5630078 0.15457655 ## [3,] -23.188704 3.7722060 -0.09723774 ## [4,] -5.942974 0.9476773 -0.16452015 ## [5,] -4.014277 0.6433704 0.27845344 Llegaríamos a conclusiones similares si interpretamos cualquiera de los dos análisis (verifica por ejemplo el ordenamiento de rubros y años en cada dimensión). Ejemplos: donde es buena idea centrar Por ejemplo, si hacemos componentes principales con los siguientes datos: whisky &lt;- read_csv(&#39;../datos/whiskies.csv&#39;) head(whisky) ## # A tibble: 6 × 17 ## RowID Distillery Body Sweetness Smoky Medicinal Tobacco Honey Spicy Winey ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Aberfeldy 2 2 2 0 0 2 1 2 ## 2 02 Aberlour 3 3 1 0 0 4 3 2 ## 3 03 AnCnoc 1 3 2 0 0 2 0 0 ## 4 04 Ardbeg 4 1 4 4 0 0 2 0 ## 5 05 Ardmore 2 2 2 0 0 1 1 1 ## 6 06 ArranIsleOf 2 3 1 1 0 1 1 1 ## # … with 7 more variables: Nutty &lt;dbl&gt;, Malty &lt;dbl&gt;, Fruity &lt;dbl&gt;, ## # Floral &lt;dbl&gt;, Postcode &lt;chr&gt;, Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt; whisky_sabor &lt;- whisky |&gt; select(Body:Floral) comp_w &lt;- princomp(whisky_sabor) Veamos los pesos de las primeras cuatro dimensiones round(comp_w$loadings[, 1:4], 2) ## Comp.1 Comp.2 Comp.3 Comp.4 ## Body 0.36 0.49 0.03 0.07 ## Sweetness -0.20 0.05 -0.26 0.37 ## Smoky 0.48 0.07 0.22 -0.09 ## Medicinal 0.58 -0.16 0.04 -0.08 ## Tobacco 0.09 -0.02 0.00 0.03 ## Honey -0.22 0.42 0.11 -0.03 ## Spicy 0.06 0.18 0.70 0.17 ## Winey -0.04 0.64 -0.23 0.23 ## Nutty -0.05 0.26 -0.18 -0.85 ## Malty -0.13 0.10 0.11 -0.07 ## Fruity -0.20 0.12 0.40 -0.09 ## Floral -0.38 -0.13 0.34 -0.15 La primera componente separa whisky afrutado/floral/dulce de los whishies ahumados con sabor medicinal. La segunda componente separa whiskies con más cuerpo, características de vino y miel de otros más ligeros. Las siguientes componentes parece oponer Spicy contra Fruity y Floral, y la tercera principalmente contiene la medición de Nutty. Según vimos arriba, podemos ver que porcentaje de la varianza explica cada componente summary(comp_w) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## Standard deviation 1.5268531 1.2197972 0.86033607 0.79922719 0.74822104 ## Proportion of Variance 0.3011098 0.1921789 0.09560193 0.08250322 0.07230864 ## Cumulative Proportion 0.3011098 0.4932887 0.58889059 0.67139381 0.74370245 ## Comp.6 Comp.7 Comp.8 Comp.9 Comp.10 ## Standard deviation 0.6811330 0.62887454 0.59593956 0.52041611 0.49757158 ## Proportion of Variance 0.0599231 0.05108089 0.04587064 0.03498097 0.03197728 ## Cumulative Proportion 0.8036256 0.85470644 0.90057708 0.93555805 0.96753533 ## Comp.11 Comp.12 ## Standard deviation 0.42174644 0.271073661 ## Proportion of Variance 0.02297382 0.009490848 ## Cumulative Proportion 0.99050915 1.000000000 Y vemos que las primeras dos componentes explican casi el 50% de la varianza. Las siguientes componentes aportan relativamente pooca varianza comparada con la primera Podemos graficar los whiskies en estas dos dimensiones: library(ggrepel) scores_w &lt;- comp_w$scores |&gt; as_tibble() scores_w$Distillery &lt;- whisky$Distillery ggplot(scores_w, aes(x=Comp.1, y= -Comp.2, label=Distillery)) + geom_vline(xintercept=0, colour = &#39;red&#39;) + geom_hline(yintercept=0, colour = &#39;red&#39;) + geom_point()+ geom_text_repel(size=2.5, segment.alpha = 0.3, force = 0.1, seed=202) + xlab(&#39;Fruity/Floral vs. Smoky/Medicional&#39;) + ylab(&#39;Winey/Body and Honey&#39;) ## Warning: ggrepel: 12 unlabeled data points (too many overlaps). Consider ## increasing max.overlaps ¿Que pasa si usamos svd sin centrar? Vemos que la primera componente simplemente captura los distintos niveles promedio de las variables. Esta componente no es muy interesante, pues por las características del whisky es normal que Medicinal o Tabaco tengo una media baja, comparado con dulzor, Smoky, etc. Adicionalmente, el vector \\(u\\) asociado a esta dimensión tiene poca variación: svd_w &lt;- svd(whisky_sabor) svd_w$v[,1:2] ## [,1] [,2] ## [1,] -0.39539241 -0.38286900 ## [2,] -0.42475240 0.19108176 ## [3,] -0.28564145 -0.48775482 ## [4,] -0.09556061 -0.57453247 ## [5,] -0.02078706 -0.09187451 ## [6,] -0.24191199 0.20518808 ## [7,] -0.26485793 -0.07103866 ## [8,] -0.19488910 0.01930294 ## [9,] -0.27858538 0.03430020 ## [10,] -0.33669488 0.11644937 ## [11,] -0.34001725 0.18933847 ## [12,] -0.31441595 0.37730436 plot(svd_w$v[,1], apply(whisky_sabor, 2, mean)) mean(svd_w$u[,1]) ## [1] -0.1063501 sd(svd_w$u[,1]) ## [1] 0.01792508 Observación La primera componente de svd está haciendo el trabajo de ajustar la media. Como no nos interesa este hecho, podemos mejor centrar desde el principio y trabajar con las componentes principales. ¿Cómo se ven las siguientes dos dimensiones del análisis no centrado? Ejemplo: donde no centrar funciona bien Considera el ejemplo de la tarea con la tabla de gastos en distintas categorías de alimentos según el decil de ingreso del hogar. ¿Por qué en este ejemplo centrar por columna no es tan buena idea? Si hacemos el centrado, quitamos información importante de la tabla, que es que los distintos deciles tienen distintos niveles de gasto. Veamos como lucen los dos análisis. Para componentes principales: deciles &lt;- read_csv(&#39;../datos/enigh_deciles.csv&#39;) deciles |&gt; arrange(desc(d1)) ## # A tibble: 13 × 11 ## nombre d1 d2 d3 d4 d5 d6 d7 dd8 d9 d10 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 CEREAL… 1.33e6 1.87e6 2.25e6 2.33e6 2.58e6 2.59e6 2.84e6 2.77e6 2.74e6 2.71e6 ## 2 CARNES 1.07e6 1.75e6 2.13e6 2.51e6 2.97e6 3.23e6 3.71e6 3.94e6 4.18e6 4.72e6 ## 3 VERDUR… 9.74e5 1.28e6 1.48e6 1.59e6 1.67e6 1.73e6 1.78e6 1.81e6 1.83e6 1.98e6 ## 4 LECHE … 5.86e5 8.95e5 1.24e6 1.40e6 1.58e6 1.78e6 1.97e6 2.12e6 2.36e6 3.09e6 ## 5 OTROS … 2.90e5 4.49e5 6.90e5 7.82e5 1.03e6 1.12e6 1.45e6 1.54e6 2.28e6 2.71e6 ## 6 HUEVO 2.55e5 3.60e5 4.22e5 4.43e5 4.06e5 4.05e5 4.51e5 4.19e5 3.99e5 3.65e5 ## 7 FRUTAS 1.92e5 2.84e5 3.38e5 4.68e5 5.18e5 5.71e5 7.05e5 7.65e5 8.82e5 1.38e6 ## 8 AZUCAR… 1.67e5 2.13e5 2.00e5 1.91e5 2.02e5 1.90e5 1.57e5 1.74e5 1.64e5 1.63e5 ## 9 ACEITE… 1.36e5 1.90e5 1.80e5 1.84e5 1.94e5 1.97e5 1.89e5 1.81e5 1.82e5 2.09e5 ## 10 PESCAD… 1.10e5 1.88e5 2.14e5 2.36e5 2.87e5 2.97e5 3.34e5 4.37e5 4.97e5 8.65e5 ## 11 TUBERC… 1.07e5 1.58e5 1.91e5 2.02e5 2.29e5 2.15e5 2.14e5 2.24e5 2.22e5 2.28e5 ## 12 CAFE, … 7.19e4 1.20e5 1.09e5 9.71e4 1.25e5 1.29e5 1.10e5 1.26e5 1.43e5 2.25e5 ## 13 ESPECI… 5.76e4 8.06e4 9.18e4 1.09e5 1.16e5 1.34e5 1.55e5 1.52e5 1.68e5 1.82e5 deciles &lt;- deciles |&gt; column_to_rownames(var = &quot;nombre&quot;) comp_enigh &lt;- princomp(deciles) Veamos las primeras dos componente, cuyas direcciones principales son: comp_enigh$loadings[,1:2] ## Comp.1 Comp.2 ## d1 0.1224572 0.29709420 ## d2 0.1858230 0.35463967 ## d3 0.2324626 0.34856083 ## d4 0.2610938 0.29531199 ## d5 0.3010861 0.23322242 ## d6 0.3221099 0.16749474 ## d7 0.3650886 0.07154327 ## dd8 0.3783732 -0.02514659 ## d9 0.4019500 -0.30553359 ## d10 0.4425357 -0.62905737 Y los scores son: comp_enigh$scores[,1:2] ## Comp.1 Comp.2 ## CEREALES 4548094.6 1191716.24 ## CARNES 7068531.1 -392100.57 ## PESCADOS Y MARISCOS -1880143.9 -290076.15 ## LECHE Y SUS DERIVADOS 2688171.0 -541441.01 ## HUEVO -1882277.7 346789.97 ## ACEITES Y GRASAS -2525107.6 157761.37 ## TUBERCULOS -2460994.3 134899.61 ## VERDURAS, LEGUMBRES, LEGUMINOSAS 2029622.4 715856.37 ## FRUTAS -960953.6 -445884.20 ## AZUCAR Y MIELES -2551904.3 217378.43 ## CAFE, TE Y CHOCOLATE -2685873.3 33326.41 ## ESPECIAS Y ADEREZOS -2679471.1 33837.02 ## OTROS ALIMENTOS DIVERSOS 1292306.9 -1162063.49 Y la tabla de rango 1 es tab_1 &lt;- tcrossprod(comp_enigh$scores[,1], comp_enigh$loadings[,1]) colnames(tab_1) &lt;- colnames(deciles) tab_1 &lt;- tab_1 |&gt; as_tibble() |&gt; mutate(categoria = rownames(deciles)) |&gt; gather(decil, gasto, d1:d10) tab_1$categoria &lt;- reorder(tab_1$categoria, tab_1$gasto, mean) ggplot(tab_1, aes(x=categoria, y=gasto, colour=decil, group=decil)) + geom_line() + coord_flip() Que podemos comparar con el análisis no centrado: svd_enigh &lt;- svd(deciles) tab_1 &lt;- tcrossprod(svd_enigh$u[,1], svd_enigh$v[,1]) colnames(tab_1) &lt;- colnames(deciles) tab_1 &lt;- tab_1 |&gt; as_tibble() |&gt; mutate(categoria = rownames(deciles)) |&gt; gather(decil, gasto, d1:d10) tab_1$categoria &lt;- reorder(tab_1$categoria, tab_1$gasto, mean) ggplot(tab_1, aes(x=categoria, y=gasto, colour=decil, group=decil)) + geom_line() + coord_flip() Y aunque los resultados son similares, puede ser más simple entender la primera dimensión del svd no centrado que guarda los efectos de los distintos niveles de gasto de los deciles. En el caso del análisis centrado, tenemos una primera componente que sólo se entiende bien sabiendo los niveles promedio de gasto a lo largo de las categorías. Observación: Quizá una solución más natural es hacer el análisis de componentes principales usando la transpuesta de esta matriz (usa la función prcomp), donde tiene más sentido centrar por categoría de alimento, y pensar que las observaciones son los distintos deciles (que en realidad son agrupaciones de observaciones). Otros tipos de centrado Es posible hacer doble centrado, por ejemplo (por renglón y por columna). Discute por qué el doble centrado puede ser una buena idea para los datos del tipo de Netflix. Reescalando variables Cuando las columnas tienen distintas unidades (especialmente si las escalas son muy diferentes), conviene reescalar la matriz antes de hacer el análisis centrado o no centrado. De otra forma, parte del análisis intenta absorber la diferencia en unidades, lo cual generalmente no es de interés. - En componentes principales, podemos estandarizar las columnas. - En el análisis no centrado, podemos poner las variables en escala 0-1, por ejemplo, o dividir entre la media (si son variables positivas). Ejemplo comp &lt;- princomp(attenu |&gt; select(-station, -event)) comp$loadings[,1] ## mag dist accel ## 0.005746131 0.999982853 -0.001129688 Y vemos que la dirección de la primera componente es justamente en la dirección de la variable dist (es decir, la primera componente es dist). Esto es porque la escala de dist es más amplia: apply(attenu |&gt; select(-station), 2, mean) ## event mag dist accel ## 14.7417582 6.0840659 45.6032967 0.1542198 Esto lo corregimos estandarizando las columnas, o equivalentemente, usando cor = TRUE como opción en princomp comp &lt;- princomp(attenu |&gt; select(-station, -event), cor = TRUE) comp$loadings[,1] ## mag dist accel ## 0.5071375 0.7156080 -0.4803298 "],["sistemas-de-recomendación-y-filtrado-colaborativo.html", "5 Sistemas de recomendación y filtrado colaborativo 5.1 Enfoques de recomendación 5.2 Datos 5.3 Modelos de referencia y evaluación 5.4 Modelo de referencia 5.5 Filtrado colaborativo: similitud", " 5 Sistemas de recomendación y filtrado colaborativo En esta sección discutiremos métodos que se pueden utilizar para hacer recomendaciones de documentos, películas, artículos, etc. a personas según sus intereses. Problema: predecir la respuesta de personas a estímulos a los que no han sido expuestos, basados en respuesta a otros estímulos de esta y quizá otras personas similares. Por ejemplo, si consideramos usuarios de Netflix: ¿qué tanto le puede gustar a X la película Y? Usuarios de Amazon: ¿qué tan probable es que compren Z artículo si se les ofrece? 5.1 Enfoques de recomendación Hay varios enfoques que podemos utilizar para atacar este problema: Principalmente basados en contenido: En función de características de los estímulos, productos o películas (por ejemplo, género, actores, país de origen, año, etc.) intentamos predecir el gusto por el estímulo. En este enfoque, construimos variables derivadas del contenido de los artículos (por ejemplo: qué actores salen, año, etc. o en textos palabras que aparecen), e intentamos predecir el gusto a partir de esas características. Ejemplo: Si mu gustó Twilight entonces el sistema recomienda otros dramas+vampiros (” por ejemplo “Entrevista con un vampiro”). Principalmente colaborativos: utilizamos gustos o intereses de usuarios/artículos similares — en el sentido de que les han gustado los mismos artículos/les gustaron a las mismas personas. Ejemplo: Me gustó StarWars y Harry Potter, varios otros usuarios a los que también les gustaron estas dos películas también les gustó “Señor de los anillos,” así que recomendamos “Señor de los Anillos.” Entre estos métodos, veremos principalmente métodos basados en reducción de dimensionalidad o modelos de factores latentes: encontrar factores latentes que describan usuarios y películas, y predecimos dependiendo de los niveles de factores latentes de personas y películas. 5.2 Datos Los datos utilizados para este tipo de sistemas son de dos tipos: Ratings explícitos dados por los usuarios (por ejemplo, Netflix mucho tiempo fue así: \\(1-5\\) estrellas) Ratings implícitos que se derivan de la actividad de los usuarios (por ejemplo, vio la película completa, dio click en la descripción de un producto, etc.). Ejemplo Consideramos evaluaciones en escala de gusto: por ejemplo \\(1-5\\) estrellas, o \\(1\\)-me disgustó mucho, \\(5\\)-me gustó mucho, etc. Podemos representar las evaluaciones como una matriz: SWars1 SWars4 SWars5 HPotter1 HPotter2 Twilight a 3 5 5 2 - - b 3 - 4 - - - c - - - 5 4 - d 1 - 2 - 5 4 Y lo que queremos hacer es predecir los valores faltantes de esta matriz, y seleccionar para cada usuario los artículos con predicción más alta, por ejemplo SWars1 SWars4 SWars5 HPotter1 HPotter2 Twilight a 3 5 5 2 1.2 3 b 3 1.4 4 3 3.3 1.2 c 4.2 2.8 4.5 5 4 3.1 d 1 1.2 2 4.8 5 4 Podemos pensar en este problema como uno de imputación de datos faltantes. Las dificultades particulares de este problema son: Datos ralos: cada usuario sólo ha visto y calificado una proporción baja de películas, y hay películas con pocas vistas. Escalabilidad: el número de películas y usuarios es generalmente grande Por estas razones, típicamente no es posible usar técnicas estadísticas de imputación de datos (como imputación estocástica basada en regresión) Usaremos mejor métodos más simples basados en similitud entre usuarios y películas y descomposición de matrices. 5.3 Modelos de referencia y evaluación Vamos a comenzar considerando modelos muy simples. El primero que se nos puede ocurrir es uno de homogeneidad de gustos: para una persona \\(i\\), nuestra predicción de su gusto por la película \\(j\\) es simplemente la media de la película \\(j\\) sobre todos los usuarios que la han visto. Este sería un buen modelo si los gustos fueran muy parecidos sobre todos los usuarios. Esta sería una recomendación de “artículos populares.” Introducimos la siguente notación: \\(x_{ij}\\) es la evaluación del usuario \\(i\\) de la película \\(j\\). Obsérvese que muchos de estos valores no son observados (no tenemos información de varias \\(x_{ij}\\)). \\(\\hat{x}_{ij}\\) es la predicción que hacemos de gusto del usuario \\(i\\) por la película \\(j\\) En nuestro primer modelo simple, nuestra predicción es simplemente \\[\\hat{x}_{ij} = \\hat{b}_j\\] donde \\[\\hat{b_j} = \\frac{1}{N_j}\\sum_{s} x_{sj},\\] y este promedio es sobre los \\(N_j\\) usuarios que vieron (y calificaron) la película \\(j\\). ¿Cómo evaluamos nuestras predicciones? 5.3.1 Evaluación de predicciones Usamos muestras de entrenamiento y validación. Como en el concurso de Netflix, utilizaremos la raíz del error cuadrático medio: \\[RECM =\\left ( \\frac{1}{T} \\sum_{(i,j) \\, observada} (x_{ij}-\\hat{x}_{ij})^2 \\right)^{\\frac{1}{2}}\\] aunque también podríamos utilizar la desviación absoluta media: \\[DAM =\\frac{1}{T} \\sum_{(i,j) \\, observada} |x_{ij}-\\hat{x}_{ij}|\\] Nótese que estas dos cantidades se evaluán sólo sobre los pares \\((i,j)\\) para los que tengamos una observación \\(x_{ij}\\). Observaciones: Generalmente evaluamos sobre un conjunto de validación separado del conjunto de entrenamiento. Escogemos una muestra de películas, una muestra de usuarios, y ponemos en validación el conjunto de calificaciones de esos usuarios para esas películas. Para hacer un conjunto de prueba, idealmente los datos deben ser de películas que hayan sido observadas por los usuarios en el futuro (después del periodo de los datos de entrenamiento). Nótese que no seleccionamos todas las evaluaciones de un usuario, ni todas las evaluaciones de una película. Con estas estrategias, veríamos qué pasa con nuestro modelo cuando tenemos una película que no se ha visto o un usuario nuevo. Lo que queremos ententer es cómo se desempeña nuestro sistema cuando tenemos cierta información de usuarios y de películas. Ejemplo: datos de Netflix Los datos del concurso de Netflix originalmente vienen en archivos de texto, un archivo por película. The movie rating files contain over \\(100\\) million ratings from \\(480\\) thousand randomly-chosen, anonymous Netflix customers over \\(17\\) thousand movie titles. The data were collected between October, \\(1998\\) and December, \\(2005\\) and reflect the distribution of all ratings received during this period. The ratings are on a scale from \\(1\\) to \\(5\\) (integral) stars. To protect customer privacy, each customer id has been replaced with a randomly-assigned id. The date of each rating and the title and year of release for each movie id are also provided. The file “training_set.tar” is a tar of a directory containing \\(17770\\) files, one per movie. The first line of each file contains the movie id followed by a colon. Each subsequent line in the file corresponds to a rating from a customer and its date in the following format: CustomerID,Rating,Date MovieIDs range from \\(1\\) to \\(17770\\) sequentially. CustomerIDs range from \\(1\\) to \\(2649429\\), with gaps. There are \\(480189\\) users. Ratings are on a five star (integral) scale from \\(1\\) to \\(5\\). Dates have the format YYYY-MM-DD. En primer lugar haremos un análisis exploratorio de los datos para entender algunas de sus características, ajustamos el modelo simple de gustos homogéneos (la predicción es el promedio de calificaciones de cada película), y lo evaluamos. Comenzamos por cargar los datos: library(tidyverse) theme_set(theme_minimal()) cb_palette &lt;- c(&quot;#000000&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) # url &lt;- &quot;https://s3.amazonaws.com/ma-netflix/dat_muestra_nflix.csv&quot; pelis_nombres &lt;- read_csv(&#39;../datos/netflix/movies_title_fix.csv&#39;, col_names = FALSE, na = c(&quot;&quot;, &quot;NA&quot;, &quot;NULL&quot;)) ## Rows: 17770 Columns: 3 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): X3 ## dbl (2): X1, X2 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. names(pelis_nombres) &lt;- c(&#39;peli_id&#39;,&#39;año&#39;,&#39;nombre&#39;) dat_netflix &lt;- read_csv( &quot;../datos/netflix/dat_muestra_nflix.csv&quot;, progress = FALSE) |&gt; select(-usuario_id_orig) |&gt; mutate(usuario_id = as.integer(as.factor(usuario_id))) ## Rows: 20968941 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (4): peli_id, usuario_id_orig, calif, usuario_id ## date (1): fecha ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(dat_netflix) ## # A tibble: 6 × 4 ## peli_id calif fecha usuario_id ## &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;int&gt; ## 1 1 3 2004-04-14 1 ## 2 1 3 2004-12-28 2 ## 3 1 4 2004-04-06 3 ## 4 1 4 2004-03-07 4 ## 5 1 4 2004-03-29 5 ## 6 1 2 2004-07-11 6 dat_netflix |&gt; tally() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 20968941 Y ahora calculamos las medias de cada película. medias_pelis &lt;- dat_netflix |&gt; group_by(peli_id) |&gt; summarise(media_peli = mean(calif), num_calif_peli = n()) medias_pelis &lt;- left_join(medias_pelis, pelis_nombres) ## Joining, by = &quot;peli_id&quot; arrange(medias_pelis, desc(media_peli)) |&gt; top_n(200, media_peli) |&gt; mutate(media_peli = round(media_peli, 2)) |&gt; DT::datatable() Nótese que varias de las películas con mejor promedio tienen muy pocas evaluaciones. Podemos examinar más detalladamente graficando número de evaluaciones vs promedio: ggplot(medias_pelis, aes(x=num_calif_peli, y=media_peli)) + geom_point(alpha = 0.1) + xlab(&quot;Número de calificaciones&quot;) + ylab(&quot;Promedio de calificaciones&quot;) Y vemos que hay más variabilidad en los promedios cuando hay menos evaluaciones, como es de esperarse. ¿Puedes ver algún problema que tendremos que enfrentar con el modelo simple? Si filtramos por número de calificaciones (al menos \\(500\\) por ejemplo), estas son las películas mejor calificadas (la mayoría conocidas y populares): arrange(medias_pelis, desc(media_peli)) |&gt; filter(num_calif_peli &gt; 500) |&gt; top_n(200, media_peli) |&gt; mutate(media_peli = round(media_peli, 2)) |&gt; DT::datatable() Ahora seleccionamos nuestra muestra de entrenamiento y de validación. Seleccionamos una muestra de usuarios y de películas: set.seed(28882) usuarios &lt;- dat_netflix |&gt; select(usuario_id) |&gt; distinct() valida_usuarios &lt;- usuarios |&gt; sample_frac(0.2) peliculas &lt;- dat_netflix |&gt; select(peli_id) |&gt; distinct() valida_pelis &lt;- peliculas |&gt; sample_frac(0.2) Y separamos calificaciones de entrenamiento y validación: dat_valida &lt;- dat_netflix |&gt; semi_join(valida_usuarios) |&gt; semi_join(valida_pelis) ## Joining, by = &quot;usuario_id&quot; ## Joining, by = &quot;peli_id&quot; dat_entrena &lt;- dat_netflix |&gt; anti_join(dat_valida) ## Joining, by = c(&quot;peli_id&quot;, &quot;calif&quot;, &quot;fecha&quot;, &quot;usuario_id&quot;) n_valida &lt;- dat_valida |&gt; tally() |&gt; pull(n) n_entrena &lt;- dat_entrena |&gt; tally() |&gt; pull(n) sprintf(&quot;Entrenamiento: %1d, Validación: %2d, Total: %3d&quot;, n_entrena, n_valida, n_entrena + n_valida) ## [1] &quot;Entrenamiento: 20191991, Validación: 776950, Total: 20968941&quot; Ahora construimos predicciones con el modelo simple de arriba y evaluamos con validación: medias_pred &lt;- dat_entrena |&gt; group_by(peli_id) |&gt; summarise(media_pred = mean(calif)) media_total_e &lt;- dat_entrena |&gt; ungroup() |&gt; summarise(media = mean(calif)) |&gt; pull(media) dat_valida_pred &lt;- dat_valida |&gt; left_join(medias_pred |&gt; collect()) head(dat_valida_pred) ## # A tibble: 6 × 5 ## peli_id calif fecha usuario_id media_pred ## &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2 5 2005-07-18 116 3.64 ## 2 2 4 2005-06-07 126 3.64 ## 3 2 4 2005-07-30 129 3.64 ## 4 2 1 2005-09-07 131 3.64 ## 5 2 1 2005-02-13 103 3.64 ## 6 8 1 2005-03-21 1007 3.19 Nota que puede ser que algunas películas seleccionadas en validación no tengan evaluaciones en entrenamiento: table(is.na(dat_valida_pred$media_pred)) ## ## FALSE ## 776950 dat_valida_pred &lt;- mutate(dat_valida_pred, media_pred = ifelse(is.na(media_pred), media_total_e, media_pred)) No sucede en este ejemplo, pero si sucediera podríamos usar el promedio general de las predicciones. Evaluamos ahora el error: recm &lt;- function(calif, pred){ sqrt(mean((calif - pred)^2)) } error &lt;- dat_valida_pred |&gt; ungroup() |&gt; summarise(error = mean((calif - media_pred)^2)) error ## # A tibble: 1 × 1 ## error ## &lt;dbl&gt; ## 1 1.03 Este error está en las mismas unidades de las calificaciones (estrellas en este caso). Antes de seguir con nuestra refinación del modelo, veremos algunas observaciones acerca del uso de escala en análisis de datos: Cuando tratamos con datos en escala encontramos un problema técnico que es el uso distinto de la escala por los usuarios, muchas veces independientemente de sus gustos Veamos los datos de Netflix para una muestra de usuarios: # muestra de usuarios entrena_usu &lt;- sample(unique(dat_entrena$usuario_id), 50) muestra_graf &lt;- filter(dat_entrena, usuario_id %in% entrena_usu) # medias generales por usuario, ee de la media muestra_res &lt;- muestra_graf |&gt; group_by(usuario_id) |&gt; summarise(media_calif = mean(calif), sd_calif = sd(calif)/sqrt(length(calif))) muestra_res$usuario_id &lt;- reorder(factor(muestra_res$usuario_id), muestra_res$media_calif) ggplot(muestra_res, aes(x=factor(usuario_id), y = media_calif, ymin = media_calif - sd_calif, ymax = media_calif + sd_calif)) + geom_linerange() + geom_point() + xlab(&#39;Usuario ID&#39;) + theme(axis.text.x=element_blank()) Y notamos que hay unos usuarios que tienen promedios por encima de \\(4.5\\), mientras que otros califican por debajo de \\(3\\) en promedio. Aunque esto puede deberse a las películas que han visto, generalmente una componente de esta variabilidad se debe a cómo usa la escala cada usuario. En primer lugar, quizá uno podría pensar que un modelo base consiste de simplemente predecir el promedio de una película sobre todos los usuarios que la calificaron (sin incluir el sesgo de cada persona). Esto no funciona bien porque típicamente hay distintos patrones de uso de la escala de calificación, que depende más de forma de uso de escala que de la calidad de los items. Hay personas que son: Barcos: \\(5\\),\\(5\\),\\(5\\),\\(4\\),\\(4\\),\\(5\\) Estrictos: \\(2\\),\\(3\\),\\(3\\),\\(1\\),\\(1\\),\\(2\\) No se compromete: \\(3\\),\\(3\\),\\(3\\),\\(3\\),\\(4\\) Discrimina: \\(5\\),\\(4\\),\\(5\\),\\(1\\),\\(2\\),\\(4\\) El estilo de uso de las escalas varía por persona. Puede estar asociado a aspectos culturales (países diferentes usan escalas de manera diferente), quizá también de personalidad, y a la forma de obtener las evaluaciones (cara a cara, por teléfono, internet). Lo primero que vamos a hacer para controlar esta fuente de variación es ajustar las predicciones dependiendo del promedio de calificaciones de cada usuario. 5.3.2 (Opcional) Efectos en análisis de heterogeneidad en uso de escala Muchas veces se considera que tratar como numéricos a calificaciones en escala no es muy apropiado, y que el análisis no tiene por qué funcionar pues en realidad las calificaciones están en una escala ordinal. Sin embargo, La razón principal por las que análisis de datos en escala es difícil no es que usemos valores numéricos para los puntos de la escala. Si esto fuera cierto, entonces por ejemplo, transformar una variable con logaritmo también sería “malo.” La razón de la dificultad es que generalmente tenemos que lidiar con la heterogeneidad en uso de la escala antes de poder obtener resultados útiles de nuestro análisis. Supongamos que \\(X_1\\) y \\(X_2\\) son evaluaciones de dos películas. Por la discusión de arriba, podríamos escribir \\[X_1 = N +S_1,\\] \\[X_2 = N + S_2,\\] donde \\(S_1\\) y \\(S_2\\) representan el gusto por la película, y \\(N\\) representa el nivel general de calificaciones. Consideramos que son variables aleatorias (\\(N\\) varía con las personas, igual que \\(S_1\\) y \\(S_2\\)). Podemos calcular \\[Cov(X_1,X_2)\\] para estimar el grado de correlación de gusto por las dos películas, como si fueran variables numéricas. Esta no es muy buena idea, pero no tanto porque se trate de variables ordinales, sino porque en realidad quisiéramos calcular: \\[Cov(S_1, S_2)\\] que realmente representa la asociación entre el gusto por las dos películas. El problema es que \\(S_1\\) y \\(S_2\\) son variables que no observamos. ¿Cómo se relacionan estas dos covarianzas? \\[Cov(X_1,X_2)=Cov(N,N) + Cov(S_1,S_2) + Cov(N, S_2)+Cov(N,S_1)\\] Tenemos que \\(Cov(N,N)=Var(N)=\\sigma_N ^2\\), y suponiendo que el gusto no está correlacionado con los niveles generales de respuesta, \\(Cov(N_1, S_2)=0=Cov(N_2,S_1)\\), de modo que \\[Cov(X_1,X_2)= Cov(S_1,S_2) + \\sigma_N^2.\\] donde \\(\\sigma_N^2\\) no tiene qué ver nada con el gusto por las películas. De forma que al usar estimaciones de \\(Cov(X_1,X_2)\\) para estimar \\(Cov(S_1,S_2)\\) puede ser mala idea porque el sesgo hacia arriba puede ser alto, especialmente si la gente varía mucho es un sus niveles generales de calificaciones (hay muy barcos y muy estrictos). 5.3.3 Ejemplo Los niveles generales de \\(50\\) personas: set.seed(128) n &lt;- 50 niveles &lt;- tibble(persona = 1:n, nivel = rnorm(n,2)) Ahora generamos los gustos (latentes) por dos artículos, que suponemos con correlación negativa: x &lt;- rnorm(n) gustos &lt;- tibble(persona=1:n, gusto_1 = x + rnorm(n), gusto_2 = -x + rnorm(n)) head(gustos,3) ## # A tibble: 3 × 3 ## persona gusto_1 gusto_2 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.948 0.459 ## 2 2 1.15 -0.676 ## 3 3 -1.31 2.58 cor(gustos[,2:3]) ## gusto_1 gusto_2 ## gusto_1 1.0000000 -0.5136434 ## gusto_2 -0.5136434 1.0000000 Estos dos items tienen gusto correlacionado negativamente: ggplot(gustos, aes(x=gusto_1, y=gusto_2)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Pero las mediciones no están correlacionadas: medicion_1 &lt;- niveles$nivel + gustos$gusto_1+rnorm(n,0.3) medicion_2 &lt;- niveles$nivel + gustos$gusto_2+rnorm(n,0.3) mediciones &lt;- tibble(persona = 1:n, medicion_1, medicion_2) cor(mediciones[,2:3]) ## medicion_1 medicion_2 ## medicion_1 1.00000000 -0.05825995 ## medicion_2 -0.05825995 1.00000000 Así que aún cuando el gusto por \\(1\\) y \\(2\\) están correlacionadas negativamente, las mediciones de gusto no están correlacionadas. ggplot(mediciones, aes(x=medicion_1, y=medicion_2)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Observaciones: Un modelado más cuidadoso de este tipo de datos requiere más trabajo. Pero para el trabajo usual, generalmente intentamos controlar parte de la heterogeneidad centrando las calificaciones por usuario. Es decir, a cada calificación de una persona le restamos la media de sus calificaciones, que es una estimación del nivel general \\(N\\). Esta idea funciona si \\(k\\) no es muy chico. Si el número de calificaciones por persona (\\(k\\)) es chico, entonces tenemos los siguientes problemas: El promedio de evaluaciones es una estimación ruidosa del nivel general. Podemos terminar con el problema opuesto: nótese que si \\(X_1,\\ldots, X_k\\) son mediciones de gusto distintos items, entonces \\[Cov(X_1-\\bar{X}, X_2-\\bar{X})=Cov(S_1-\\bar{S},S_2-\\bar{S}),\\] \\[=Cov(S_1,S_2)-Cov(S_1,\\bar{S})-Cov(S_2,\\bar{S}) + Var(\\bar{S})\\] Si \\(k\\) es chica, suponiendo que los gustos no están correlacionados, los términos intermedios puede tener valor negativo relativamente grande ( es de orden \\(\\frac{1}{k}\\)), aún cuando el último término sea chico (de orden \\(\\frac{1}{k^2}\\)) Así que ahora las correlaciones estimadas pueden tener sesgo hacia abajo, especialmente si \\(k\\) es chica. Más avanzado, enfoque bayesiano: https://www.jstor.org/stable/2670337 5.4 Modelo de referencia Ahora podemos plantear el modelo base de referencia. Este modelo es útil para hacer benchmarking de intentos de predicción, como primera pieza para construcción de modelos más complejos, y también como una manera simple de producir estimaciones cuando no hay datos suficientes para hacer otro tipo de predicción. Si \\(x_{ij}\\) es el gusto del usuario \\(i\\) por la película \\(j\\), entonces nuestra predicción es \\[\\hat{x}_{ij} = \\hat{b}_j + (\\hat{a}_i-\\hat{\\mu} ) \\] donde \\(a_i\\) indica un nivel general de calificaciones del usuario \\(i\\), y \\(b_j\\) es el nivel general de gusto por la película. Usualmente ponemos: Media general \\[\\hat{\\mu} =\\frac{1}{T}\\sum_{s,t} x_{s,t}\\] Promedio de calificaciones de usuario \\(i\\) \\[\\hat{a}_i =\\frac{1}{M_i}\\sum_{t} x_{i,t} \\] Promedio de calificaciones de la película \\(j\\) \\[\\hat{b}_j =\\frac{1}{N_j}\\sum_{s} x_{s,j}\\] También podemos escribir, en términos de desviaciones: \\[\\hat{x}_{ij} = \\hat{\\mu} + \\hat{c}_i + \\hat{d}_j \\] donde: Media general \\[\\hat{\\mu} =\\frac{1}{T}\\sum_{s,t} x_{st}\\] Desviación de las calificaciones de usuario \\(i\\) respecto a la media general \\[\\hat{c}_i =\\frac{1}{M_i}\\sum_{t} x_{it} - \\hat{\\mu} \\] Desviación de la película \\(j\\) respecto a la media general \\[\\hat{d_j} =\\frac{1}{N_j}\\sum_{s} x_{sj}- \\hat{\\mu}\\] Una vez que observamos una calificación \\(x_{ij}\\), el residual del modelo de referencia es \\[r_{ij} = x_{ij} - \\hat{x_{ij}}\\] Ejercicio: modelo de referencia para Netflix Calculamos media de películas, usuarios y total: medias_usuarios &lt;- dat_entrena |&gt; group_by(usuario_id) |&gt; summarise(media_usu = mean(calif), num_calif_usu = length(calif)) |&gt; select(usuario_id, media_usu, num_calif_usu) medias_peliculas &lt;- dat_entrena |&gt; group_by(peli_id) |&gt; summarise(media_peli = mean(calif), num_calif_peli = length(calif)) |&gt; select(peli_id, media_peli, num_calif_peli) media_total_e &lt;- mean(dat_entrena$calif) Y construimos las predicciones para el conjunto de validación dat_valida &lt;- dat_valida |&gt; left_join(medias_usuarios) |&gt; left_join(medias_peliculas) |&gt; mutate(media_total = media_total_e) |&gt; mutate(pred = media_peli + (media_usu - media_total)) |&gt; mutate(pred = ifelse(is.na(pred), media_total, pred)) ## Joining, by = &quot;usuario_id&quot; ## Joining, by = &quot;peli_id&quot; Nótese que cuando no tenemos predicción bajo este modelo para una combinación de usuario/película, usamos el promedio general (por ejemplo). Finalmente evaluamos dat_valida |&gt; ungroup() |&gt; summarise(error = recm(calif, pred)) ## # A tibble: 1 × 1 ## error ## &lt;dbl&gt; ## 1 0.929 Observación: ¿Qué tan bueno es este resultado? De wikipedia: Prizes were based on improvement over Netflix’s own algorithm, called Cinematch, or the previous year’s score if a team has made improvement beyond a certain threshold. A trivial algorithm that predicts for each movie in the quiz set its average grade from the training data produces an RMSE of \\(1.0540\\). Cinematch uses “straightforward statistical linear models with a lot of data conditioning.” Using only the training data, Cinematch scores an RMSE of \\(0.9514\\) on the quiz data, roughly a 10% improvement over the trivial algorithm. Cinematch has a similar performance on the test set, \\(0.9525\\). In order to win the grand prize of \\(1,000,000\\), a participating team had to improve this by another \\(10%\\), to achieve \\(0.8572\\) on the test set. Such an improvement on the quiz set corresponds to an RMSE of \\(0.8563\\). Aunque nótese que estrictamente hablando no podemos comparar nuestros resultados con estos números, en los que se usa una muestra de prueba separada de películas vistas despúes del periodo de entrenamiento. 5.5 Filtrado colaborativo: similitud Además de usar promedios generales por película, podemos utilizar similitud de películas/personas para ajustar predicciones según los gustos de artículos o películas similares. Este es el enfoque más simple del filtrado colaborativo. Comencemos entonces con la siguiente idea: Supongamos que queremos hacer una predicción para el usuario \\(i\\) en la película \\(j\\), que no ha visto. Si tenemos una medida de similitud entre películas, podríamos buscar películas similares a \\(j\\) que haya visto \\(i\\), y ajustar la predicción según la calificación de estas películas similares. Tomamos entonces nuestra predicción base, que le llamamos \\(x_{ij}^0\\) y hacemos una nueva predicción: \\[\\hat{x}_{ij} = x_{ij}^0 + \\frac{1}{k}\\sum_{t \\in N(i,j)} (x_{it} - x_{it}^0 )\\] donde \\(N(i,j)\\) son películas similares a \\(j\\) que haya visto \\(i\\). Ajustamos \\(x_{ij}^0\\) por el gusto promedio de películas similares a \\(j\\), a partir de las predicciones base. Esto quiere decir que si las películas similares a \\(j\\) están evaluadas por encima del esperado para el usuario \\(i\\), entonces subimos la predicción, y bajamos la predicción cuando las películas similares están evaluadas por debajo de lo esperado. Nótese que estamos ajustando por los residuales del modelo base. Podemos también utilizar un ponderado por gusto según similitud: si la similitud entre las películas \\(j\\) y \\(t\\) es \\(s_{jt}\\), entonces podemos usar \\[\\begin{equation} \\hat{x}_{ij} = x_{ij}^0 + \\frac{\\sum_{t \\in N(i,j)} s_{jt}(x_{it} - x_{it}^0 )}{\\sum_{t \\in N(i,j)} s_{jt}} \\tag{5.1} \\end{equation}\\] Cuando no tenemos películas similares que hayan sido calificadas por nuestro usuario, entonces usamos simplemente la predicción base. 5.5.1 Cálculo de similitud entre usuarios/películas Proponemos utilizar la distancia coseno de las calificaciones centradas por usuario. Como discutimos arriba, antes de calcular similitud conviene centrar las calificaciones por usuario para eliminar parte de la heterogeneidad en el uso de la escala. Ejemplo SWars1 SWars4 SWars5 HPotter1 HPotter2 Twilight a 5 5 5 2 NA NA b 3 NA 4 NA NA NA c NA NA NA 5 4 NA d 1 NA 2 NA 5 4 e 4 5 NA NA NA 2 Calculamos medias por usuarios y centramos: apply(mat.cons,1, mean, na.rm=TRUE) ## a b c d e ## 4.250000 3.500000 4.500000 3.000000 3.666667 mat.c &lt;- mat.cons - apply(mat.cons,1, mean, na.rm=TRUE) knitr::kable(mat.c, digits = 2) SWars1 SWars4 SWars5 HPotter1 HPotter2 Twilight a 0.75 0.75 0.75 -2.25 NA NA b -0.50 NA 0.50 NA NA NA c NA NA NA 0.50 -0.5 NA d -2.00 NA -1.00 NA 2.0 1.00 e 0.33 1.33 NA NA NA -1.67 Y calculamos similitud coseno entre películas, suponiendo que las películas no evaluadas tienen calificación \\(0\\): sim_cos &lt;- function(x,y){ sum(x*y, na.rm = T)/(sqrt(sum(x^2, na.rm = T))*sqrt(sum(y^2, na.rm = T))) } mat.c[,1] ## a b c d e ## 0.7500000 -0.5000000 NA -2.0000000 0.3333333 mat.c[,2] ## a b c d e ## 0.750000 NA NA NA 1.333333 sim_cos(mat.c[,1], mat.c[,2]) ## [1] 0.2966402 sim_cos(mat.c[,1], mat.c[,6]) ## [1] -0.5925503 Observación: Hacer este supuesto de valores \\(0\\) cuando no tenemos evaluación no es lo mejor, pero como centramos por usuario tiene más sentido hacerlo. Si utilizaramos las calificaciones no centradas, entonces estaríamos suponiendo que las no evaluadas están calificadas muy mal (\\(0\\), por abajo de \\(1\\),\\(2\\),\\(3\\),\\(4\\),\\(5\\)). Si calculamos similitud entre usuarios de esta forma, las distancia coseno es simplemente el coeficiente de correlación. Nótese que estamos calculando similitud entre items, centrando por usuario, y esto no es lo mismo que correlación entre columnas. Ejemplo: ¿cómo se ven las calificaciones de películas similares/no similares? Centramos las calificaciones por usuario y seleccionamos tres películas que pueden ser interesantes. dat_entrena_c &lt;- dat_entrena |&gt; group_by(usuario_id) |&gt; mutate(calif_c = calif - mean(calif)) ## calculamos un id secuencial. dat_entrena_c$id_seq &lt;- as.numeric(factor(dat_entrena_c$usuario_id)) filter(pelis_nombres, str_detect(nombre,&#39;Gremlins&#39;)) ## # A tibble: 3 × 3 ## peli_id año nombre ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2897 1990 Gremlins 2: The New Batch ## 2 6482 1984 Gremlins ## 3 10113 2004 The Wiggles: Whoo Hoo! Wiggly Gremlins! filter(pelis_nombres, str_detect(nombre,&#39;Harry Met&#39;)) ## # A tibble: 2 × 3 ## peli_id año nombre ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2660 1989 When Harry Met Sally ## 2 11850 2003 Dumb and Dumberer: When Harry Met Lloyd dat_1 &lt;- filter(dat_entrena_c, peli_id==2897) # Gremlins 2 dat_2 &lt;- filter(dat_entrena_c, peli_id==6482) # Gremlins 1 dat_3 &lt;- filter(dat_entrena_c, peli_id==2660) # WHMS Juntamos usuarios que calificaron cada par: comunes &lt;- inner_join(dat_1[, c(&#39;usuario_id&#39;,&#39;calif_c&#39;)], dat_2[, c(&#39;usuario_id&#39;,&#39;calif_c&#39;)] |&gt; rename(calif_c_2=calif_c)) ## Joining, by = &quot;usuario_id&quot; comunes_2 &lt;- inner_join(dat_1[, c(&#39;usuario_id&#39;,&#39;calif_c&#39;)], dat_3[, c(&#39;usuario_id&#39;,&#39;calif_c&#39;)] |&gt; rename(calif_c_2=calif_c)) ## Joining, by = &quot;usuario_id&quot; Y ahora graficamos. ¿Por qué se ven bandas en estas gráficas? ggplot(comunes, aes(x=calif_c, y=calif_c_2)) + geom_jitter(width = 0.2, height = 0.2, alpha = 0.5) + geom_smooth() + xlab(&#39;Gremlins 2&#39;) + ylab(&#39;Gremlins 1&#39;) ggplot(comunes_2, aes(x=calif_c, y=calif_c_2)) + geom_jitter(width = 0.2, height = 0.2, alpha = 0.5) + geom_smooth() + xlab(&#39;Gremlins 2&#39;) + ylab(&#39;When Harry met Sally&#39;) Pregunta: ¿por qué los datos se ven en bandas? Y calculamos la similitud coseno: sim_cos(comunes$calif_c, comunes$calif_c_2) ## [1] 0.1769532 sim_cos(comunes_2$calif_c, comunes_2$calif_c_2) ## [1] -0.3156217 Así que las dos Gremlins son algo similares, pero Gremlins \\(1\\) y Harry Met Sally no son similares. Podemos ahora seleccionar algunas películas y ver cuáles son películas similares que nos podrían ayudar a hacer recomendaciones: dat_entrena_2 &lt;- dat_entrena_c |&gt; ungroup() |&gt; select(peli_id, id_seq, calif_c) ejemplos &lt;- function(pelicula, min_pares = 100){ mi_peli &lt;- filter(dat_entrena_2, peli_id==pelicula) |&gt; rename(peli_id_1 = peli_id, calif_c_1 = calif_c) # vamos a calcular todas las similitudes con mi_peli - esto no es buena # idea y discutiremos más adelante cómo evitarlo datos_comp &lt;- left_join(dat_entrena_2, mi_peli) |&gt; filter(!is.na(peli_id_1) ) # calcular similitudes out_sum &lt;- datos_comp |&gt; group_by(peli_id) |&gt; summarise(similitud = sim_cos(calif_c, calif_c_1), num_pares = n()) |&gt; filter(num_pares &gt; min_pares) |&gt; left_join(pelis_nombres) out_sum |&gt; arrange(desc(similitud)) } Nótese que las similitudes aparentan ser ruidosas si no filtramos por número de evaluaciones: ejemplos(8199) |&gt; head(20) |&gt; knitr::kable() ## Joining, by = &quot;id_seq&quot; ## Joining, by = &quot;peli_id&quot; peli_id similitud num_pares año nombre 8199 1.0000000 1379 1985 The Purple Rose of Cairo 6247 0.4858263 431 1984 Broadway Danny Rose 16171 0.4670051 618 1987 Radio Days 1058 0.3940953 485 1972 Play it Again Sam 6610 0.3814492 106 1965 The Pawnbroker 17341 0.3748542 545 1983 Zelig 1234 0.3526710 115 1994 Crooklyn 12896 0.3512115 754 1994 Bullets Over Broadway 9540 0.3509817 101 1987 The Decalogue 2599 0.3503702 112 1959 Black Orpheus 16709 0.3480806 342 1980 Stardust Memories 5577 0.3480255 158 1957 Throne of Blood 10605 0.3479292 148 1976 The Front 15808 0.3468051 165 1996 Citizen Ruth 10918 0.3370929 139 1987 Black Adder III 10601 0.3349099 152 1972 The Discreet Charm of the Bourgeoisie 16889 0.3294090 408 1969 Take the Money and Run 6070 0.3217371 117 1989 Black Adder IV 11301 0.3118192 137 1973 The Wicker Man 11720 0.3050482 108 1978 Goin’ South ejemplos(6807) |&gt; head(20) |&gt; knitr::kable() ## Joining, by = &quot;id_seq&quot; ## Joining, by = &quot;peli_id&quot; peli_id similitud num_pares año nombre 6807 1.0000000 2075 1963 8 1/2 16241 0.7429705 677 1960 La Dolce Vita 15303 0.6768162 118 1952 Umberto D. 9485 0.6680970 269 1967 Persona 5605 0.6356236 141 1946 Children of Paradise 15786 0.6017523 564 1957 Wild Strawberries 2965 0.5878489 760 1957 The Seventh Seal 13377 0.5808940 123 1963 Winter Light 10276 0.5738983 654 1950 Rashomon 1708 0.5719259 328 1936 Modern Times 1735 0.5712199 314 1974 Amarcord 7912 0.5677752 243 1960 L’Avventura 12721 0.5674579 283 1952 Ikiru 10661 0.5665439 169 1953 Tokyo Story 17184 0.5612483 240 1939 The Rules of the Game 10277 0.5571376 717 1948 The Bicycle Thief 12871 0.5523090 123 1926 Our Hospitality / Sherlock Jr. 8790 0.5516619 112 1929 Man with the Movie Camera 840 0.5498725 212 1941 The Lady Eve 15016 0.5497364 227 1928 The Passion of Joan of Arc El problema otra vez es similitudes ruidosas que provienen de pocas evaluaciones en común. Ejercicio Intenta con otras películas que te interesen, y prueba usando un mínimo distinto de pares para incluir en la lista ejemplos(11271) |&gt; head(20) ## Joining, by = &quot;id_seq&quot; ## Joining, by = &quot;peli_id&quot; ## # A tibble: 20 × 5 ## peli_id similitud num_pares año nombre ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 11271 1 5583 1994 Friends: Season 1 ## 2 5309 0.944 2679 1997 Friends: Season 4 ## 3 3078 0.938 3187 1994 The Best of Friends: Season 2 ## 4 5414 0.934 3335 1996 Friends: Season 3 ## 5 2938 0.930 3000 1994 The Best of Friends: Season 1 ## 6 15307 0.924 2931 1996 The Best of Friends: Season 3 ## 7 8438 0.920 2903 1998 Friends: Season 5 ## 8 7158 0.919 2094 1997 The Best of Friends: Season 4 ## 9 2942 0.913 2708 1999 Friends: Season 6 ## 10 15689 0.912 2894 1994 The Best of Friends: Vol. 1 ## 11 9909 0.909 2167 2002 Friends: Season 9 ## 12 5837 0.908 2554 1999 Friends: Season 7 ## 13 16083 0.907 1960 1994 The Best of Friends: Vol. 2 ## 14 1877 0.907 3123 1995 Friends: Season 2 ## 15 1256 0.898 1723 1994 The Best of Friends: Vol. 4 ## 16 15777 0.898 2478 2001 Friends: Season 8 ## 17 14283 0.886 1544 1994 The Best of Friends: Vol. 3 ## 18 7780 0.858 2481 2004 Friends: The Series Finale ## 19 13042 0.629 416 2001 Will &amp; Grace: Season 4 ## 20 1915 0.623 169 2000 Law &amp; Order: Special Victims Unit: The Sec… ejemplos(11929) |&gt; head(20) ## Joining, by = &quot;id_seq&quot; ## Joining, by = &quot;peli_id&quot; ## # A tibble: 20 × 5 ## peli_id similitud num_pares año nombre ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 11929 1 1955 1997 Anaconda ## 2 11687 0.716 274 1994 Street Fighter ## 3 15698 0.685 175 1995 Fair Game ## 4 13417 0.680 116 1997 Home Alone 3 ## 5 1100 0.671 169 2000 Dr. T &amp; the Women ## 6 12149 0.671 708 1997 Speed 2: Cruise Control ## 7 17221 0.670 645 1993 Look Who&#39;s Talking Now ## 8 5366 0.662 139 1999 The Mod Squad ## 9 16146 0.654 146 2003 The Real Cancun ## 10 5106 0.652 129 1994 House Party 3 ## 11 12171 0.648 452 2000 Battlefield Earth ## 12 16283 0.645 170 1994 Cops &amp; Robbersons ## 13 15958 0.644 102 1994 Darkman II: The Return of Durant ## 14 12517 0.643 284 2003 Gigli ## 15 13607 0.642 137 1992 Stop! Or My Mom Will Shoot ## 16 1363 0.640 200 1993 Leprechaun ## 17 12349 0.639 227 1993 Super Mario Bros. ## 18 7142 0.637 118 1996 Barb Wire ## 19 3573 0.635 286 1993 Cop and a Half ## 20 5005 0.634 290 1995 Vampire in Brooklyn ejemplos(2660) |&gt; head(20) ## Joining, by = &quot;id_seq&quot; ## Joining, by = &quot;peli_id&quot; ## # A tibble: 20 × 5 ## peli_id similitud num_pares año nombre ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2660 1 16940 1989 When Harry Met Sally ## 2 16722 0.509 149 2001 The Godfather Trilogy: Bonus Material ## 3 12530 0.458 2861 2004 Sex and the City: Season 6: Part 2 ## 4 8272 0.451 187 1960 The Andy Griffith Show: Season 1 ## 5 14648 0.447 680 2003 Finding Nemo (Full-screen) ## 6 11975 0.441 293 2005 Mad Hot Ballroom ## 7 16711 0.441 3305 2003 Sex and the City: Season 6: Part 1 ## 8 4427 0.433 744 2001 The West Wing: Season 3 ## 9 12184 0.430 2676 2002 Sex and the City: Season 5 ## 10 13073 0.426 2414 1946 It&#39;s a Wonderful Life ## 11 7639 0.423 171 2004 Star Wars Trilogy: Bonus Material ## 12 13556 0.422 923 1999 The West Wing: Season 2 ## 13 14567 0.421 136 1959 Ben-Hur: Collector&#39;s Edition: Bonus Materi… ## 14 14550 0.420 11743 1994 The Shawshank Redemption: Special Edition ## 15 270 0.420 3618 2001 Sex and the City: Season 4 ## 16 3456 0.419 553 2004 Lost: Season 1 ## 17 9909 0.418 1763 2002 Friends: Season 9 ## 18 16674 0.414 103 1994 Forrest Gump: Bonus Material ## 19 1542 0.410 9277 1993 Sleepless in Seattle ## 20 12870 0.408 9397 1993 Schindler&#39;s List 5.5.2 Implementación Si queremos implementar este tipo de filtrado colaborativo (por similitud), el ejemplo de arriba no es práctico pues tenemos que calcular todas las posibles similitudes. Sin embargo, como nos interesa principalmente encontrar los pares de similitud alta, podemos usar LSH: Empezamos haciendo LSH de las películas usando el método de hiperplanos aleatorios como función hash (pues este es el método que captura distancias coseno bajas). Nuestro resultado son todos los items o películas agrupadas en cubetas. Podemos procesar las cubetas para eliminar falsos positivos (o items con muy pocas evaluaciones). Ahora queremos estimar el rating del usuario \\(i\\) de una película \\(j\\) que no ha visto. Extraemos las cubetas donde cae la película \\(j\\), y extraemos todos los items. Entre todos estos items, extraemos los que ha visto el usuario \\(i\\), y aplicamos el promedio (5.1). Observaciones En principio, este análisis podría hacerse usando similitud entre usuarios en lugar de items. En la práctica (ver (Leskovec, Rajaraman, and Ullman 2014)), el enfoque de similitud entre items es superior, pues similitud es un concepto que tiene más sentido en items que en usuarios (los usuarios pueden tener varios intereses traslapados). Nótese que en ningún momento tuvimos que extraer variables de películas, imágenes, libros, etc o lo que sea que estamos recomendando. Esta es una fortaleza del filtrado colaborativo. Por otro lado, cuando tenemos pocas evaluaciones o calificaciones este método no funciona bien (por ejemplo, no podemos calcular las similitudes pues no hay traslape a lo largo de usuarios). En este caso, este método puede combinarse con otros (por ejemplo, agregar una parte basado en modelos de gusto por género, año, etc.) Referencias "],["dimensiones-latentes-para-recomendación.html", "6 Dimensiones latentes para recomendación 6.1 Factorización de matrices 6.2 Mínimos cuadrados alternados 6.3 Retroalimentación implícita", " 6 Dimensiones latentes para recomendación En las similitudes que vimos arriba, es razonable pensar que hay ciertos “conceptos” que agrupan o separan películas, y así mismo, que los usuarios se distinguen por el gusto o no que tienen por estos “conceptos.” En esta parte, consideramos la idea de utilizar reducción de dimensionalidad para hacer recomendaciones. Esta idea propone que hay ciertos factores latentes (no observados) que describen películas con “contenido implícito similar,” y usuarios según su interés en esa dimensión. Otra manera de llamar estos factores latentes es embedding: buscamos un embedding (una representación numérica en cierta dimensión no muy alta) que nos permita predecir el gusto de un usuario por una película. Este método nos permitirá también controlar mejor los resultados ruidosos que obtuvimos en los ejemplos anteriores (usando regularización y reducción de dimensión). 6.0.1 Ejemplo: una dimensión latente Por ejemplo: consideramos una dimensión de películas serias contra películas divertidas. \\(3\\) películas podrían describirse con \\[v=(-2,0,1)\\], lo que interpretamos como la película \\(1\\) es divertida (negativa en seriedad-diversión), la película \\(2\\) está en promedio, y la película \\(3\\) es más seria que las dos anteriores. Por otro lado, tenemos descriptores de 5 usuarios: \\[u=(2,3,-3,0,1)\\] que dice que a los primeros dos usuarios les gustan las películas serias, al tercero le gustan las divertidas, y los dos últimos no tienen preferencia clara a lo largo de esta dimensión. Qusiéramos predecir el gusto usando estos dos vectores. Nuestras predicciones (considerando que \\(u\\) y \\(v\\) son matrices de una columna) serían simplemente \\[\\widetilde{X} = u v^t\\] u &lt;- c(2,3,-3,0,1) v &lt;- c(-2,0,1) gustos &lt;- u %*% t(v) gustos ## [,1] [,2] [,3] ## [1,] -4 0 2 ## [2,] -6 0 3 ## [3,] 6 0 -3 ## [4,] 0 0 0 ## [5,] -2 0 1 Así que al usuario \\(1\\) le recomndamos la película \\(3\\), pero al usuario \\(3\\) le recomendamos la película \\(1\\). La idea es entonces encontrar pesos para películas \\(u\\) y para usuarios \\(v\\) de forma que \\(X\\approx \\widetilde{X} = uv^t\\): podemos reproducir las calificaciones observadas a partir de nuestro modelo de factores latentes. Nótese sin embargo que hay varias dimensiones que pueden describir a películas y usuarios: por ejemplo, seria-divertida, artística-hollywood, ciencia ficción, con/sin violencia, etc. Podemos proponer más dimensiones latentes de la siguiente forma: 6.0.2 Ejemplo: dos dimensiones latentes Tenemos la dimensión anterior de seria-divertida v_1 &lt;- c(-2,0,1) u_1 &lt;- c(2,3,-3,0,1) Y supongamos que tenemos otra dimensión con violencia - sin violencia v_2 &lt;- c(-3,2,2) u_2 &lt;- c(-3,-3,0,-2,4) Que quiere decir que las películas \\(2, 3\\) tienen volencia, pero la película \\(1\\) no. Por otra parte, a los usuarios \\(1,2\\) y \\(5\\) no les gustan las películas con violencia, mientras que al usuario \\(5\\) si les gustan. La idea ahora es que el gusto de una persona por una película se escribe como combinación de las dos dimensiones. Por ejemplo, para la persona \\(1\\) tenemos, y la película \\(1\\), empezamos haciendo u_1[1]*v_1[1] ## [1] -4 u_2[1]*v_2[1] ## [1] 9 lo que quiere decir que el hecho de que la película \\(1\\) no sea seria le resta \\(4\\) en gusto (pues la película \\(1\\) está del lado “divertido”), pero le suma \\(9\\) en gusto, pues es una película sin violencia y esta persona está del lado “sin violencia.” Sumamos para encontrar el gusto total u_1[1]*v_1[1] + u_2[1]*v_2[1] ## [1] 5 Para calcular los gustos sobre todas las personas y películas, haríamos U &lt;- cbind(u_1, u_2) V &lt;- cbind(v_1, v_2) U ## u_1 u_2 ## [1,] 2 -3 ## [2,] 3 -3 ## [3,] -3 0 ## [4,] 0 -2 ## [5,] 1 4 V ## v_1 v_2 ## [1,] -2 -3 ## [2,] 0 2 ## [3,] 1 2 U %*% t(V) ## [,1] [,2] [,3] ## [1,] 5 -6 -4 ## [2,] 3 -6 -3 ## [3,] 6 0 -3 ## [4,] 6 -4 -4 ## [5,] -14 8 9 El renglón \\(j\\) de \\(U\\) son los valores en las dimensiones latentes para la película \\(i\\) (descriptores de usuarios). El renglón \\(j\\) de \\(V\\) son los valores en las dimensiones latentes para el usuario \\(j\\) (descriptores de películas) Con \\(k\\) dimensiones latentes, el modelo que proponemos es: \\[\\widetilde{X} = UV^t\\] donde \\(U\\) es una matrix de \\(n\\times k\\) (\\(n=\\) número de usuarios), y \\(V\\) es una matriz de \\(p \\times k\\), donde \\(p\\) es el número de películas. Buscamos que, si \\(X\\) son las verdaderas calificaciones, entonces \\[X\\approx \\widetilde{X}.\\] y nótese que esta aproximación es en el sentido de las entradas de \\(X\\) que son observadas. Sin embargo, \\(\\widetilde{X}\\) nos da predicciones para todos los pares película-persona. Bajo este modelo, la predicción para el usuario \\(i\\) y la película \\(j\\) es la siguiente suma sobre las dimensiones latentes: \\[\\widetilde{x}_{ij} =\\sum_k u_{ik} v_{jk}\\] que expresa el hecho de que el gusto de \\(i\\) por \\(j\\) depende de una combinación (suma) de factores latentes de películas ponderados por gusto por esos factores del usuario. El número de factores latentes \\(k\\) debe ser seleccionado (por ejemplo, según el error de validación). Dado \\(k\\), para encontrar \\(U\\) y \\(V\\) (un total de \\(k(n+p)\\) parámetros) buscamos minimizar \\[\\sum_{(i,j)\\, obs} (x_{ij}-\\widetilde{x}_{ij})^2,\\] que también podemos escribir este problema (recuérdese que \\(u_i\\) y \\(v_j\\) aquí son vectores renglón) como \\[\\min_{U,V}\\sum_{(i,j)\\, obs} (x_{ij}-u_iv_j^t)^2\\] donde \\(u_i\\) es el renglón \\(i\\)-esimo de \\(U\\) (gustos latentes del usuario \\(i\\) en cada dimensión), y \\(v_j\\) es el renglón \\(j\\)-ésimo de la matriz \\(V\\) (calificación latente de la película en cada dimensión) ¿Por qué funciona la idea de factores latentes? El método de factorización de matrices de grado bajo (\\(k\\)) funciona compartiendo información a lo largo de películas y usuarios. Como tenemos que ajustar los datos observados, y solo tenemos a nuestra disposición \\(k\\) descriptores para cada película y usuario, una minimización exitosa captura regularidades en los datos. Es importante que la representación sea de grado relativamente bajo, pues esta “compresión” es la que permite que las dimensiones latentes capturen regularidades que están en los datos observados (que esperamos encontrar en el proceso de ajuste). Al reducir la dimensión, también funcionan mejor métricas relativamente simples para calcular similitud entre usuarios o películas. Por ejemplo, supongamos que el gusto por las películas sólo depende de una dimensión sería - divertida. Si ajustamos un modelo de un solo factor latente, un mínimo se alcanzaría separando con la dimensión latente las películas serias de las divertidas, y los usuarios que prefieren películas serias o divertidas. Esta sería una buena explicación de los datos observados, y las predicciones para películas no vistas sería buena usando simplemente el valor en seriedad de la película (extraída de otras personas con gustos divertido o serio) y el gusto por seriedad de esa persona (extraida de la observación de que le gustan otras películas serias u otras divertidas). 6.0.3 Combinación con modelo base Podemos usar también ideas de nuestro modelo base y modelar desviaciones en lugar de calificaciones directamente: Si \\(X^0\\) son las predicciones del modelo de referencia, y \\[R = X-X^0\\] son los residuales del modelo base, buscamos mejor \\[R\\approx \\widetilde{X} = UV^t\\] de manera que las predicciones finales son \\[X^0 + \\widetilde{X}\\] Veremos también más adelante cómo regularizar estos sesgos como parte de la construcción del modelo. 6.1 Factorización de matrices Como vimos arriba, reexpresamos nuestro problema como un problema de factorización de matrices (encontrar \\(U\\) y \\(V\\)). Hay varias alternativas populares para atacarlo: Descomposición en valores singulares (SVD). Mínimos cuadrados alternados. Descenso en gradiente estocástico. No vamos a ver más de este enfoque de SVD que discutimos anteriormente, pues no es del todo apropiado: nuestras matrices tienen muchos datos faltantes, y SVD no está diseñado para lidiar con este problema. Se pueden hacer ciertas imputaciones (por ejemplo, insertar 0’s una vez que centramos por usuario), pero los siguientes dos métodos están mejor adaptados para nuestro problema. 6.2 Mínimos cuadrados alternados Supongamos entonces que queremos encontrar matrices \\(U\\) y \\(V\\), donde \\(U\\) es una matrix de \\(n \\times k\\) (\\(n=\\) número de usuarios), y \\(V\\) es una matriz de \\(p \\times k\\), donde \\(p\\) es el número de películas que nos de una aproximación de la matrix \\(X\\) de calificaciones \\[ X \\approx UV^t \\] Ahora supongamos que conocemos \\(V_1\\). Si este es el caso, entonces queremos resolver para \\(U_1\\): \\[ \\min_{U_1}|| X - U_1V_1^t||_{obs}^2\\] Como \\(V_1^t\\) están fijas, este es un problema de mínimos cuadrados usual, y puede resolverse analíticamente (o usar descenso en gradiente, que es simple de calcular de forma analítica) para encontrar \\(U_1\\). Una vez que encontramos \\(U_1\\), la fijamos, e intentamos ahora resolver para \\(V\\): \\[ \\min_{V_2}|| X - U_1V_2^t||_{obs}^2\\] Y una vez que encontramos \\(V_2\\) resolvemos \\[ \\min_{U_2}|| X - U_2V_2^t||_{obs}^2\\] Continuamos este proceso hasta encontrar un mínimo local o hasta cierto número de iteraciones. Para inicializar \\(V_1\\), en (Zhou et al. 2008) se recomienda tomar como primer renglón el promedio de las calificaciones de las películas, y el resto números aleatorios chicos (por ejemplo \\(U(0,1)\\)). También pueden inicializarse con números aleatorios chicos las dos matrices. 6.2.1 Mínimos cuadrados alternados con regularización Para agregar regularización y lidiar con los datos ralos, podemos incluir un coeficiente adicional. Minimizamos entonces (como en (Zhou et al. 2008)): \\[\\min_{U,V}\\sum_{(i,j)\\, obs} (x_{ij}-u_i^tv_j)^2 + \\lambda \\left ( \\sum_i n_{i}||u_i||^2 + \\sum_j m_{j} ||v_j||^2 \\right)\\] y modificamos de manera correspondiente cada paso de mínimos cuadrados mostrado arriba. \\(n_{i}\\) es el número de evaluaciones del usuario \\(i\\), y \\(m_j\\) es el número de evaluaciones de la película \\(j\\). Observaciones: Nótese que penalizamos el tamaños de los vectores \\(u_i\\) y \\(v_j\\) para evitar sobreajuste (como en regresión ridge). Nótese también que los pesos \\(n_i\\) y \\(m_j\\) en esta regularización hace comparables el término que aparece en la suma de los residuales al cuadrado (la primera suma), y el término de regularización: por ejemplo, si el usuario \\(i\\) hizo \\(n_i\\) evaluaciones, entonces habrá \\(n_i\\) términos en la suma de la izquierda. Lo mismo podemos decir acerca de las películas. Este no es el único término de regularización posible. Por ejemplo, podríamos no usar los pesos \\(n_i\\) y \\(m_j\\), y obtendríamos un esquema razonable también, donde hay más regularización relativa para usuarios/películas que tengan pocas evaluaciones. Este método está implementado en spark. La implementación está basada parcialmente en (Zhou et al. 2008). La inicialización es diferente en spark, ver el código, donde cada renglón se inicializa con un vector de \\(N(0,1)\\) normalizado. En este caso, copiamos nuestra tabla a spark (nota: esto es algo que normalmente no haríamos, los datos habrían sido cargados en el cluster de spark de otra forma): library(tidyverse) library(sparklyr) # configuración para spark spark_install(version = &quot;3.1.2&quot;) config &lt;- spark_config() config$`spark.env.SPARK_LOCAL_IP.local` &lt;- &quot;0.0.0.0&quot; config$`sparklyr.shell.driver-memory` &lt;- &quot;8G&quot; config$spark.executor.memory &lt;- &quot;2G&quot; # conectar con &quot;cluster&quot; local sc &lt;- spark_connect(master = &quot;local&quot;, config = config) spark_set_checkpoint_dir(sc, &#39;./checkpoint&#39;) dat_tbl &lt;- spark_read_csv(sc, name=&quot;netflix&quot;, &quot;../datos/netflix/dat_muestra_nflix.csv&quot;) dat_tbl &lt;- dat_tbl |&gt; select(-fecha) usuario_val &lt;- dat_tbl |&gt; select(usuario_id) |&gt; sdf_distinct() |&gt; sdf_sample(fraction = 0.1) |&gt; compute(&quot;usuario_val&quot;) pelicula_val &lt;- dat_tbl |&gt; select(peli_id) |&gt; sdf_distinct() |&gt; sdf_sample(fraction = 0.1) |&gt; compute(&quot;pelicula_val&quot;) valida_tbl &lt;- dat_tbl |&gt; inner_join(usuario_val) |&gt; inner_join(pelicula_val) |&gt; compute(&quot;valida_tbl&quot;) ## Joining, by = &quot;usuario_id&quot; ## Joining, by = &quot;peli_id&quot; entrena_tbl &lt;- dat_tbl |&gt; anti_join(valida_tbl) |&gt; compute(&quot;entrena_tbl&quot;) ## Joining, by = c(&quot;peli_id&quot;, &quot;usuario_id_orig&quot;, &quot;calif&quot;, &quot;usuario_id&quot;) entrena_tbl |&gt; tally() ## # Source: spark&lt;?&gt; [?? x 1] ## n ## &lt;dbl&gt; ## 1 20794212 valida_tbl |&gt; tally() ## # Source: spark&lt;?&gt; [?? x 1] ## n ## &lt;dbl&gt; ## 1 189332 Vamos a hacer primero una descomposición en \\(15\\) factores, con regularización relativamente alta: modelo &lt;- ml_als(entrena_tbl, rating_col = &#39;calif&#39;, user_col = &#39;usuario_id&#39;, item_col = &#39;peli_id&#39;, rank = 15, reg_param = 0.05, checkpoint_interval = 4, max_iter = 50) # Nota: checkpoint evita que la gráfica de cálculo # sea demasiado grande. Cada 4 iteraciones hace una # nueva gráfica con los resultados de la última iteración. modelo ## ALSModel (Transformer) ## &lt;als__bde359f9_618d_44b9_8b75_23fcc4e83f55&gt; ## (Parameters -- Column Names) ## cold_start_strategy: nan ## item_col: peli_id ## prediction_col: prediction ## user_col: usuario_id ## (Transformer Info) ## item_factors: &lt;tbl_spark&gt; ## rank: int 15 ## recommend_for_all_items: &lt;function&gt; ## recommend_for_all_users: &lt;function&gt; ## user_factors: &lt;tbl_spark&gt; Hacemos predicciones para el conjunto de validación: preds &lt;- ml_predict(modelo, valida_tbl) |&gt; mutate(prediction = ifelse(isnan(prediction), 3.5, prediction)) ml_regression_evaluator(preds, label_col = &quot;calif&quot;, prediction_col = &quot;prediction&quot;, metric_name = &quot;rmse&quot;) ## [1] 0.8354879 Y podemos traer a R los datos de validación (que son chicos) para examinar: preds_df &lt;- preds |&gt; collect() #traemos a R con collect ggplot(preds_df, aes(x = prediction)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Y redujimos considerablemente el error del modelo base. Examinamos ahora las dimensiones asociadas con películas: modelo$item_factors ## # Source: spark&lt;?&gt; [?? x 3] ## id features features_1 ## &lt;int&gt; &lt;list&gt; &lt;dbl&gt; ## 1 10 &lt;dbl [15]&gt; -0.606 ## 2 20 &lt;dbl [15]&gt; -0.625 ## 3 30 &lt;dbl [15]&gt; -0.767 ## 4 40 &lt;dbl [15]&gt; -0.598 ## 5 50 &lt;dbl [15]&gt; -0.736 ## 6 60 &lt;dbl [15]&gt; -0.235 ## 7 70 &lt;dbl [15]&gt; -0.537 ## 8 80 &lt;dbl [15]&gt; -1.29 ## 9 90 &lt;dbl [15]&gt; -0.532 ## 10 100 &lt;dbl [15]&gt; -0.788 ## # … with more rows V_df &lt;- modelo$item_factors |&gt; select(id, features) |&gt; collect() |&gt; unnest_wider(features, names_sep = &quot;_&quot;) Nota: La columna features contiene la misma información de feature_1,feature_2,…, pero en forma de lista. Examinemos la interpretación de los factores latentes de las películas. pelis_nombres &lt;- read_csv(&#39;../datos/netflix/movies_title_fix.csv&#39;, col_names = FALSE, na = c(&quot;&quot;, &quot;NA&quot;, &quot;NULL&quot;)) ## Rows: 17770 Columns: 3 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): X3 ## dbl (2): X1, X2 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. names(pelis_nombres) &lt;- c(&#39;peli_id&#39;,&#39;año&#39;,&#39;nombre&#39;) medias_peliculas &lt;- entrena_tbl |&gt; group_by(peli_id) |&gt; summarise(num_calif_peli = n(), media_peli = mean(calif)) |&gt; collect() ## Warning: Missing values are always removed in SQL. ## Use `mean(x, na.rm = TRUE)` to silence this warning ## This warning is displayed only once per session. latentes_pelis &lt;- V_df |&gt; rename(peli_id = id) |&gt; left_join(pelis_nombres |&gt; left_join(medias_peliculas)) ## Joining, by = &quot;peli_id&quot; ## Joining, by = &quot;peli_id&quot; latentes_pelis &lt;- latentes_pelis |&gt; mutate(num_grupo = ntile(num_calif_peli, 10)) Podemos examinar las dimensiones latentes: top_tail &lt;- function(latentes_pelis, feature){ top_df &lt;- arrange(latentes_pelis, {{ feature }} ) |&gt; select(nombre, {{ feature }}, media_peli, num_calif_peli) |&gt; filter(num_calif_peli &gt; 2000) |&gt; head(100) tail_df &lt;- arrange(latentes_pelis, desc({{ feature }}) ) |&gt; select(nombre, {{ feature }}, media_peli, num_calif_peli) |&gt; filter(num_calif_peli &gt; 2000) |&gt; head(100) print(top_df) print(tail_df) bind_rows(top_df, tail_df) } res &lt;- top_tail(latentes_pelis, features_3) ## # A tibble: 100 × 4 ## nombre features_3 media_peli num_calif_peli ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 The Best of Friends: Vol. 2 -0.929 4.11 3024 ## 2 Friends: The Series Finale -0.908 4.12 3809 ## 3 Friends: Season 3 -0.889 4.08 4844 ## 4 The Best of Friends: Vol. 1 -0.888 4.04 4999 ## 5 The Best of Friends: Season 3 -0.884 4.16 3894 ## 6 The Best of Friends: Season 4 -0.883 4.24 3406 ## 7 Friends: Season 4 -0.866 4.15 4853 ## 8 Friends: Season 1 -0.864 4.10 5583 ## 9 Friends: Season 5 -0.858 4.23 4386 ## 10 Friends: Season 6 -0.853 4.28 3975 ## # … with 90 more rows ## # A tibble: 100 × 4 ## nombre features_3 media_peli num_calif_peli ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Brazil 1.21 3.81 4557 ## 2 Hellraiser 1.16 3.39 3582 ## 3 Evil Dead 2: Dead by Dawn 1.15 3.72 4148 ## 4 Dawn of the Dead 1.14 3.59 3432 ## 5 Night of the Living Dead 1.13 3.55 4722 ## 6 The Evil Dead 1.10 3.76 4240 ## 7 Repo Man 1.07 3.47 2662 ## 8 Dead Man 1.06 3.63 2345 ## 9 A Clockwork Orange 1.02 3.72 15551 ## 10 Blade Runner 1.01 3.96 13298 ## # … with 90 more rows Otra dimensión latente: res &lt;- top_tail(latentes_pelis, features_4) ## # A tibble: 100 × 4 ## nombre features_4 media_peli num_calif_peli ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 But I&#39;m a Cheerleader -0.671 3.38 3087 ## 2 The Craft -0.585 3.43 4000 ## 3 Bio-Dome -0.566 2.80 2249 ## 4 Romy and Michele&#39;s High School Reunion -0.562 3.18 3696 ## 5 Hocus Pocus -0.553 3.33 4131 ## 6 The Sweetest Thing -0.549 3.16 3377 ## 7 Now and Then -0.539 3.60 2712 ## 8 Don&#39;t Tell Mom the Babysitter&#39;s Dead -0.530 3.18 3585 ## 9 Practical Magic -0.526 3.57 6847 ## 10 Encino Man -0.510 3.11 4148 ## # … with 90 more rows ## # A tibble: 100 × 4 ## nombre features_4 media_peli num_calif_peli ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Patton 1.18 3.99 7820 ## 2 Lawrence of Arabia 1.10 4.13 8092 ## 3 The Longest Day 1.09 4.03 2403 ## 4 Ben-Hur: Collector&#39;s Edition 1.09 3.97 5490 ## 5 The Bridge on the River Kwai 1.07 4.12 8184 ## 6 High Noon 1.03 3.91 4473 ## 7 The Treasure of the Sierra Madre 1.03 4.01 2903 ## 8 The Godfather 1.02 4.50 22253 ## 9 Spartacus 1.01 3.87 4673 ## 10 Citizen Kane 1.01 4.03 11727 ## # … with 90 more rows Nota: Podemos usar ml_recommend para producir recomendaciones de películas para usuarios, o para cada película los usuarios más afines. #top_recom &lt;- ml_recommend(modelo, type = &quot;items&quot;, n = 1) sparklyr::spark_disconnect_all() ## [1] 1 Ejercicio Examina otras dimensiones latentes, ¿qué puedes interpretar? 6.3 Retroalimentación implícita Esta sección está basada en (Hu, Koren, and Volinsky 2008). En el ejemplo que vimos arriba, la retroalimentación es expícita en el sentido de que los usuarios califican los artículos (\\(1-\\) no me gustó nada, hasta \\(5-\\) me gustó mucho). Sin embargo, es común encontrar casos donde no existe esta retroalimentación explícita, y solo tenemos medidas del gusto implícito, por ejemplo: Cuántas veces un usuario ha pedido un cierto artículo. Qué porcentaje del programa fue visto. Cuánto tiempo pasó en la página web. Cuántas veces oyó una canción. Estos datos tienen la ventaja de que describen acciones del usuario, en lugar de un rating que puede estar influido por sesgos de imagen o de la calificación que “debería” tener un artículo además de la preferencia: quizá disfruto muchísimo Buffy the Vampire Slayer, pero lo califico con un \\(3\\), aunque un documental de ballenas que simplemente me gustó le pongo un \\(5\\). En los datos implícitos se vería de todas formas mi consumo frecuente de Buffy the Vampire Slayer, y quizá unos cuantos de documentales famosos. Sea \\(r_{ij}\\) una medida implícita como las mencionadas arriba, para el usuario \\(i\\) y el artículo \\(j\\). Ponemos \\(r_{i,j}=0\\) cuando no se ha observado interacción entre este usuario y el artículo. Una diferencia importante con los ratings explícitos es que los datos implícitos son en un sentido menos informativos que los explícitos: Puede ser que el valor de \\(r_{ij}\\) sea relativamente bajo (pocas interacciones), pero de todas formas se trate de un artículo que es muy preferido (por ejemplo, solo vi Star Wars I una vez, pero me gusta mucho, o nunca he encontrado Star Wars I en el catálogo). Esto no pasa con los ratings, pues ratings bajos indican baja preferencia. Sin embargo, estamos seguros de que niveles altos de interacción (oyó muchas veces una canción, etc.), es indicación de preferencia alta. Usualmente la medida \\(r_{ij}\\) no tiene faltantes, o tiene un valor implícito para faltantes. Por ejemplo, si la medida es % de la película que vi, todas las películas con las que no he interactuado tienen \\(r_{ij}=0\\). Así que en nuestro modelo no necesariamente queremos predecir directamente la variable \\(r_{ij}\\): puede haber artículos con predicción baja de \\(r_{ij}\\) que descubramos de todas formas van a ser altamente preferidos. Un modelo que haga una predicción de \\(r_{îj}\\) reflejaría más los patrones de consumo actual en lugar de permitirnos descubrir artículos preferidos con los que no necesariamente existe interacción. 6.3.1 Ejemplo Consideremos los siguientes usuarios, donde medimos por ejemplo el número de minutos que pasó cada usuario viendo cada película: imp &lt;- tibble(usuario = 1:6, StarWars1 = c(0, 0, 0, 150, 300, 250), StarWars2 = c(250, 200, 0, 200, 220,180), StarWars3 = c(0, 250, 300, 0, 0, 0), Psycho = c(5, 1, 0, 0, 0, 2)) imp ## # A tibble: 6 × 5 ## usuario StarWars1 StarWars2 StarWars3 Psycho ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 250 0 5 ## 2 2 0 200 250 1 ## 3 3 0 0 300 0 ## 4 4 150 200 0 0 ## 5 5 300 220 0 0 ## 6 6 250 180 0 2 Quiséramos encontrar una manera de considerar los 0’s como información más suave (es decir, alguien puede tener valores bajos de interacción con una película, pero esto no implica necesariamente que no sea preferida). Esto implica que es más importante ajustar los valores altos del indicador implícito de preferencia. Una solución propuesta en (Hu, Koren, and Volinsky 2008) (e implementada en spark) es darle menos importancia al valor \\(r_{ij}\\) en la construcción de los factores latentes, especialmente si tiene valores bajos. Para hacer esto, primero definimos la variable de preferencia \\[p_{ij} = \\begin{cases} 1, &amp;\\mbox{si } r_{ij}&gt;0,\\\\ 0, &amp;\\mbox{si } r_{ij}=0.\\\\ \\end{cases}\\] Esta variable \\(p_{ij}\\), cuando vale uno, indica algún nivel de confianza en la preferencia. ¿Pero qué tanto valor debemos darle a esta preferencia? Definimos la confianza como \\[c_{ij} = 1+ \\alpha r_{ui},\\] donde \\(\\alpha\\) es un parámetro que hay que afinar (por ejemplo \\(\\alpha\\) entre \\(1\\) y \\(50\\)). Para predicciones de vistas de TV, en (Hu, Koren, and Volinsky 2008) utilizan \\(\\alpha = 40\\), donde \\(r_{ij}\\) es el número de veces que el usuario ha visto un programa (contando vistas parciales, así que es un número real). La función objetivo (sin regularización) se define como \\[\\begin{equation} L = \\sum_{(i,j)} c_{ij}(p_{ij} - \\sum_{l=1}^k u_{i,l}v_{j,l})^2 \\tag{6.1} \\end{equation}\\] Nótese que : Cuando \\(c_ij\\) es alta (porque \\(r_{i,j}\\) es alta), para minimizar esta cantidad tenemos que hacer la predicción de $p_{ij}cercana a 1, pues el error se multiplica por \\(c_{ij}\\). Sin embargo, Cuando \\(r_{i,j}\\) es bajo, no es tan importante ajustar esta información con precisión: si \\(p_{ij} = 1\\), puede ser que \\(\\sum_{l=1}^k u_{i,l}v_{j,l}\\) sea muy bajo, y si \\(p_{ij}=0\\), puede ser que \\(\\sum_{l=1}^k u_{i,l}v_{j,l}\\) sea cercano a 1 sin afectar tanto el error. Esto permite que en el ajuste podamos descubrir artículos con \\(p_{ij}\\) alta para algún usuario, aún cuando \\(r_{ij}\\) es cero o muy chico. 6.3.2 Ejemplo Veamos cómo se ven soluciones de un factor imp_mat &lt;- imp |&gt; select(-usuario) |&gt; as.matrix() error_explicito &lt;- function(uv){ u &lt;- matrix(uv[1:6], ncol = 1) v &lt;- matrix(uv[7:10], ncol = 1) sum((imp_mat - u %*% t(v))^2) } error_implicito &lt;- function(uv){ u &lt;- matrix(uv[1:6], ncol = 1) v &lt;- matrix(uv[7:10], ncol = 1) pref_mat &lt;- as.numeric(imp_mat &gt; 0) - u %*% t(v) confianza &lt;- 1 + 0.1 * imp_mat sum((confianza * pref_mat)^2 ) } Si intentamos ajustar los ratings implícitos como si fueran explícitos, obtenemos los siguientes ajustados con un solo factor latente: uv_inicial &lt;- runif(10) opt_exp &lt;- optim(par = uv_inicial, error_explicito, method = &quot;BFGS&quot;) opt_exp$par[7:10] ## [1] 10.78862140 13.37128079 3.38943790 0.08702536 t(t(opt_exp$par[1:6])) %*% t(opt_exp$par[7:10]) |&gt; round() ## [,1] [,2] [,3] [,4] ## [1,] 118 146 37 1 ## [2,] 124 154 39 1 ## [3,] 36 44 11 0 ## [4,] 151 187 47 1 ## [5,] 217 269 68 2 ## [6,] 180 223 56 1 Nótese que esta solución no es muy buena: una componente intenta capturar los patrones de consumo de estas cuatro películas. Si usamos preferencias y confianza, obtenemos: opt_imp &lt;- optim(par = uv_inicial, error_implicito, method = &quot;BFGS&quot;) opt_imp$par[7:10] ## [1] 0.8123915 0.8136888 0.8102302 0.5046791 t(t(opt_imp$par[1:6])) %*% t(opt_imp$par[7:10]) |&gt; round(2) ## [,1] [,2] [,3] [,4] ## [1,] 1 1 0.99 0.62 ## [2,] 1 1 1.00 0.62 ## [3,] 1 1 1.00 0.62 ## [4,] 1 1 0.99 0.62 ## [5,] 1 1 1.00 0.62 ## [6,] 1 1 1.00 0.62 que indica que la información en esta matriz es consistente con que todos los usuarios tienen preferencia alta por las tres películas de Star Wars, y menos por la cuarta. Igual que en los ejemplos anteriores, usualmente se agregan términos de regularización para los vectores renglón \\(u_i\\) y \\(v_j\\). 6.3.3 Evaluación para modelos implícitos La evaluación para modelos implícitos no es tan simple como en el caso explícito, pues no estamos modelando directamente los valores observados \\(r_{ij}\\). Medidas como RECM o MAD que usamos en el caso explícito no son tan apropiadas para este problema. Una alternativa es, para cada usuario \\(i\\), ordenar los artículos de mayor a menor valor de \\(\\hat{p}_{ij} = u_iv_j^t\\) (canciones, pellículas), y calculamos: \\[ rank = \\frac{\\sum_{j} p_{ij}rank_{i,j}}{\\sum_j p_{ij}} \\] donde \\(rank_{ij}\\) es el percentil del artículo \\(j\\) en la lista ordenada de artículos. \\(rank_{ij}=0\\) para el mejor artículo, y \\(rank_{ij}=1\\) para el peor. Es decir, obtenemos valores más bajos si observamos que los usuarios interactúan con artículos que están más arriba en el ranking. Esta suma es un promedio sobre los rankings del usuario con \\(p_{ij}=1\\), y menores valores son mejores (quiere decir que hubo alguna preferencia por los items con \\(rank_{ij}\\) bajo, es decir, los mejores de nuestra lista predicha. Es posible también hacer un promedio ponderado por \\(r_{ij}\\): \\[ rank = \\frac{\\sum_{j} r_{ij}rank_{i,j}}{\\sum_j r_{ij}} \\] que es lo mismo que la ecuación anterior pero ponderando por el interés mostrado en cada artículo con \\(p_{ij}=1\\). Menores valores de \\(rank\\) son mejores. Si escogemos al azar el ordenamiento de los artículos, el valor esperado de \\(rank_{ij}\\) es \\(0.5\\) (en medio de la lista), lo que implica que el valor esperado de \\(rank\\) es \\(0.50\\). Cualquier modelo con \\(rank\\geq 0.5\\) es peor que dar recomendaciones al azar. Esta cantidad la podemos evaluar en entrenamiento y en validación. Para construir el conjunto de validación podemos hacer: Escogemos un número de usuarios para validación (por ejemplo \\(20\\%\\)) Ponemos \\(50\\%\\) de los artículos evaluados por estas personas en validación, por ejemplo. Estas cantidades dependen de cuántos datos tengamos, como siempre, para tener un tamaño razonable de datos de validación. Referencias "],["pagerank-y-análisis-de-redes.html", "7 Pagerank y análisis de redes 7.1 Introducción 7.2 Tipos de redes y su representación 7.3 Visualización de redes 7.4 Medidas de centralidad para redes 7.5 Gráficas dirigidas 7.6 Pagerank", " 7 Pagerank y análisis de redes 7.1 Introducción Pagerank asigna un número real a cada página de una red (web). Este número es un indicador de su importancia. Las ideas fundamentales son: Las páginas de internet forman una red o gráfica, donde los nodos son las páginas y las aristas dirigidas son las ligas de unas páginas a otras. La importancia de un página A depende de cuántas otras páginas apuntan a la página A. También depende de qué tan importantes sean las páginas que apuntan a A. Cuando hacemos una búsqueda, primero se filtran las páginas que tienen el contenido de nuestra búsqueda, y después los resultados se ordenan según el pagerank de estas páginas filtradas. ¿Qué problema resuelve? En un principio, se usaron métodos como índices y recuperación de documentos usando técnicas como tf-idf. El problema es que es muy fácil que un spammer sesgue los resultados para que sus páginas tengan alto nivel de relevancia en este sentido. Así que la importancia no se juzga sólo con el contenido, sino de los votos de otras páginas importantes. Este es un sistema más difícil de engañar. Es crucial usar la importancia de los in-links de una página; si no, sería tambíen fácil crear muchas páginas spam que apunten a otra dada para aumentar su importancia. El Pagerank, más en general, es una medida de centralidad o importancia de los nodos de una red dirigida. Comenzaremos considerando redes más variadas (por ejemplo, redes sociales) y el concepto general de centralidad. 7.1.1 Centralidad en redes Consideremos una red de personas, que representamos como una gráfica \\(G\\) no dirigida o dirigida, dependiendo del caso. Las personas son los nodos y sus relaciones se representan con aristas. Quiséramos construir una medida de importancia o centralidad de una persona dentro de la red. Por ejemplo: Redes sociales de internet: las ligas representan relación de amigos, o la de seguidor. Importancia: número de amigos o seguidores (grado de entrada o salida). Redes de citas bibliográficas: las ligas representan quién comparte o usa la información de quién. Importancia: número de citas o usos, ser citado por alguien importante, etc. Red de empleados de una oficina: las ligas representan interacciones en algún periodo. Importancia: quién puede conectar de manera más inmediata a dos personas. Ejemplo de Moviegalaxies.com: Pulp Fiction Dos personajes están ligados si tienen interacciones en la película. El tamaño y color de los nodos dependen de su “centralidad” en la red. Pulp fiction (Gráfica creada con Gephi). 7.2 Tipos de redes y su representación Una red es un conjunto de nodos conectados por algunas aristas. Las aristas pueden ser Dirigidas: hay un nodo origen y un nodo destino. No dirigidas: una arista representa una conexión simétrica entre dos nodos. Podemos representar redes de varias maneras. Una primera manera es con una lista de pares de vértices o nodos que están conectados por una arista. Por ejemplo, para una red dirigida: library(tidyverse) library(tidygraph) library(ggraph) aristas &lt;- tibble(from = c(1, 1, 1, 1, 2), to = c(2, 3, 4, 5, 3)) aristas ## # A tibble: 5 × 2 ## from to ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 ## 2 1 3 ## 3 1 4 ## 4 1 5 ## 5 2 3 red_tbl &lt;- tidygraph::as_tbl_graph(aristas, directed = TRUE) red_tbl ## # A tbl_graph: 5 nodes and 5 edges ## # ## # A directed acyclic simple graph with 1 component ## # ## # Node Data: 5 × 1 (active) ## name ## &lt;chr&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## # ## # Edge Data: 5 × 2 ## from to ## &lt;int&gt; &lt;int&gt; ## 1 1 2 ## 2 1 3 ## 3 1 4 ## # … with 2 more rows Que podemos visualizar como sigue: graficar_red_dirigida &lt;- function(red_tbl){ ggraph(red_tbl) + geom_edge_link(arrow = arrow(), end_cap = circle(4, &#39;mm&#39;)) + geom_node_point(size = 10, colour = &#39;salmon&#39;) + geom_node_text(aes(label = name)) + theme_graph() + coord_fixed() } graficar_red_dirigida(red_tbl) ## Using `sugiyama` as default layout También es posible representar una red mediante una matriz de adyacencia. La matriz de adyacencia para una red es la matriz \\(A\\) tal que \\[A_{ij} = 1\\] si existe una arista de \\(i\\) a \\(j\\). En el caso no dirigido, \\(A\\) es una matriz simétrica. matriz_ad &lt;- igraph::get.adjacency(red_tbl) matriz_ad ## 5 x 5 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 2 3 4 5 ## 1 . 1 1 1 1 ## 2 . . 1 . . ## 3 . . . . . ## 4 . . . . . ## 5 . . . . . Es más conveniente representar estas matrices como matrices ralas, como veremos más adelante. Nota de R: utilizamos el paquete tidygraph y ggraph para hacer manipulaciones de gráficas y graficación. Estos paquetes son extensiones del paquete igraph, que es el que contiene los algoritmos de visualización, procesamiento y resumen de redes. 7.3 Visualización de redes Existen varios algoritmos para visualizar redes que revelan distintos aspectos de su estructura (ver por ejemplo ?layout en R, en el paquete igraph). Por ejemplo, aquí construimos una red aleatoria, y hacemos un layout de nodos aleatorio: set.seed(1234) g &lt;- play_erdos_renyi(n = 20, p = 0.1, directed = FALSE) |&gt; as_tbl_graph() ggraph(g, layout = &#39;randomly&#39;) + geom_edge_link() + geom_node_point(size = 2, colour = &#39;salmon&#39;) + theme_graph() Y comparamos con la representación producida por un algoritmo basado en fuerzas. ggraph(g, layout = &#39;fr&#39;) + geom_edge_link() + geom_node_point(size = 2, colour = &#39;salmon&#39;) + theme_graph() Los algoritmos basados en fuerzas para representar redes en \\(2\\) o \\(3\\) dimensiones se basan principalmente en la siguiente idea: Las aristas actúan como resortes, que no permiten que nodos ligados se alejen mucho Los nodos tienen fuerzas de repulsión entre ellos (la analogía física es de cargas elécricas), y también a veces de gravedad entre ellos. El algoritmo de representación intenta minimizar la energía de la configuración del sistema de atracciones y repulsiones. Hay muchas variaciones de estos algoritmos, por ejemplo: graphopt en igraph, fruchtermann-rheingold, kamada-kawai, gem, escalamiento multidimensional, forceAtlas, etc. Intenta mover los nodos de las siguiente gráfica para entender el funcionamiento básico de estos algoritmos: library(visNetwork) edges &lt;- g |&gt; activate(edges) |&gt; as_tibble() set.seed(13) red_vis &lt;- visNetwork(nodes = tibble(id = 1:20, label = 1:20), edges, width = &quot;100%&quot;) |&gt; visPhysics(solver =&#39;forceAtlas2Based&#39;, forceAtlas2Based = list(gravitationalConstant = - 50, # negativo! centralGravity = 0.01, springLength = 100, springConstant = 1, avoidOverlap = 1 )) red_vis 7.3.1 Ejercicio Para la gráfica anterior, busca qué parámetros puedes cambiar en el algoritmo y experimenta cambiándolos (cuánta repulsión, rigidez de los resortes, número de iteraciones, etc.) Otras familias de algoritmos intentan distintas estrategias, como los layout de círculo, estrella, para árboles, etc. ggraph(g, layout = &#39;circle&#39;) + geom_edge_link() + geom_node_point(size = 2, colour = &#39;salmon&#39;) + theme_graph() 7.4 Medidas de centralidad para redes Como discutimos arriba, las medidas de centralidad en redes intentan capturar un concepto de importancia o conectividad de un nodo en una red. Primero comenzamos con el caso no dirigido. Medidas básicas de centralidad son Grado o grado de entrada/salida: cuántas ligas tiene un nodo (no dirigidos, de entrada o de salida). Betweeness: qué tan importante o único es un nodo para conectar otros pares de nodos de la red (por ejemplo, una persona con betweeness alto controla más fácilmente el flujo de información en una red social). Cercanía: qué tan lejos en promedio están los otros nodos de la red (pues puede encontrar y conectar más fácilmente otras dos nodos en la red). Centralidad de eigenvector/Pagerank: la centralidad de un nodo es una especie de promedio de la centralidad de sus vecinos. 7.4.1 Grado Sea \\(G\\) una gráfica no dirigida, y sea \\(A\\) la matriz de adyacencia de \\(G\\). Si \\(i\\) es un nodo (vértice) dado, entonces su grado es \\[c_G(i)=\\sum_{j\\neq i} A_{i,j}.\\] que cuenta cúantas aristas conectan con el nodo \\(i\\). graficar_red_nd &lt;- function(dat_g, layout = &#39;kk&#39;){ ggraph(dat_g, layout = layout) + geom_edge_link(alpha=0.2) + geom_node_point(aes(size = importancia), colour = &#39;salmon&#39;) + geom_node_text(aes(label = nombre), nudge_y = 0.2, size=3) + theme_graph(base_family = &#39;sans&#39;) } g_grado &lt;- g |&gt; activate(nodes) |&gt; mutate(importancia = centrality_degree()) |&gt; mutate(nombre = 1:20) graficar_red_nd(g_grado) ¿Qué no captura el grado como medida de centralidad? El grado es una medida local que no toma en cuenta la topología más global de la red: cómo están conectados nodos más lejanos alrededor del nodo que nos interesa. Distancia a otros nodos En primer lugar, por ejemplo, no captura que algunos nodos están más cercanos en promedio a los nodos de la red que otros. g_simple &lt;- igraph::graph(c(1, 2, 2, 3, 3, 4, 4, 5), directed = FALSE) |&gt; as_tbl_graph() |&gt; mutate(importancia = centrality_degree()) |&gt; mutate(nombre = LETTERS[1:5]) graficar_red_nd(g_simple) Obsérvese en este ejemplo que el nodo \\(C\\) es más importante que \\(D\\), en el sentido de que está más cercano a los nodos de toda la red, aún cuando el grado es el mismo para ambos. Caminos que pasan por un nodo En la siguiente gráfica, el nodo \\(G\\) es importante porque es la única conexión entre dos partes de la red, y esto no lo captura el grado: triangulo_1 &lt;- c(1,2,2,3,3,1) triangulo_2 &lt;- triangulo_1 + 3 red_3 &lt;- igraph::graph(c(triangulo_1, triangulo_2, c(7,1,7,4)), directed = FALSE) |&gt; as_tbl_graph() |&gt; mutate(importancia = centrality_degree()) |&gt; mutate(nombre = LETTERS[1:7]) graficar_red_nd(red_3) Nodos conectados a otros nodos importantes En la siguiente gráfica el nodo \\(H\\) tienen el mismo grado que \\(F\\), pero \\(H\\) está conectado a un nodo más importante (\\(A\\)) red_4 &lt;- igraph::graph(c(2,1,3,1,4,1,5,1,2,3,6,2,1,7,1,8), directed=FALSE) |&gt; as_tbl_graph() |&gt; mutate(importancia = centrality_degree()) |&gt; mutate(nombre = LETTERS[1:8]) graficar_red_nd(red_4) 7.4.2 Medida de centralidad: Betweeness o Intermediación La medida de centralidad de intermediación de un nodo \\(u\\) se define como: \\[c_b (u) = \\sum_{j&lt;k, u\\neq j,u\\neq i} \\frac{ g(j,k |u)}{ g(j,k)},\\] donde \\(g(j,k)\\) es el número de caminos más cortos distintos entre \\(j\\) y \\(k\\) y \\(g(j,k |u)\\) es el número de caminos más cortos distintos entre \\(j\\) y \\(k\\) que pasan por \\(u\\). \\(g(j,k | u ) = 0\\) cuando \\(j=u\\) o \\(k=u\\). Los caminos que más aportan a la intermediación de un nodo \\(u\\) son aquellos que conectan nodos que no tienen otra alternativa más que pasar por \\(u\\). Esta medida se puede normalizar poniendo (\\(n\\) es el total de nodos de la red) \\[\\overline{c}_b (i)=c_b (i)/\\binom{n-1}{2},\\] pues el denominador es el máximo valor de intermediación que puede alcanzar un vértice en una red de \\(n\\) nodos (demuéstralo). Ejemplo red_4 &lt;- igraph::graph(c(2,1,3,1,4,1,5,1,2,3,6,2,2,5,1,6,6,7), directed = FALSE) |&gt; as_tbl_graph() |&gt; mutate(importancia = centrality_betweenness()) |&gt; mutate(nombre = LETTERS[1:7]) graficar_red_nd(red_4) + labs(subtitle = &#39;Intermediación&#39;) Por ejemplo, consideremos el nodo \\(B\\). Hay dos caminos más cortos de \\(C\\) a \\(F\\) (de tamaño \\(2\\)), y uno de ellos pasa por \\(B\\). De modo que los caminos de \\(C\\) a \\(F\\) aportan \\(0.5\\) al betweeness de \\(B\\). De los caminos más cortos entre \\(E\\) y \\(D\\), ninguno pasa por \\(B\\), así que este par de vértices aporta \\(0\\) al betweeness. Verifica el valor de betweeness para \\(B\\) haciendo los cálculos restantes: red_4 |&gt; as_tibble() ## # A tibble: 7 × 2 ## importancia nombre ## &lt;dbl&gt; &lt;chr&gt; ## 1 7.5 A ## 2 2.5 B ## 3 0 C ## 4 0 D ## 5 0 E ## 6 5 F ## 7 0 G Ejemplo de grado e intermediación: Pulp Fiction En esta red, el color es una medición de betweeness y el tamaño del nodo una medición del grado. Aunque Butch y Jules tienen grados similares, Butch tiene intermediación más alto pues provee más ligas únicas más cortas entre los personajes, mientras que la mayoría de los de Jules pasan también por Vincent. Pulp fiction 7.4.3 Medida de centralidad: Cercanía También es posible definir medidas de importancia según el promedio de cercanía a todos los nodos. Éste se calcula como el inverso del promedio de distancias del nodo a todos los demás. Ejemplo red_5 &lt;- igraph::graph(c(2,1,3,1,4,9,5,2,2,3,6,1,7,8, 8,9,9,1,1,8,1,7), directed = FALSE) red_5 &lt;- red_5 |&gt; as_tbl_graph() |&gt; mutate(importancia = centrality_closeness(normalized = TRUE)) |&gt; mutate(nombre = LETTERS[1:9]) red_5 |&gt; activate(nodes) |&gt; as_tibble() ## # A tibble: 9 × 2 ## importancia nombre ## &lt;dbl&gt; &lt;chr&gt; ## 1 0.8 A ## 2 0.571 B ## 3 0.533 C ## 4 0.381 D ## 5 0.381 E ## 6 0.471 F ## 7 0.5 G ## 8 0.571 H ## 9 0.571 I graficar_red_nd(red_5) + labs(subtitle = &#39;Cercanía&#39;) En este ejemplo, el nodo \\(F\\) tiene cercanía más alta que \\(D\\), por ejemplo, pues se conecta a un nodo bien conectado de la red (en grado y betweeness): Ejercicio Verifica que la cercanía de \\(A\\) es \\(0.80\\). 7.4.4 Centralidad de eigenvector Esta medida considera que la importancia de un nodo está dado por la suma normalizada de las importancias de sus vecinos. De esta forma, es importante estar cercano a nodos importantes (como en cercanía), pero también cuenta conectarse a muchos nodos (como en grado). Nótese que esta es una descripción circular: para saber la importancia de un nodo, hay que saber la importancia de sus vecinos. Consideremos el ejemplo siguiente: red_6 &lt;- igraph::graph(c(1,2,1,3,1,4,5,2), directed = FALSE) |&gt; as_tbl_graph() |&gt; mutate(nombre = 1:5, importancia = 0) graficar_red_nd(red_6) + theme(legend.position=&quot;none&quot;)+ labs(subtitle = &#39;Eigenvector&#39;) Supongamos que las importancias de estos \\(5\\) nodos son \\[(x_1,\\ldots, x_5)\\] donde \\(x_i\\geq 0\\). Suponemos también que estas importancias están normalizadas, de forma que \\(\\sum_i x_i = 1\\). De acuerdo a la idea mencionada arriba, calculamos entonces cómo se ve la suma de las importancias de nodos adyacentes a cada nodo. Para el nodo uno, \\[{y_1} = x_2 + x_3 + x_4\\] para el nodo \\(2\\) \\[y_2 = x_1 + x_5\\] y para los siguientes nodos tendríamos \\[y_3 = x_1\\] \\[y_4 = x_1\\] \\[y_5 = x_2\\]. Este sistema lo podemos escribir de forma matricial, usando la matriz de adyacencia, como \\[ y = \\left ( \\begin{array}{rrrrr} 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right ) x \\] Por definición de las importancias, tenemos que normalizar este vector \\(y\\) para obtener importancias originales. Es decir, existe una \\(\\lambda\\) tal que \\[\\frac{1}{\\lambda}y = x\\] Donde \\(\\lambda\\geq 0\\) es el factor de normalización. En resumen, \\(x\\geq 0\\) debe satisfacer, para alguna \\(\\lambda &gt; 0\\), la ecuación \\[A^t x = \\lambda x,\\] es decir, \\(x\\) es un vector propio de la matriz de adyacencia con valor propio positivo. Sin embargo, ¿cuando existe un vector \\(x\\geq 0\\) con \\(\\lambda&gt;0\\) que satisfaga esta propiedad? ¿es único? Ejercicio Resuelve el sistema de ecuaciones de arriba y verifica que tal vector existe. ¿Cuál es el valor de lambda? Matrices no negativas Para entender la existencia y forma de la centralidad de eigenvector, comenzamos recordando algunos teoremas básicos de álgebra lineal. En primer lugar, tenemos: Espectro de matrices no-negativas Si \\(A\\) es una matriz no negativa, entonces: Existe un valor propio real no-negativo \\(\\lambda_0\\) tal que \\(\\lambda_0\\geq |\\lambda|\\) para cualquier otro valor propio \\(\\lambda\\) de \\(A\\). Al valor propio \\(\\lambda_0\\) está asociado al menos un vector propio \\(x\\) con entradas no negativas. Nota: Parte de este teorema se puede entender observando que si \\(A\\) es no-negativa, entonces mapea el cono \\(\\{(x_1,x_2,\\ldots, x_m) | x_i \\geq 0\\}\\) dentro de sí mismo, lo que implica que debe dejar invariante alguna dirección dentro de este cono. Si este vector propio no-negativo fuera único (hasta normalización) y distinto del vector \\(0\\), entonces esto nos daría un conjunto de medidas (únicas hasta normalización) \\(x\\) para la importancia de los nodos: Ejemplo 1 par(mar=c(0,0,0,0)); plot(red_6, vertex.size = 40) A_red &lt;- igraph::get.adjacency(red_6) A_red ## 5 x 5 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] . 1 1 1 . ## [2,] 1 . . . 1 ## [3,] 1 . . . . ## [4,] 1 . . . . ## [5,] . 1 . . . desc_A &lt;- eigen(A_red) desc_A |&gt; map( ~ round(.x, 3)) ## $values ## [1] 1.848 0.765 0.000 -0.765 -1.848 ## ## $vectors ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.653 -0.271 0.000 0.271 0.653 ## [2,] 0.500 0.500 0.000 0.500 -0.500 ## [3,] 0.354 -0.354 0.707 -0.354 -0.354 ## [4,] 0.354 -0.354 -0.707 -0.354 -0.354 ## [5,] 0.271 0.653 0.000 -0.653 0.271 Las medida de centralidad de eigenvector da entonces, en este caso: x &lt;- desc_A$vectors[,1] print(x, digits = 2) ## [1] 0.65 0.50 0.35 0.35 0.27 par(mar=c(0,0,0,0)); plot(red_6, vertex.size = 100*x) Ejemplo 2 Sin embargo, puede ser que obtengamos más de un valor propio no negativo con vectores asociados no negativos, por ejemplo: red &lt;- igraph::graph(c(1,2,2,3,3,1,2,4,5,6), directed = FALSE) par(mar=c(0,0,0,0)); plot(red, vertex.size=20) A_red &lt;- igraph::get.adjacency(red) A_red ## 6 x 6 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] . 1 1 . . . ## [2,] 1 . 1 1 . . ## [3,] 1 1 . . . . ## [4,] . 1 . . . . ## [5,] . . . . . 1 ## [6,] . . . . 1 . Nótese que los eigenvectores \\(1\\) y \\(2\\) son no negativos, y están asociados a vectores propios no negativos: desc_A &lt;- eigen(A_red) desc_A|&gt; map( ~ round(.x, 3)) ## $values ## [1] 2.170 1.000 0.311 -1.000 -1.000 -1.481 ## ## $vectors ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -0.523 0.000 -0.368 0.000 -0.707 0.302 ## [2,] -0.612 0.000 0.254 0.000 0.000 -0.749 ## [3,] -0.523 0.000 -0.368 0.000 0.707 0.302 ## [4,] -0.282 0.000 0.815 0.000 0.000 0.506 ## [5,] 0.000 0.707 0.000 0.707 0.000 0.000 ## [6,] 0.000 0.707 0.000 -0.707 0.000 0.000 Nota: si encontramos un vector propio \\(x\\) con entradas negativas o cero, podemos convertirlo a un vector propio con entradas no negativas tomando \\(-x\\). Los vectores propios debemos considerarlos módulo una constante multiplicativa, y los vectores particulares que se encuentran dependen del algoritmo. En este caso, la medida de centralización dependería de qué peso le ponemos al primer vector propio vs el segundo vector propio. En este ejemplo, la unicidad no sucede pues la red asociada no es conexa. Matrices irreducibles y gráficas fuertemente conexas ¿Cuándo podemos garantizar unicidad en la solución de \\(Ax=\\lambda x\\) con \\(\\lambda &gt;0\\) y que \\(x\\) sea un vector no-negativo distinto de \\(0\\)? Sea \\(A\\) la matriz de adyacencia de una gráfica no dirigida. Si la gráfica asociada a \\(A\\) es fuertemente conexa (existen caminos entre cualquier par de vértices) entonces decimos que \\(A\\) es irreducible. Podemos dar también una definición de irreducibilidad sólo en términos de \\(A\\): \\(A\\) es irreducible cuando para toda \\(i,j\\) existe \\(m\\geq 0\\) tal que \\((A^m)_{i,j} &gt; 0\\). Nota: discute por qué estas dos definiciones son equivalentes. Utilizaremos uno de los teoremas más importantes del álgebra lineal: Teorema de Perron-Frobenius Si \\(A\\) es una matriz no-negativa irreducible, entonces Existe un valor propio real positivo simple \\(\\lambda_0\\) tal que \\(\\lambda_0 &gt; |\\lambda|\\) para cualquier otro valor propio \\(\\lambda\\) de \\(A\\), asociado a un vector propio \\(x\\) con entradas positivas. No existe ningún otro vector propio con entradas no negativas que no sea paralelo a \\(x\\). Y entonces podemos definir una medida única de centralidad módulo una constante multiplicativa. Si \\(A\\) es la matriz de adyacencia de una red no dirigida, y \\(A\\) es irreducible (significa que la red es fuertemente conexa), definimos la centralidad de eigenvector de un nodo \\(i\\) como la \\(i\\)-esima componente del vector positivo \\(x\\) (con \\(\\sum x_i = 1\\)) asociado al valor propio (único) de Perron-Frobenius. Ejemplo: facultad de tres universidades install.packages(&#39;igraphdata&#39;) ## Installing package into &#39;/usr/local/lib/R/site-library&#39; ## (as &#39;lib&#39; is unspecified) library(&quot;igraphdata&quot;) data(&quot;UKfaculty&quot;) ukf.und &lt;- igraph::as.undirected(UKfaculty) head(dat_1 &lt;- igraph::get.data.frame((ukf.und))) ## from to weight ## 1 1 4 4 ## 2 3 4 1 ## 3 5 6 1 ## 4 5 7 2 ## 5 6 7 28 ## 6 3 9 1 grupo &lt;- igraph::get.vertex.attribute(UKfaculty, &#39;Group&#39;) nodos &lt;- data.frame(id = 1:length(grupo)) visNetwork(nodos, dat_1, width = &quot;100%&quot;) |&gt; visPhysics(solver =&#39;forceAtlas2Based&#39;, forceAtlas2Based = list(gravitationalConstant = -10), stabilization = TRUE) Ahora calculamos centralidad de eigenvector. A &lt;- igraph::get.adjacency(ukf.und) desc_A &lt;- eigen(as.matrix(A)) desc_A$values |&gt; map_dbl( ~ round(.x, 3)) ## [1] 19.284 13.549 8.527 6.019 5.204 4.560 4.314 3.629 3.521 3.050 ## [11] 2.966 2.682 2.395 2.316 2.169 2.164 1.869 1.793 1.672 1.457 ## [21] 1.314 1.263 1.119 0.912 0.833 0.767 0.677 0.629 0.609 0.537 ## [31] 0.406 0.353 0.229 0.109 0.070 -0.108 -0.214 -0.297 -0.323 -0.533 ## [41] -0.609 -0.677 -0.752 -0.892 -0.913 -0.987 -1.087 -1.178 -1.188 -1.247 ## [51] -1.349 -1.406 -1.495 -1.553 -1.660 -1.691 -1.803 -1.956 -2.024 -2.179 ## [61] -2.210 -2.268 -2.371 -2.435 -2.611 -2.669 -2.828 -2.893 -3.016 -3.176 ## [71] -3.262 -3.336 -3.625 -3.805 -4.082 -4.351 -4.481 -4.888 -5.214 -5.341 ## [81] -5.982 vec &lt;- as.numeric(desc_A$vector[,1]) desc_A$values[1] ## [1] 19.28427 e_vector &lt;- -vec qplot(e_vector,xlab=&quot;Primer vector propio&quot;,main=&quot;Importancia por centralidad de eigenvector&quot;)+theme(plot.title = element_text(hjust=0.5)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. colores &lt;- colorRampPalette(c(&#39;red&#39;,&#39;green&#39;)) colores_1 &lt;- colores(length(e_vector)) nodos &lt;- data.frame(id=1:length(vec), value = e_vector, color = colores_1[rank(e_vector)]) visNetwork(nodos, dat_1 |&gt; select(-weight) |&gt; mutate(colour = &quot;#999999&quot;), width = &quot;100%&quot;) |&gt; visPhysics(solver =&#39;forceAtlas2Based&#39;, stabilization = TRUE) |&gt; visNodes(value = 1, scaling = list(min = 1, max = 200)) Podemos calcular también usando gggraph: uk_tbl &lt;- ukf.und |&gt; as_tbl_graph() |&gt; activate(nodes) |&gt; mutate(nombre = row_number()) |&gt; mutate(importancia = centrality_eigen()) ggraph(uk_tbl, layout = &#39;fr&#39;) + geom_edge_link(alpha=0.2) + geom_node_point(aes(size = importancia^2, colour = factor(Group))) + theme_graph(base_family = &#39;sans&#39;) 7.5 Gráficas dirigidas Estos conceptos pueden aplicarse también para gráficas dirigidas, cuando hay un concepto de dirección en las relaciones de los nodos. In degree, out degree. Betweenness puede definirse en función de caminos dirigidos. Cercanía también (in closeness, out closeness). Centralidad de eigenvector: misma idea, pero la matriz \\(A\\) no es simétrica. En este caso, \\(A_{ij} = 1\\) cuando hay una arista que va de \\(i\\) a \\(j\\). Consideremos en particular cómo se calcula la centralidad de eigenvector para una red dirigida. set.seed(28011) red_6 &lt;- igraph::erdos.renyi.game(5, p.or.m=0.5, directed=T) par(mar=c(0,0,0,0)) plot(red_6, vertex.size=40) Su matriz de adyacencia es no simétrica A &lt;- igraph::get.adjacency(red_6) A ## 5 x 5 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] . . 1 1 1 ## [2,] 1 . 1 . 1 ## [3,] . 1 . . 1 ## [4,] 1 1 . . . ## [5,] 1 1 . 1 . Para este ejemplo, las ecuaciones de importancia son como sigue. Para el nodo \\(1\\), \\[\\lambda x_1 = x_2 + x_4 + x_5\\] para el nodo 5, por ejemplo, es \\[\\lambda x_5 = x_1 + x_2 + x_3\\] y así sucesivamente. Obsérvese que en cada ecuación se consideran las aristas entrantes, de forma que la ecuación del nodo \\(1\\) requiere la columna \\(1\\) de la matriz de adyacencia, el nodo \\(2\\) la columna \\(2\\), etc. Es decir, la ecuación que debemos resolver es \\[A^t x = \\lambda x\\] En el ejemplo anterior, desc_A &lt;- eigen(t(as.matrix(A))) v &lt;- desc_A$vectors[,1] print(as.numeric(v / sum(v)), digits = 2) ## [1] 0.23 0.21 0.16 0.17 0.23 ¿Cuándo podemos garantizar unicidad en la solución de \\(A^tx=\\lambda x\\) con \\(\\lambda &gt; 0\\)? Sea \\(A\\) la matriz de adyacencia de una gráfica dirigida. Igual que en el caso de gráficas no dirigidas: Si la gráfica asociada a \\(A\\) es fuertemente conexa (para cualquier par de vértices hay caminos \\(i\\to j\\) y \\(j\\to i\\)) entonces decimos que \\(A\\) es irreducible. Igualmente, \\(A\\) es irreducible si y sólo si para cualquier \\(i,j\\) existe una \\(m&gt;0\\) tal que \\((A^m)_{i,j}&gt;0\\). Y podemos igualmente aplicar Perron-Frobenius, de donde concluimos: Si \\(A\\) es irreducible, entonces \\(\\lambda_0 &gt; 0\\) es un eigenvector simple de \\(A\\), un vector propio asociado \\(x\\) tiene entradas positivas, y no existe ningún otro vector propio con entradas no negativas que no sea paralelo a \\(x\\). Y entonces podemos definir una medida única de centralidad módulo una constante multiplicativa. Ejemplos: ¿qué pasa si \\(A\\) es no reducible? Hay distintas maneras en que la matriz no es reducible, y cada una de ellas contradice algún aspecto del teorema de Perron Frobenius. Por ejemplo, si la gráfica es conexa pero no fuertemente conexa, podemos tener vectores propios no positivos (algunos nodos resultan con peso \\(0\\)): red &lt;- igraph::graph(c(1,2,2,3,3,4,4,3), directed = TRUE) par(mar=c(0,0,0,0)); plot(red, vertex.size = 40) A_red &lt;- as.matrix(igraph::get.adjacency(red)) A_red ## [,1] [,2] [,3] [,4] ## [1,] 0 1 0 0 ## [2,] 0 0 1 0 ## [3,] 0 0 0 1 ## [4,] 0 0 1 0 eigen(t(A_red)) |&gt; map( ~ round(.x, 2)) ## $values ## [1] 1 -1 0 0 ## ## $vectors ## [,1] [,2] [,3] [,4] ## [1,] 0.00 0.00 0.00 0.00 ## [2,] 0.00 0.00 0.71 -0.71 ## [3,] 0.71 0.71 0.00 0.00 ## [4,] 0.71 -0.71 -0.71 0.71 Si la gráfica es disconexa, podemos tener valores propios no simples con distintos vectores propios no negativos: red &lt;- igraph::graph(c(1,2,2,3,3,2,4,5,5,4), directed = TRUE) par(mar=c(0,0,0,0)); plot(red, vertex.size = 20) A_red &lt;- as.matrix(igraph::get.adjacency(red)) A_red ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 1 0 0 0 ## [2,] 0 0 1 0 0 ## [3,] 0 1 0 0 0 ## [4,] 0 0 0 0 1 ## [5,] 0 0 0 1 0 eigen(t(A_red))|&gt; map( ~ round(.x, 2)) ## $values ## [1] 1 -1 1 -1 0 ## ## $vectors ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.00 0.00 0.00 0.00 0.71 ## [2,] 0.00 0.00 0.71 -0.71 0.00 ## [3,] 0.00 0.00 0.71 0.71 -0.71 ## [4,] 0.71 -0.71 0.00 0.00 0.00 ## [5,] 0.71 0.71 0.00 0.00 0.00 O ningún valor propio positivo: red &lt;- igraph::graph(c(1, 2, 2, 3), directed = TRUE) par(mar=c(0,0,0,0)); plot(red, vertex.size = 20) A_red &lt;- as.matrix(igraph::get.adjacency(red)) A_red ## [,1] [,2] [,3] ## [1,] 0 1 0 ## [2,] 0 0 1 ## [3,] 0 0 0 eigen(t(A_red)) |&gt; map( ~ round(.x, 2)) ## $values ## [1] 0 0 0 ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0 0 0 ## [2,] 0 0 0 ## [3,] 1 -1 1 7.6 Pagerank Pagerank es una medida similar a la centralidad de eigenvector: La importancia de una página (nodo) es un promedio ponderado de las importancias de otras páginas que apuntan (ligan) hacia ella. Diferencia: usamos gráficas dirigidas, y distribuimos el peso de las aristas dependiendo de cuántas ligas hacia afuera tiene una página: es decir, la importancia de una página se diluye entre el número de ligas a sus hijos. Esto tiene sentido pues si tenemos una página importante que apunta a pocas páginas, debe dar más importancia a estas pocas páginas que si apuntara a muchas páginas. Para una red dirigida de páginas de internet, definimos entonces su matriz de transición \\(M\\) como sigue: \\(M_{ij} =1/k\\) si la página \\(i\\) tiene \\(k\\) ligas hacia afuera, y una de ellas va a la página \\(j\\) \\(M_{ij} = 0\\) en otro caso. Intentaremos hacer algo similar a la centralidad de eigenvector, pero usando la matriz de transición en lugar de la matriz de adyacencia. Ejemplo red_p &lt;- igraph::graph(c(1,2,1,4,1,3,2,1,2,4,3,1,4,3,2,3)) par(mar=c(0,0,0,0)) plot(red_p, vertex.size=40, edge.curved=T, edge.arrow.size=0.5) La matriz de adyacencia no necesariamente es simétrica, pues la gráfica es dirigida: A &lt;- igraph::get.adjacency(red_p) A ## 4 x 4 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] . 1 1 1 ## [2,] 1 . 1 1 ## [3,] 1 . . . ## [4,] . . 1 . Y calculamos la matriz M, que es la matriz \\(A\\) con los renglones normalizados por su suma: M &lt;- A / Matrix::rowSums(A) M ## 4 x 4 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] . 0.3333333 0.3333333 0.3333333 ## [2,] 0.3333333 . 0.3333333 0.3333333 ## [3,] 1.0000000 . . . ## [4,] . . 1.0000000 . 7.6.1 La matriz \\(M\\) es estocástica A la matriz \\(M\\) le llamamos una matriz estocástica, pues cada uno de sus renglones son no negativos y suman \\(1\\). Bajo este supuesto, es posible demostrar otra versión del teorema de Perron Frobenius: Principio del Pagerank Supongamos que \\(M\\) es irreducible, que en el caso dirigido quiere decir que la red es fuertemente conexa: existe un camino dirigido entre cualquier par de vértices. Entonces el valor propio de Perron-Frobenius (simple) para \\(M^t\\) es \\(\\lambda_0=1\\), y el único vector propio no negativo (módulo longitud) es estrictamente positivo y asociado a \\(\\lambda_0=1\\). Esto implica que si \\(r\\) es el vector de importancias según pagerank, entonces \\(r\\) debe satisfacer \\[r_j = \\sum_{i\\to j} \\frac{r_i}{d_i}\\] donde la suma es sobre las ligas de entrada a \\(j\\), y \\(d_i\\) es el grado de salida del nodo \\(i\\) (dividimos la importancia de \\(r_i\\) sobre todas sus aristas de salida). En forma matricial, esto se escribe como: \\[M^tr = r.\\] Ejercicio Escribe las ecuaciones de las dos formas mostradas arriba para la red red_p ## IGRAPH 6afeba1 D--- 4 8 -- ## + edges from 6afeba1: ## [1] 1-&gt;2 1-&gt;4 1-&gt;3 2-&gt;1 2-&gt;4 3-&gt;1 4-&gt;3 2-&gt;3 Ejemplo: pagerank simple decomp &lt;- eigen(t(as.matrix(M))) decomp ## eigen() decomposition ## $values ## [1] 1.0000000+0.0000000i -0.3333333+0.4714045i -0.3333333-0.4714045i ## [4] -0.3333333+0.0000000i ## ## $vectors ## [,1] [,2] [,3] [,4] ## [1,] 0.6902685+0i 0.7071068+0.0000000i 0.7071068+0.0000000i 7.071068e-01+0i ## [2,] 0.2300895+0i -0.2357023-0.3333333i -0.2357023+0.3333333i -7.071068e-01+0i ## [3,] 0.6135720+0i -0.1571348+0.4444444i -0.1571348-0.4444444i -2.919335e-16+0i ## [4,] 0.3067860+0i -0.3142697-0.1111111i -0.3142697+0.1111111i -3.184729e-16+0i vec_1 &lt;- abs(as.numeric(decomp$vector[,1])) vec_1 ## [1] 0.6902685 0.2300895 0.6135720 0.3067860 round(vec_1/sum(vec_1), 2) ## [1] 0.38 0.12 0.33 0.17 par(mar=c(0,0,0,0)) plot(red_p, vertex.size=100*vec_1, edge.curved=T, edge.arrow.size=0.5) ¿Por qué \\(1\\) tiene mayor importancia que \\(3\\)? Nótese que el nodo \\(3\\) tiene mayor grado que \\(1\\). La razón de que el pagerank de \\(1\\) es mayor que el de \\(3\\) es que la importancia del nodo \\(1\\) se diluye pues tiene grado \\(3\\) de salida, mientras que toda la importancia de \\(3\\) se comunica al nodo \\(1\\). 7.6.2 Primeras dificultades ¿Qué puede fallar cuando queremos encontrar el pagerank de un gráfica que representa sitios de internet, por ejemplo? Si existen callejones sin salida la matriz \\(M\\) no es estocástica, pues tiene un renglón de ceros - no podemos aplicar la teoría de arriba. Por ejemplo, la siguiente matriz no tiene un valor propio igual a \\(1\\) (conclusión invalidada: hay un valor propio igual a \\(1\\)): red_p &lt;- igraph::graph(c(1,2,2,1,1,3)) par(mar=c(0,0,0,0)); plot(red_p, vertex.size=30, edge.curved=T,edge.arrow.size=0.5) A &lt;- igraph::get.adjacency(red_p) M &lt;- t(scale(t(as.matrix(A)), center=FALSE, scale=apply(A,1,sum))) # ponemos cero porque ni siquiera se puede normalizar: M[3, ] &lt;- 0 eigen(t(M)) ## eigen() decomposition ## $values ## [1] 0.7071068 -0.7071068 0.0000000 ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.7071068 0.7071068 0 ## [2,] 0.5000000 -0.5000000 0 ## [3,] 0.5000000 -0.5000000 1 Si existen trampas de telaraña (spider traps) entonces la matriz \\(M\\) es estocástica, pero las soluciones concentran toda la importancia en la trampa (en este caso consiste de los nodos \\(1\\) y \\(2\\)) (conclusión invalidada: el vector de importancias asociado al eigenvalor \\(1\\) es positivo). red_p &lt;- igraph::graph(c(1,2,2,1,3,1,3,4,4,5,5,3)) par(mar=c(0,0,0,0)); plot(red_p, vertex.size=30, edge.curved=T,edge.arrow.size=0.5) A &lt;- igraph::get.adjacency(red_p) M &lt;- t(scale(t(as.matrix(A)), center = FALSE, scale = apply(A,1,sum))) eigen(t(M)) ## eigen() decomposition ## $values ## [1] 1.0000000+0.0000000i -1.0000000+0.0000000i 0.7937005+0.0000000i ## [4] -0.3968503+0.6873648i -0.3968503-0.6873648i ## ## $vectors ## [,1] [,2] [,3] [,4] ## [1,] 0.7071068+0i -0.7071068+0i 0.4794851+0i 0.0242710-0.1851728i ## [2,] 0.7071068+0i 0.7071068+0i 0.6041134+0i -0.2173362+0.0901689i ## [3,] 0.0000000+0i 0.0000000+0i -0.4470916+0i 0.6699710+0.0000000i ## [4,] 0.0000000+0i 0.0000000+0i -0.2816501+0i -0.2110276-0.3655106i ## [5,] 0.0000000+0i 0.0000000+0i -0.3548568+0i -0.2658782+0.4605145i ## [,5] ## [1,] 0.0242710+0.1851728i ## [2,] -0.2173362-0.0901689i ## [3,] 0.6699710+0.0000000i ## [4,] -0.2110276+0.3655106i ## [5,] -0.2658782-0.4605145i La red puede ser disconexa. Por ejemplo, cuando hay dos componentes irreducibles, existe más de un vector propio asociado al valor propio \\(1\\) (conclusión invalidada: la solución es única y el valor propio \\(1\\) es simple), así que hay tantas soluciones como combinaciones lineales de los eigenvectores que aparecen: red_p &lt;- igraph::graph(c(1,2,2,3,3,1,4,5,5,6,6,5,6,4)) par(mar=c(0,0,0,0)); plot(red_p, vertex.size=30, edge.curved=T,edge.arrow.size=0.5) A &lt;- igraph::get.adjacency(red_p) |&gt; as.matrix() M &lt;- t(scale(t(A), center=FALSE, scale=apply(A,1,sum))) M ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0 1 0 0.0 0.0 0 ## [2,] 0 0 1 0.0 0.0 0 ## [3,] 1 0 0 0.0 0.0 0 ## [4,] 0 0 0 0.0 1.0 0 ## [5,] 0 0 0 0.0 0.0 1 ## [6,] 0 0 0 0.5 0.5 0 ## attr(,&quot;scaled:scale&quot;) ## [1] 1 1 1 1 1 2 sol &lt;- eigen(t(M)) sol ## eigen() decomposition ## $values ## [1] 1.0+0.0000000i -0.5+0.8660254i -0.5-0.8660254i 1.0+0.0000000i ## [5] -0.5+0.5000000i -0.5-0.5000000i ## ## $vectors ## [,1] [,2] [,3] [,4] ## [1,] 0.0000000+0i 0.5773503+0.0i 0.5773503+0.0i -0.5773503+0i ## [2,] 0.0000000+0i -0.2886751-0.5i -0.2886751+0.5i -0.5773503+0i ## [3,] 0.0000000+0i -0.2886751+0.5i -0.2886751-0.5i -0.5773503+0i ## [4,] 0.3333333+0i 0.0000000+0.0i 0.0000000+0.0i 0.0000000+0i ## [5,] 0.6666667+0i 0.0000000+0.0i 0.0000000+0.0i 0.0000000+0i ## [6,] 0.6666667+0i 0.0000000+0.0i 0.0000000+0.0i 0.0000000+0i ## [,5] [,6] ## [1,] 0.0000000+0.0000000i 0.0000000+0.0000000i ## [2,] 0.0000000+0.0000000i 0.0000000+0.0000000i ## [3,] 0.0000000+0.0000000i 0.0000000+0.0000000i ## [4,] 0.3535534+0.3535534i 0.3535534-0.3535534i ## [5,] 0.3535534-0.3535534i 0.3535534+0.3535534i ## [6,] -0.7071068+0.0000000i -0.7071068+0.0000000i round(sol$values,2) ## [1] 1.0+0.00i -0.5+0.87i -0.5-0.87i 1.0+0.00i -0.5+0.50i -0.5-0.50i vec &lt;- sol$vectors[,abs(sol$values-1) &lt; 1e-8] Re(vec) ## [,1] [,2] ## [1,] 0.0000000 -0.5773503 ## [2,] 0.0000000 -0.5773503 ## [3,] 0.0000000 -0.5773503 ## [4,] 0.3333333 0.0000000 ## [5,] 0.6666667 0.0000000 ## [6,] 0.6666667 0.0000000 En términos de nuestra solución para dar importancia de páginas: No es razonable que nuestra solución concentre toda la importancia en spider traps. Si la gráfica es disconexa no podemos dar importancia relativa a las componentes resultantes. Si hay callejones sin salida entonces nuestra formulación no sirve. Para encontrar una solución, podemos pensar en el proceso estocástico asociado a esta formulación de pagerank. 7.6.3 El proceso estocástico (cadena de Markov) asociado al Pagerank, versión simple Podemos interpretar este proceso mediante una cadena de Markov. Consideramos una persona que navega al azar en nuestra red (haciendo click en las ligas disponibles en cada nodo): Comienza en una página tomada al azar (equiprobable). Cada vez que llega a una página, escoge al azar alguna de las páginas ligadas en su página actual y navega hacia ella. Suponemos por el momento que no hay callejones sin salida (estos evitan que pueda saltar a otro lado). La pregunta que queremos contestar: ¿cuál es la probabilidad que en distintos momentos el navegador aleatorio se encuentre en una página dada? Claramente páginas que tienen muchas ligas de entrada (son importantes) tendrán más visitas, y más aún si estas ligas de entrada provienen de nodos con muchas ligas de entrada (es decir, a su vez son importantes). Sin embargo, es necesario hacer algunos refinamientos si queremos contestar de manera simple esta pregunta. Denotamos por \\(X_1, X_2,\\ldots\\) la posición del navegador en cada momento del tiempo. Cada \\(X_i\\) es una variable aleatoria que toma valores en los nodos \\(\\{1,2,\\ldots,n\\}\\). \\(X_1, X_2,\\ldots\\) es un proceso estocástico discreto en tiempo discreto. Para determinar un proceso estocástico, debemos dar la distribución conjunta de cualquier subconjunto \\(X_{s_1},X_{s_2},\\ldots, X_{s_k}\\) de variables. En este caso, la posición en \\(s+1\\) sólo depende de la posición en el momento \\(s\\), de forma que basta con especificar \\[P(X_{s+1}=j\\vert X_s=i) = P_{ij}, \\] para cada par de páginas \\(i\\) y \\(j\\). 7.6.4 Matriz de transición Ahora podemos ver que Si hay una liga de \\(i\\) a \\(j\\), entonces \\(P_{ij}=1/k(i)\\), donde \\(k(i)\\) es el número de ligas salientes de \\(i\\). Si no hay liga de \\(i\\) a \\(j\\), entonces \\(P_{ij}=0\\). Es claro que \\(P\\) es igual a la matriz \\(M\\) que definimos con anterioridad. 7.6.5 Distribución de equilibrio (versión simple) La matriz \\(P\\) es estocástica. Si suponemos que \\(P\\) es irreducible (la gráfica es fuertemente conexa), entonces por la teoría que vimos arriba existe un vector \\(\\pi &gt; 0\\) tal que \\(P^t\\pi = \\pi\\). Ahora podemos interpretar este vector en términos del navegador aleatorio: En términos del modelo del navegador aleatorio, ¿ qué significa entonces que un vector \\(\\pi\\) satisfaga \\(P^t \\pi = \\pi\\), con \\(\\pi \\geq 0\\)? Suponemos \\(\\pi\\) normalizado por la suma: \\(\\sum_i \\pi_i=1\\). Significa que si escogemos un estado al azar con probabilidad \\(\\pi\\), entonces, después de un salto, las probabilidades de encontrar al navegador en cada estado está dado también por \\(\\pi\\). Igualmente, la probabilidad de encontrar al navegador en cualquier momento en el estado \\(i\\) es igual a \\(\\pi_i\\). Por esta razón, a \\(\\pi\\) se le llama una distribución de equilibro para el proceso del navegador aleatorio. 7.6.6 Distribución de equilibrio y probabilidades a largo plazo Sin embargo, en un principio no conocemos la distribución de equilibrio \\(\\pi\\). Lo que haríamos sería escoger un nodo al azar y comenzar a navegar desde ahí, o quizá empezaríamos en un nodo fijo, por ejemplo, el \\(1\\), y nuestra pregunta sigue siendo ¿cuál es la probabilidad que en distintos momentos el navegador aleatorio se encuentre en una página dada? En primer lugar, necesitamos el supuesto de conexidad fuerte (irreducibilidad), pues en otro caso los lugares donde puede estar el navegador aleatorio dependen de dónde empezo, y la respuesta es más complicada. Veremos que además, necesitamos un supuesto de aperiodicidad, pues ciclos periódicos en la red puede forzar a que sólo en ciertos momentos sean permitidos ciertos estados, otra vez, dependiendo de dónde empezó el navegador aleatorio. Requerimos entonces: Conexidad fuerte (irreducibilidad) Aperiodicidad Veremos ahora por qué la periodicidad puede ser un problema. Ejemplo: periodicidad Consideremos este ejemplo fuertemente conexo (irreducible), con un ciclo de tamaño \\(3\\): red_p &lt;- igraph::graph(c(1,2,2,3,3,1)) par(mar=c(0,0,0,0)); plot(red_p, vertex.size=30, edge.curved=T,edge.arrow.size=0.5) A &lt;- igraph::get.adjacency(red_p) M &lt;- t(scale(t(as.matrix(A)), center=FALSE, scale=apply(A,1,sum))) M ## [,1] [,2] [,3] ## [1,] 0 1 0 ## [2,] 0 0 1 ## [3,] 1 0 0 ## attr(,&quot;scaled:scale&quot;) ## [1] 1 1 1 Y notamos que si comenzamos en el estado uno, la probabilidad de estar en cada estado al tiempo \\(2\\) es t(M) %*% c(1,0,0) ## [,1] ## [1,] 0 ## [2,] 1 ## [3,] 0 En el tiempo \\(3\\): t(M) %*% t(M) %*% c(1,0,0) ## [,1] ## [1,] 0 ## [2,] 0 ## [3,] 1 En el tiempo \\(4\\): t(M) %*% t(M) %*% t(M) %*% c(1,0,0) ## [,1] ## [1,] 1 ## [2,] 0 ## [3,] 0 y así sucesivamente, de manera que las probabilidades de visita a cada nodo dependen siempre fuertemente del tiempo. La solución a nuestra pregunta de dónde estará el navegador aleatorio al tiempo \\(t\\) no es tan simple de formular. En este ejemplo, vemos que el problema son estructuras periódicas en la red del navegador aleatorio. Veremos ahora cómo lidiar con este problema. 7.6.7 Matriz de transición a \\(k\\) pasos Recordamos que la matriz \\((P)^k\\) da las probabilidades de transición a \\(k\\) pasos. Por ejemplo, para \\(k=2\\) tenemos la probabiildad de pasar de \\(i\\) a \\(j\\) en dos pasos es igual a (probabilidad total): \\[P(X_3=j|X_1=i) =\\sum_k P(X_3=j|X_2=k, X_1=i) P(X_2=k|X_1=i)\\] Por la propiedad de Markov, podemos simplificar \\[P(X_3=j|X_1=i) =\\sum_k P(X_3=j|X_2=k) P(X_2=k|X_1=i)\\] y si sustituimos \\(P\\) \\[P(X_3=j|X_1=i) =\\sum_k P_{i,k} P_{k,j}. \\] Como el lado derecho es la componente \\(i,j\\) de \\(P^2\\), tenemos que \\(P^2\\) da las probabilidades de transición en \\(2\\) pasos. Como ejercicio, calcula e interpreta \\(P^2\\) para el ejemplo anterior. Cadenas aperiódicas Decimos que \\(P\\) irreducible es aperiódica cuando \\(P^{k_0}&gt;0\\) para alguna \\(k_0\\) suficientemente grande. Nótese que en el ejemplo anterior \\(P\\) no es aperiódica. Con esta condición (no importa en qué estado estamos al tiempo \\(k_0\\) el navegador puede estar en cualquier estado), podemos dar una respuesta más simple a la pregunta acerca de las probabilidades de largo plazo de la cadena: Distribución a largo plazo Si \\(P\\) es una matriz irreducible y aperiódica, y \\(\\pi\\) es su distribución de equilibrio, entonces \\[\\lim_{n\\to \\infty} P(X_n=i) = \\pi_i,\\] independientemente de la distribución o nodo inicial. De esta forma tenemos que: Sea \\(v\\) una distribución inicial sobre los estados (cualquiera). Entonces, si \\(P\\) es irreducible y aperiódica, \\[(P^n)^t v \\to \\pi\\] cuando \\(n\\to \\infty\\). Lo cual nos da el algoritmo básico para encontrar la importancia de Pagerank dada por la distribución de equilibrio: Método de potencias para pagerank simple Tomamos \\(v\\) una distribución arbitraria (por ejemplo equidistribuida sobre los estados). Ponemos \\(v_1=v\\) e iteramos \\[v_{n+1} = M^t v_{n}\\] hasta que \\(||v_{n+1} -v_{n}||&lt;\\epsilon.\\) Veremos que este algoritmo simple es escalable a problemas muy grandes, pues sólo involucra multiplicar por una matriz que típicamente es rala. 7.6.8 Pagerank: teletransportación/perturbación de la matriz \\(M\\) Para que funcione este algoritmo, como vimos antes, tenemos que lidiar con los problemas que vimos arriba (spider traps, callejones sin salidas, falta de conexidad, periodicidad). La solución es relativamente simple: fijamos una \\(\\alpha\\) cercana a uno. A cada tiempo: Con probabilidad \\(\\alpha\\), el navegador escoge alguno de las ligas de salida y brinca (como en el proceso original), Sin embargo, con probabilidad \\(1-\\alpha\\), el navegador se teletransporta a un nodo escogido al azar (equiprobable), de todos los nodos posibles. Si el navegador está en un callejón sin salida, siempre se teletransporta como en el inciso anterior. Esta alteración del proceso elimina spider traps, callejones sin salidas, disconexiones y periodicidad. Ahora construimos la matriz \\(M_1\\) de este proceso. Sea \\(M\\) la matriz original, que suponemos por el momento no tiene callejones sin salida. ¿Cómo se ven las nuevas probabildades de transición? Si \\(i,j\\) son nodos, \\(n\\) es el número total de nodos, tenemos simplemente (si \\(i\\) no es spider-trap): \\[(M_1)_{i,j} = \\alpha M_{ij} + (1-\\alpha)\\frac{1}{n}\\] Esto es por probabilidad total: la probabilidad de ir de \\(i\\) a \\(j\\) dada teletransportación es \\(1/n\\), y la probabilidad de ir de \\(i\\) a \\(j\\) dado que no hubo teletransportación es \\(M_{ij}\\) (que puede ser cero). La probabilidad total se calcula promediando estas dos probabilidades según la probabilidad de teletransportación o no. Si \\(i\\) es un spider-trap, entonces ponemos simplemente \\[(M_1)_{i,j} = 1/n\\] Idea: La matriz \\(M_1\\) estocástica es positiva, de modo que automáticamente es irreducible y aperiódica. Tomando \\(\\alpha=0.85\\) obtenemos por ejemplo para nuestra matriz \\(M\\) anterior: red.p &lt;- igraph::graph(c(1,2,2,3,3,1,2,3,3,2,4,5,5,4)) par(mar=c(0,0,0,0)); plot(red.p, vertex.size=30, edge.curved=T,edge.arrow.size=0.5) A &lt;- igraph::get.adjacency(red.p) M &lt;- t(scale(t(as.matrix(A)), center=FALSE, scale=apply(A,1,sum))) unos &lt;- rep(1,nrow(M)) alpha &lt;- 0.85 M.1 &lt;- alpha*M + (1-alpha)*unos%*%t(unos)/nrow(M) M.1 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.030 0.880 0.03 0.03 0.03 ## [2,] 0.030 0.030 0.88 0.03 0.03 ## [3,] 0.455 0.455 0.03 0.03 0.03 ## [4,] 0.030 0.030 0.03 0.03 0.88 ## [5,] 0.030 0.030 0.03 0.88 0.03 ## attr(,&quot;scaled:scale&quot;) ## [1] 1 2 2 1 1 sol &lt;- eigen(t(M.1)) sol$values ## [1] 1.000+0.000i -0.850+0.000i 0.850+0.000i -0.425+0.425i -0.425-0.425i as.numeric(sol$vectors[,1]) ## [1] 0.2828726 0.5233143 0.5106595 0.4389488 0.4389488 Ejercicio Verifica que la matriz \\(M_1\\) construida arriba es en realidad una matriz estocástica. No es buena idea preprocesar desde el principio la matriz \\(M\\) para evitar estos casos (la matriz \\(M\\) es típicamente rala, pues hay relativamente pocas ligas en cada página comparado con el total de nodos, y no queremos llenar los renglones igual a cero con una cantidad fija). En lugar de eso, podemos hacer los siguiente: Supongamos entonces queremos calcular \\(x&#39; = M_1 x\\), tomando en cuenta callejones sin salida. Podemos hacer, para \\(x\\) distribución inicial: Calcular \\(y = \\alpha M^t x\\) (operación con matriz rala). Nótese que los renglones de \\(M\\) que son iguales a cero deben ser sustituidos por \\(1/N\\) (callejones sin salida). Esto implica que necesitamos sumar la misma cantidad a todas las entradas de \\(y\\): \\[\\frac{1}{N}\\sum_{j\\,es\\, callejon} x_j\\]. Por otra parte, para los que no son callejones, tenemos que sumar la cantidad fija \\[(1-\\alpha)\\frac{1}{N}\\sum_{j\\, no\\, es\\, callejon} x_j\\] En cualquier caso, a todas las entradas se les suma una cantidad fija \\[\\frac{1}{N}\\sum_{j\\,es\\, callejon} x_j + (1-\\alpha)\\frac{1}{N}\\sum_{j\\, no\\, es\\, callejon} x_j,\\] y las componentes del vector resultante deben sumar uno. La estrategia de cálculo es entonces: una vez que tenemos \\(y\\), simplemente ajustamos \\(x= y + (1-S)/N\\), donde \\(S=\\sum_i y_i\\), es decir: sumamos una cantidad fija para asegurar que suman uno las componentes de \\(x\\). 7.6.9 Pagerank para buscador Usamos índice invertido (términos a documentos) para escoger las páginas que son relevantes al query. Regresamos los resultados ordenados por el PageRank. Consideramos las páginas de un solo sitio (universidad de Hollings): library(Matrix) library(dplyr) tab_1 &lt;- read.table(&#39;../datos/data_hollings/hollins.txt&#39;) write.csv(tab_1, file=&#39;../datos/data_hollings/ejemplo_red_hollings.csv&#39;, row.names=FALSE) head(tab_1) ## V1 V2 ## 1 1 2 ## 2 8 2 ## 3 16 2 ## 4 18 2 ## 5 20 2 ## 6 23 2 i &lt;- tab_1$V1 j &lt;- tab_1$V2 x &lt;- 1 Normalizamos y usamos estructura rala: mat &lt;- data.frame(i=i, j=j, x=x) mat_norm &lt;- mat |&gt; group_by(i) |&gt; mutate(p = x / sum(x)) M &lt;-sparseMatrix(i = mat_norm$i, j = mat_norm$j, x = mat_norm$p, dims = c(6012,6012)) dim(M) ## [1] 6012 6012 sum(M&gt;0)/6012^2 ## [1] 0.0006605496 Nótese que hacer los cálculos con la matriz rala es más eficiente: library(microbenchmark) r &lt;- rep(1, 6012)/6012 M_t &lt;- t(M) microbenchmark(r_1 &lt;- M_t %*% r, times=10, unit = &#39;ms&#39;) ## Unit: milliseconds ## expr min lq mean median uq max neval ## r_1 &lt;- M_t %*% r 0.1439 0.1454 0.42317 0.1473 0.1515 2.8684 10 MM &lt;- as.matrix(M_t) microbenchmark(r_1 &lt;- MM %*% r, times=10, unit = &#39;ms&#39;) ## Unit: milliseconds ## expr min lq mean median uq max neval ## r_1 &lt;- MM %*% r 37.0067 37.0467 37.15051 37.1548 37.2025 37.3428 10 Así que podemos hacer nuestro algoritmo: r &lt;- rep(1, 6012)/6012 for(i in 1:10){ q &lt;- 0.85 * (M_t %*% r) suma_q &lt;- sum(q) r_nuevo &lt;- q + (1 - suma_q) / 6012 print(sum(abs(r_nuevo - r))) r &lt;- r_nuevo } ## [1] 0.4907346 ## [1] 0.2554216 ## [1] 0.1399171 ## [1] 0.08249344 ## [1] 0.05276466 ## [1] 0.03433384 ## [1] 0.02356974 ## [1] 0.01614466 ## [1] 0.01175786 ## [1] 0.008488335 nombres.pag &lt;- read.table(&#39;../datos/data_hollings/nombres_paginas.txt&#39;) nombres.pag$pagerank &lt;- as.numeric(r) arrange(nombres.pag, desc(pagerank)) |&gt; head(10) ## V1 V2 ## 1 2 http://www.hollins.edu/ ## 2 37 http://www.hollins.edu/admissions/visit/visit.htm ## 3 38 http://www.hollins.edu/about/about_tour.htm ## 4 61 http://www.hollins.edu/htdig/index.html ## 5 52 http://www.hollins.edu/admissions/info-request/info-request.cfm ## 6 43 http://www.hollins.edu/admissions/apply/apply.htm ## 7 425 http://www.hollins.edu/academics/library/resources/web_linx.htm ## 8 27 http://www.hollins.edu/admissions/admissions.htm ## 9 28 http://www.hollins.edu/academics/academics.htm ## 10 29 http://www.hollins.edu/grad/coedgrad.htm ## pagerank ## 1 0.020342191 ## 2 0.009487376 ## 3 0.008793044 ## 4 0.008237781 ## 5 0.008202176 ## 6 0.007310231 ## 7 0.006709038 ## 8 0.006121904 ## 9 0.005703552 ## 10 0.004470490 #write.csv(nombres.pag[,c(&#39;Label&#39;),drop=FALSE], file=&#39;pagerank_hollings.csv&#39;, row.names=FALSE, col.names=FALSE) ordenada &lt;- arrange(nombres.pag, desc(pagerank)) Hacemos una búsqueda (simple, basada solamente en el nombre de la URL: esto normalmente se haría con un índice invertido sobre el contenido): query &lt;- &#39;admission&#39; ordenada |&gt; filter(str_detect(V2, &#39;admission&#39;)) |&gt; head() ## V1 V2 ## 1 37 http://www.hollins.edu/admissions/visit/visit.htm ## 2 52 http://www.hollins.edu/admissions/info-request/info-request.cfm ## 3 43 http://www.hollins.edu/admissions/apply/apply.htm ## 4 27 http://www.hollins.edu/admissions/admissions.htm ## 5 81 http://www.hollins.edu/admissions/financial/finaid.htm ## 6 80 http://www.hollins.edu/admissions/ugradadm/ugradadm.htm ## pagerank ## 1 0.009487376 ## 2 0.008202176 ## 3 0.007310231 ## 4 0.006121904 ## 5 0.003147287 ## 6 0.002187616 ordenada |&gt; filter(str_detect(V2, &#39;admission&#39;)) |&gt; tail() ## V1 ## 58 1290 ## 59 1442 ## 60 1028 ## 61 1854 ## 62 1590 ## 63 1591 ## V2 ## 58 http://www.hollins.edu/admissions/ugradadm/horizon/apply/apply1.cfm ## 59 http://www.hollins.edu/admissions/apply/login2.cfm ## 60 http://www.hollins.edu/admissions/scholarship/work.htm ## 61 http://www.hollins.edu/admissions/ugradadm/horizon/apply/app_review2.cfm ## 62 http://www.hollins.edu/admissions/ugradadm/horizon/apply/app_review.cfm ## 63 http://www.hollins.edu/admissions/ugradadm/horizon/apply/apply1a.cfm ## pagerank ## 58 6.680171e-05 ## 59 6.569320e-05 ## 60 6.452762e-05 ## 61 6.236121e-05 ## 62 6.193540e-05 ## 63 6.193540e-05 query &lt;- &#39;hollins&#39; ordenada |&gt; filter(str_detect(V2, query)) |&gt; head() ## V1 V2 ## 1 2 http://www.hollins.edu/ ## 2 37 http://www.hollins.edu/admissions/visit/visit.htm ## 3 38 http://www.hollins.edu/about/about_tour.htm ## 4 61 http://www.hollins.edu/htdig/index.html ## 5 52 http://www.hollins.edu/admissions/info-request/info-request.cfm ## 6 43 http://www.hollins.edu/admissions/apply/apply.htm ## pagerank ## 1 0.020342191 ## 2 0.009487376 ## 3 0.008793044 ## 4 0.008237781 ## 5 0.008202176 ## 6 0.007310231 query &lt;- &#39;student&#39; ordenada |&gt; filter(str_detect(V2, query)) |&gt; head(10) ## V1 ## 1 82 ## 2 26 ## 3 5955 ## 4 6005 ## 5 18 ## 6 6004 ## 7 5877 ## 8 5956 ## 9 467 ## 10 468 ## V2 ## 1 http://www.hollins.edu/admissions/ugradadm/students/students.htm ## 2 http://www.hollins.edu/students/index.htm ## 3 http://www1.hollins.edu/Docs/CampusLife/StudentAct/studentorgs/Habitat/moreInfo.htm ## 4 http://www1.hollins.edu/Docs/CampusLife/StudentAct/studentorgs/Habitat/whatWeDo.htm ## 5 http://www1.hollins.edu/Docs/CampusLife/StudentAct/studentorgs/SGA/default.html ## 6 http://www1.hollins.edu/Docs/CampusLife/StudentAct/studentorgs/Habitat/homeBuilding.htm ## 7 http://www1.hollins.edu/Docs/CampusLife/StudentAct/studentorgs/hhrc/index.htm ## 8 http://www1.hollins.edu/Docs/CampusLife/StudentAct/studentorgs/hhrc/contact.htm ## 9 http://www1.hollins.edu/Docs/CampusLife/StudentAct/studentorgs/SGA/forms/SGA%20Short%20Term%20Scholarship%20Application%20for%202004.doc ## 10 http://www1.hollins.edu/Docs/CampusLife/StudentAct/studentorgs/SGA/forms/SGA%20Short%20Term%20Scholarship%20Application%20for%202004.pdf ## pagerank ## 1 0.0012740467 ## 2 0.0007685934 ## 3 0.0004847083 ## 4 0.0004338779 ## 5 0.0004295892 ## 6 0.0003905306 ## 7 0.0003813615 ## 8 0.0003796379 ## 9 0.0002657994 ## 10 0.0002657994 "],["detección-de-comunidades.html", "8 Detección de comunidades 8.1 Modularidad 8.2 Algoritmo miope (fast greedy)", " 8 Detección de comunidades En esta parte consideramos algunos métodos de detección de comunidades, como una introducción a este tema amplio e importante. Las redes sociales y otras comunmente tienen subgrupos de vértices o nodos tales que hay una gran cantidad de conexiones entre ellos y menos conexiones fuera de ese grupo, o comunidades que es posible identificar. Existen distintas definiciones y algoritmos útiles para hacer análisis de comunidades: algunas particionan los nodos en clusters (como algoritmos de clustering), algunos otros permiten identificar comunidades traslapadas. Por ejemplo: en nuestra red personal de facebook o algo similar existen comunidades como familia, amigos de la secundaria o preparatoria, compañeros de trabajo, etc, y quizá algunos conocidos que no pertenecen a ninguna de estas comunidades. Naturalmente pertenecemos a todos estos grupos. Por otro lado, quizá si analizamos la red de retweets de un tema particular, podemos encontrar comunidades separadas (muchos retweets entre ellos, pocos retweets fuera del grupo) y cada persona se puede clasificar como de un “equipo.” Ejemplo: red de club de karate De la descripción en igraphdata de estos datos: Social network between members of a university karate club, led by president John A. and karate instructor Mr. Hi (pseudonyms). The edge weights are the number of common activities the club members took part of. Zachary studied conflict and fission in this network, as the karate club was split into two separate clubs, after long disputes between two factions of the club, one led by John A., the other by Mr. Hi. Hacemos cálculos y graficamos: library(tidyverse) library(tidygraph) library(ggraph) library(igraphdata) data(karate) karate_red &lt;- karate |&gt; as_tbl_graph() |&gt; activate(nodes) |&gt; mutate(grado = centrality_eigen()) |&gt; mutate(grupo = group_fast_greedy(weights = weight)) |&gt; mutate(grupo = as.factor(grupo)) |&gt; mutate(name = ifelse(str_detect(name, &quot;Actor&quot;), str_sub(name, 7, 10), name)) ggraph(karate_red, layout = &#39;fr&#39;) + geom_edge_link(aes(alpha = weight)) + geom_node_point(aes(size = grado, colour = grupo)) + geom_node_text(aes(label = name), repel = TRUE) + theme_graph(base_family = &quot;sans&quot;) Este método identifica dos comunidades grandes, la de Mr Hi, la de John A, y otra más chica que está conectada solamente a la de Mr Hi. Nótese que las ligas entre elementos de la comunidad son densas dentro de la comunidad y menos densas hacia afuera, aunque hay unos individuos que están en las fronteras (como 3 y 9, por ejemplo). ¿Cómo encontrar estos módulos? 8.1 Modularidad El concepto de modularidad es uno básico para intentar cuantificar qué tan buena o “cohesiva” es una separación de nodos en grupos. La modularidad mide que tan fuertemente puede dividirse una red en módulos (grupos, clusters o comunidades disjuntas). Una división en módulos es fuerte cuando los nodos de cada módulo están bien conectados entre ellos, y menos conectados a nodos fuera de su módulo. Para definir esta cantidad, comenzamos con la matriz de adyacencia \\(A\\) de una gráfica no dirigida, y una agrupación de vértices en grupos. Para empezar, notamos que el número total de aristas en la gráfica es \\[m = \\frac{1}{2}\\sum_{u,v} A_{u,v},\\] donde dividimos entre dos para no contar doble las aristas no dirigidas. Ahora supongamos que \\(g(v)\\) es un grupo al que pertenece el vértice \\(v\\). Calculamos el número de aristas de aristas conectan elementos del mismo grupo: \\[\\frac{1}{2}\\sum_{u,v} A_{u, v} I(g(u), g(v))\\] nótese que un elemento de esta suma es igual a 0 o 1, y solo es igual a 1 cuando \\(u\\) y \\(v\\) están conectados y \\(u\\) y \\(v\\) pertenecen al mismo grupo (\\(I(g,h)\\) es la función indicadora de identidad, es decir vale 1 si \\(g=h\\) y 0 en otro caso). Entonces la fracción de aristas del total de aristas que conecta vértices dentro del mismo grupo es \\[\\frac{\\sum_{u,v} A_{u, v} I(g(u), g(v))}{\\sum_{u,v} A_{u, v}} = \\frac{1}{2m}\\sum_{u,v} A_{u, v} I(g(u), g(v))\\] donde \\(m\\) es el número de aristas. Esta cantidad va a ser grande para particiones “cohesivas,” que agrupan vértices en comunidades densas, y chica en otro caso. En general, esta formulación también aplica a multigrafos, en cuyo caso puede \\(A_{u,v}\\) cuenta el número de aristas entre los nodos \\(u\\) y \\(v\\), y no solo vale 1 o 0. Ejemplo #funcion para graficar graficar_red_nd &lt;- function(dat_g, layout = &quot;nicely&quot;, grupo, nombres = TRUE){ gg &lt;- ggraph(dat_g |&gt; activate(nodes), layout = layout) + geom_edge_link(alpha=0.2) + geom_node_point(aes(colour = {{ grupo }}), size = 5) + theme_graph(base_family = &quot;sans&quot;) if(nombres){ gg &lt;- gg + geom_node_text(aes(label = nombre), size=5, repel = TRUE) } gg } aristas &lt;- tibble(from = c(10, 10, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 11), to = c(7, 8, 9, 1, 2, 3, 1, 5, 6, 4, 9, 7, 8, 5, 7)) grupos &lt;- c(rep(&#39;a&#39;, 3), rep(&#39;b&#39;, 3), rep(&#39;c&#39;, 5)) ejemplo_mod_alta &lt;- tbl_graph( nodes = tibble(nombre = seq(1, 11, 1), grupo = grupos), edges = aristas, directed = FALSE) set.seed(89) graficar_red_nd(ejemplo_mod_alta, layout = &quot;gem&quot;, grupo = grupo) Cuya matriz de adyacencia \\(A\\) es A &lt;- igraph::get.adjacency(ejemplo_mod_alta) A ## 11 x 11 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] . 1 1 . . . . . . 1 . ## [2,] 1 . 1 . . . . . . . . ## [3,] 1 1 . . . . . . . . . ## [4,] . . . . 1 1 . . . . . ## [5,] . . . 1 . 1 . . . . 1 ## [6,] . . . 1 1 . . . . . . ## [7,] . . . . . . . 1 1 1 1 ## [8,] . . . . . . 1 . 1 1 . ## [9,] . . . . . . 1 1 . 1 . ## [10,] 1 . . . . . 1 1 1 . . ## [11,] . . . . 1 . 1 . . . . Nótense los bloques que se puede formar en la matriz de adyacencia (que es notable debido a cómo etiquetamos los nodos). Estas son las estructuras propias de una partición con modularidad alta. Podemos calcular la fracción de aristas que conectan vértices del mismo grupo como sigue: g_a &lt;- A[grupos == &quot;a&quot;, grupos == &quot;a&quot;] g_a ## 3 x 3 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] . 1 1 ## [2,] 1 . 1 ## [3,] 1 1 . g_b &lt;- A[grupos == &quot;b&quot;, grupos == &quot;b&quot;] g_c &lt;- A[grupos == &quot;c&quot;, grupos == &quot;c&quot;] frac_1 &lt;- (sum(g_a) + sum(g_b) + sum(g_c)) / sum(A) sprintf(&quot;Fracción de aristas dentro de mismo grupo: %0.3f&quot;, frac_1) ## [1] &quot;Fracción de aristas dentro de mismo grupo: 0.867&quot; Ahora vemos un ejemplo de modularidad baja: set.seed(12138) ejemplo_mod_baja &lt;- play_erdos_renyi(11, p = 0.4, directed = FALSE) |&gt; activate(nodes) |&gt; mutate(grupo = grupos, nombre = 1:11) graficar_red_nd(ejemplo_mod_baja, layout = &quot;gem&quot;, grupo = grupo) A &lt;- igraph::get.adjacency(ejemplo_mod_baja) g_a &lt;- A[grupos == &quot;a&quot;, grupos == &quot;a&quot;] g_b &lt;- A[grupos == &quot;b&quot;, grupos == &quot;b&quot;] g_c &lt;- A[grupos == &quot;c&quot;, grupos == &quot;c&quot;] g_c ## 5 x 5 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] . 1 . . . ## [2,] 1 . 1 . 1 ## [3,] . 1 . . . ## [4,] . . . . 1 ## [5,] . 1 . 1 . frac_2 &lt;- (sum(g_a) + sum(g_b) + sum(g_c)) / sum(A) sprintf(&quot;Fracción de aristas dentro de mismo grupo: %0.3f&quot;, frac_2) ## [1] &quot;Fracción de aristas dentro de mismo grupo: 0.304&quot; Discusión: esta medida (fracción de aristas que conectan nodos en el mismo grupo) es una que podemos optimizar para buscar comunidades. Sin embargo, la modularidad no se define así, sino que consideramos una normalización adicional para hacerla más comparable de red a red. Nótese que: Redes con nodos con grados más altos naturalmente tienden a tener calificaciones de modularidad más altas que redes con menos aristas. Una manera de normalizar es entonces considerar la modularidad comparada con lo que esperaríamos si, dejando fijo número de aristas y grados de vértices, las aristas se conectaran al azar. El proceso aleatorio es: Cortamos “a la mitad” todos las aristas de nuestra gráfica. Ahora conectamos mitades al azar. Calculamos la fracción de aristas que conectan nodos del mismo grupo. Y calculamos el valor esperado de esta cantidad, que restamos de la fracción que obtuvimos para la red original. El resultado es la modularidad de nuestra red: la diferencia de fracción de nodos que conectan al mismo grupo menos lo que pasaría si conectáramos al azar las aristas. Ahora podemos calcular directamente este valor esperado. Consideremos entonces dos aristas \\(u\\) y \\(v\\), que tienen grado \\(k(u)\\) y \\(k(v)\\) respectivamente, y sea \\(m\\) el número total de aristas. Entonces: Una media arista dada de \\(u\\) tiene probabilidad \\(k(v)/(2m - 1)\\) de quedar conectada a \\(v\\). El número de pruebas independientes es \\(k(u)\\), para cada media arista de \\(u\\). El valor esperado del número de conexiones entre \\(u\\) y \\(v\\) es entonces \\(k(u)k(v)/(2m -1)\\). La idea es entonces promediar los valores del número de conexiones observadas menos las esperadas según el modelo aleatorio: \\[A_{u,v} - \\frac{k(u)k(v)}{2m-1}\\] para obtener: La modularidad de una gráfica no dirigida y vértices con una agrupación dada \\(g\\) se define como \\[Q = \\frac{1}{2m}{\\sum_{u,v} \\left ( A_{u, v} - \\frac{k(u)k(v)}{2m} \\right )I(g(u), g(v))},\\] donde \\(A\\) es la matriz de adyacencia y \\(k(u)\\) es el grado de \\(u\\). Observaciones: Típicamente se usa la división entre \\(2m\\) en lugar de \\(2m-1\\). Para gráficas no muy chicas esto no es muy importante. Si esta cantidad es cercana a cero, entonces la fracción de nodos intra-grupo es similar a la de una gráfica construida al azar. Esta cantidad puede ir de -0.5 a 1. Normalmente se consideran valores mayores a 0.3 como casos de modularidad fuerte, o de existencia de comunidades (ver Leskovec, Rajaraman, and Ullman (2014)). La modularidad puede entenderse entonces como sigue: es la fracción de aristas que conectan nodos del mismo grupo menos lo que esperaríamos si las aristas se distribuyeran al azar en la gráfica (respetando el grado de cada vértice). Ejemplo Para nuestro primer ejemplo la modularidad es igraph::modularity(ejemplo_mod_alta, as.factor(grupos)) ## [1] 0.4733333 y para nuestro segundo ejemplo igraph::modularity(ejemplo_mod_baja, as.factor(grupos)) ## [1] -0.06805293 Observación: la idea ahora es construir algoritmos para encontrar agrupaciones de modularidad máxima en una gráfica dada. Estas agrupaciones nos dan las comunidades si resultan tener modularidad alta. 8.2 Algoritmo miope (fast greedy) Hay varios algoritmos para encontrar agrupaciones de modularidad alta en una gráfica dada. Consideramos el algoritmo fast greedy (Clauset, Newman, and Moore (2004), liga de arxiv), que está diseñado para redes posiblemente muy grandes. Este algoritmo es similar a clustering jerárquico: Comenzamos con todos los vértices un su propia comunidad. Buscamos el par de comunidades (al principio vértices) que al unirse da el mayor incremento (o menor decremento) en \\(Q\\). Repetimos 2 hasta que todos los puntos están en una sola comunidad. Escogemos el número de comunidades que maximiza el valor del \\(Q\\) sobre todas las iteraciones. Existen varias particularidades de este algoritmo que lo hacen rápido y apropiado para redes grandes (ver referencia. Otro método similar es el algoritmo de louvain, por ejemplo). Ejemplo library(igraphdata) data(USairports) airports &lt;- as_tbl_graph(USairports) # seleccionamos solo pasajeros aristas &lt;- airports |&gt; activate(edges) |&gt; select(to, from, Passengers) |&gt; as_tibble() # agregar aristas_agregados &lt;- aristas |&gt; filter(to != from) |&gt; mutate(to_u = ifelse(to &lt; from, to, from)) |&gt; mutate(from_u = ifelse(to &lt; from, from, to)) |&gt; group_by(to_u, from_u) |&gt; summarise(pax = sum(Passengers)) |&gt; rename(to = to_u, from = from_u) ## `summarise()` has grouped output by &#39;to_u&#39;. You can override using the `.groups` argument. # nodos, y agregar estado nodos &lt;- airports |&gt; activate(nodes) |&gt; as_tibble() |&gt; separate(City, into = c(&#39;ciudad_nombre&#39;, &#39;estado&#39;), sep = &#39;, &#39;) # construir nueva red rutas &lt;- tbl_graph(nodes = nodos, edges = aristas_agregados, directed = FALSE) rutas ## # A tbl_graph: 755 nodes and 4623 edges ## # ## # An undirected simple graph with 6 components ## # ## # Node Data: 755 × 4 (active) ## name ciudad_nombre estado Position ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 BGR Bangor ME N444827 W0684941 ## 2 BOS Boston MA N422152 W0710019 ## 3 ANC Anchorage AK N611028 W1495947 ## 4 JFK New York NY N403823 W0734644 ## 5 LAS Las Vegas NV N360449 W1150908 ## 6 MIA Miami FL N254736 W0801726 ## # … with 749 more rows ## # ## # Edge Data: 4,623 × 3 ## from to pax ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 2 8 ## 2 1 4 481 ## 3 1 6 7 ## # … with 4,620 more rows rutas &lt;- rutas |&gt; activate(nodes) |&gt; mutate(grupos = group_fast_greedy()) layout_rutas &lt;- create_layout(rutas, layout = &quot;graphopt&quot;, niter = 500) ggraph(layout_rutas) + geom_edge_link(aes(alpha = pax)) + geom_node_point(aes(colour = factor(grupos))) + theme_graph(base_family = &quot;sans&quot;) Podemos ver la modularidad igraph::modularity(rutas, as.factor(rutas |&gt; activate(nodes) |&gt; pull(grupos))) ## [1] 0.4287122 Podemos examinar algunas comunidades: rutas_12 &lt;- rutas |&gt; activate(nodes) |&gt; filter(grupos %in% c(1, 3, 4, 5, 6)) layout_rutas_12 &lt;- create_layout(rutas_12, layout = &quot;graphopt&quot;, niter = 2000) ggraph(layout_rutas_12) + geom_edge_link(aes(alpha = pax)) + geom_node_point(aes(colour = factor(grupos))) + theme_graph(base_family = &quot;sans&quot;) Podemos examinar otras soluciones: grupos_10 &lt;- igraph::fastgreedy.community(rutas) |&gt; igraph::cut_at(10) rutas &lt;- rutas |&gt; activate(nodes) |&gt; mutate(grupos_10 = grupos_10) igraph::modularity(rutas, as.factor(grupos_10)) ## [1] 0.4213694 ggraph(layout_rutas) + geom_edge_link(aes(alpha = pax)) + geom_node_point(aes(colour = factor(grupos_10))) + theme_graph(base_family = &quot;sans&quot;) Referencias "],["modelos-de-lenguaje-y-n-gramas.html", "9 Modelos de lenguaje y n-gramas 9.1 Ejemplo: Modelo de canal ruidoso 9.2 Corpus y vocabulario 9.3 Modelos de lenguaje: n-gramas 9.4 Modelo de n-gramas usando conteos 9.5 Notas de periódico: modelo de n-gramas simples. 9.6 Evaluación de modelos 9.7 Suavizamiento de conteos: otros métodos 9.8 Ejemplo: Corrector de ortografía", " 9 Modelos de lenguaje y n-gramas Los modelos de lenguaje son una parte fundamental de varias tareas de NLP (procesamiento de lenguaje natural), como reconocimiento de lenguaje hablado, reconocimiento de lenguaje escrito, traducción automática, corrección de ortografía, sistemas de predicción de escritura, etc. (ver (Jurafsky and Martin 2000), capítulo 4, o en la nueva edición el capítulo 3 ). Un modelo del lenguaje de tipo estadístico es una asignación de probabilidades \\(P(W)\\) a cada posible frase del lenguaje \\(W = w_1 w_2 w_3 \\cdots w_n\\). Estos modelos del lenguaje están diseñados para resolver distintas tareas particulares y estimar probabilidades particulares de interés, de modo que hay muchas maneras de entrenar estos modelos. Comenzaremos por entender métodos básicos para construir estos modelos, como conteo de n-gramas, y consideraremos métodos modernos que se basan en representaciones distribuidas. 9.1 Ejemplo: Modelo de canal ruidoso El modelo del canal ruidoso muestra una situación general en la que los modelos del lenguaje son importantes: Canal ruidoso En el modelo del canal ruidoso tratamos mensajes recibidos como si fueran distorsionados o transformados al pasar por un canal de comunicación ruidoso (por ejemplo, escribir en el celular). La tarea que queremos resolver con este modelo es inferir la palabra o texto correctos a partir de Un modelo de distorsión o transformación (modelo del canal) Un modelo del lenguaje. Ahora veremos por qué necesitamos estas dos partes. Supongamos que recibimos un mensaje codificado o transformado \\(X\\) (quizá texto con errores, o sonido, o una página escrita), y quisiéramos recuperar el mensaje original \\(W\\) (sucesión de palabras en texto). Este problema lo podemos enfocar como sigue: Quisiéramos calcular, para cada mensaje en texto \\(W\\) la probabilidad \\[P(W|X).\\] Propondríamos entonces como origen el texto \\(W^*\\) que maximiza esta probabilidad condicional: \\[W^* = argmax_W P(W|X),\\] que en principio es un máximo sobre todas las posibles frases del lenguaje. ¿Cómo construimos esta probabilidad condicional? Tenemos que para cada posible frase \\(W\\), \\[P(W|X) = \\frac{P(X|W)P(W)}{P(X)},\\] así que podemos escribir (\\(X\\) es constante) \\[ W^* = argmax_W P(X|W)P(W).\\] Esta forma tiene dos partes importantes: Verosimilitud: la probabilidad \\(P(X|W)\\) de observar el mensaje transfromado \\(X\\) dado que el mensaje es \\(W\\). Este es el modelo del canal (o modelo de errores), que nos dice cómo ocurren errores o transformaciones \\(X\\) cuando se pretende comunicar el mensaje \\(W\\). Inicial o previa: La probabilidad \\(P(W)\\) de observar el mensaje \\(W\\) en el contexto actual. Esto depende más del lenguaje que del canal, y le llamamos el modelo del lenguaje. Nótese que con estas dos partes tenemos un modelo generativo para mensajes del canal ruidoso: primero seleccionamos un texto mediante el modelo de lenguaje, con las probabilidades \\(P(W)\\), y dado el mensaje construimos el mensaje recibido según las probabilidades \\(P(X|W)\\). Ejemplos Supongamos que recibimos el mensaje \\(X=\\)“Estoy a días minutos,” y supongamos que tenemos tres frases en nuestro lenguaje: \\(W_1=\\)“Estoy a veinte minutos,” \\(W_2=\\)“Estoy a diez minutos,” y \\(W_3\\)= “No voy a llegar,” y \\(W_4\\)=“Estoy a tías minutos.” Supongamos que las probabilidades de cada una, dadas por el modelo de lenguaje, son (independientemente del mensaje recibido): W P(W) \\(W_1\\) 1e-03 \\(W_2\\) 1e-03 \\(W_3\\) 8e-03 \\(W_4\\) 1e-06 Ahora supongamos que el modelo del canal (digamos que sabemos el mecanismo mediante el cual se escriben los mensajes de texto) nos da: W P(W) P(X|W) \\(W_1\\) 1e-03 0.01 \\(W_2\\) 1e-03 0.12 \\(W_3\\) 8e-03 0.00 \\(W_4\\) 1e-06 0.05 Obsérvese que la probabilidad condicional más alta es la segunda, y en este modelo es imposible que \\(W_3\\) se transforme en \\(X\\) bajo el canal ruidoso. Multiplicando estas dos probabilidades obtenemos: W P(W) P(X|W) P(X|W)P(W) \\(W_1\\) 2e-03 0.01 0.00002 \\(W_2\\) 1e-03 0.12 0.00012 \\(W_3\\) 8e-03 0.00 0.00000 \\(W_4\\) 1e-06 0.05 0.00000 de modo que escogeríamos la segunda frase (máxima probabilidad condicional) como la interpretación de “Estoy a días minutos.” Nótese que \\(P(X|W_3)\\) en particular es muy bajo, porque es poco posible que el canal distosione “No voy a llegar” a “Estoy a días minutos.” Por otro lado \\(P(W_4)\\) también es muy bajo, pues la frase \\(W_4\\) tiene probabilidad muy baja de ocurrir. Ejercicio Piensa cómo sería el modelo de canal ruidoso \\(P(X|W)\\) si nuestro problema fuera reconocimiento de frases habladas o escritas, o en traducción entre dos lenguajes. 9.2 Corpus y vocabulario En los ejemplos vistos arriba, vemos que necesitamos definir qué son las palabras \\(w_i\\), qué lenguaje estamos considerando, y cómo estimamos las probabilidades. Un corpus es una colección de textos (o habla) del lenguaje que nos interesa. El vocabulario es una colección de palabras que ocurren en el corpus (o más general, en el lenguaje). La definición de palabra (token) depende de la tarea de NLP que nos interesa. Algunas decisiones que tenemos que tomar, por ejemplo: Generalmente, cada palabra está definida como una unidad separada por espacios o signos de puntuación. Los signos de puntuación pueden o no considerarse como palabras. Pueden considerarse palabras distintas las que tienen mayúscula y las que no (por ejemplo, para reconocimiento de lenguaje hablado no nos interesan las mayúsculas). Pueden considerarse palabras distintas las que están escritas incorrectamente, o no. Pueden considerarse plurales como palabras distintas, formas en masculino/femenino, etc. (por ejemplo, en clasificación de textos quizá sólo nos importa saber la raíz en lugar de la forma completa de la palabra). Comienzos y terminación de oraciones pueden considerarse como “palabras” (por ejemplo, en reconocimiento de texto hablado). Al proceso que encuentra todas las palabras en un texto se le llama tokenización o normalización de texto. Los tokens de un texto son las ocurrencias en el texto de las palabras en el vocabuario. En los ejemplos que veremos a continuación consideraremos las siguiente normalización: Consideramos como una palabra el comienzo y el fin de una oración. Normalizamos el texto a minúsculas. No corregimos ortografía, y consideramos las palabras en la forma que ocurren. Consideramos signos de puntuación como palabras. El vocabulario es el conjunto de todas las palabras posibles en nuestros textos. Lo denotaremos como \\(w^1, w^2, \\ldots, w^N\\), para un vocabulario de \\(N\\) distintas palabras o tokens. 9.3 Modelos de lenguaje: n-gramas Consideremos cómo construiríamos las probabilidades \\(P(W)\\), de manera que reflejen la ocurrencia de frases en nuestro lenguaje. Escribimos \\[W=w_1 w_2 w_3 \\cdots w_n,\\] donde \\(w_i\\) son las palabras que contienen el texto \\(W\\). Aquí nos enfrentamos al primer problema: Dada la variedad de frases que potencialmente hay en el lenguaje, no tiene mucho sentido intentar estimar o enumerar directamente estas probabilidades. Por ejemplo, si intentamos algo como intentar ver una colección de ejemplos del lenguaje, veremos que el número de frases es muy grande, y la mayor parte de los textos o frases posibles en el lenguaje no ocurren en nuestra colección (por más grande que sea la colección). Para tener un acercamiento razonable, necesitamos considerar un modelo \\(P(W)\\) con más estructura o supuestos. Hay varias maneras de hacer esto, y un primer acercamiento consiste en considerar solamente el contexto más inmediato de cada palabra. Es decir, la probabilidad de ocurrencia de palabras en una frase generalmente se puede evaluar con un contexto relativamente chico (palabras cercanas) y no es necesario considerar la frase completa. Consideramos entonces la regla del producto: \\[P(w_1 w_2 w_3 \\cdots w_n) = P(w_1)P(w_2|w_1)P(w_3|w_1 w_2) \\cdots P(w_n|w_1 w_2 w_3 \\cdots w_{n-1})\\] Y observamos entonces que basta con calcular las probabilidades condicionales de la siguiente palabra: \\[P(w_{m+1}|w_1\\cdots w_m)\\] para cualquier conjunto de palabras \\(w,w_1,\\ldots, w_m\\). A estas \\(w,w_1,\\ldots, w_m\\) palabras que ocurren justo antes de \\(w\\) les llamamos el contexto de \\(w\\) en la frase. Por la regla del producto, podemos ver entonces nuestro problema como uno de predecir la palabra siguiente, dado el contexto. Por ejemplo, si tenemos la frase “como tengo examen entonces voy a ….” la probabilidad de observar “estudiar” o “dormir” debe ser más alta que la de “gato.” A una sucesión de longitud \\(n\\) de palabras \\(w_1w_2\\cdots w_n\\) le llamamos un n-grama de nuestro lenguaje. Igualmente, calcular todas estas condicionales contando tampoco es factible, pues si el vocabulario es de tamaño \\(V\\), entonces los contextos posibles son de tamaño \\(V^m\\), el cual es un número muy grande incluso para \\(m\\) no muy grande. Pero podemos hacer una simplificación: suponer que la predicción será suficientemente buena si limitamos el contexto de la siguiente palabra a \\(n\\)-gramas, para una \\(n\\) relativamente chica. Por ejemplo, para bigramas, solo nos interesa calcular \\[P(w_m|w_{m-1}),\\] la dependencia hacia atrás contando solo la palabra anteriore. Simplificaríamos la formula de arriba como sigue: \\[P(w_1 w_2 w_3 \\cdots w_n) = P(w_1)P(w_2|w_1)P(w_3|w_2) P(w_4|w_3)\\cdots P(w_n| w_{n-1})\\] Este se llama modelo basado en bigramas. En el caso más simple, establecemos que la ocurrencia de una palabra es independiente de su contexto: \\[P(w|w_1\\cdots w_{n-1}) = P(w)\\] de forma que el modelo del lenguaje se simplifica a: \\[P(w_1 w_2 w_3 \\cdots w_n) = P(w_1)P(w_2)P(w_3) \\cdots P(w_n)\\] A este modelo le llamamos el modelo de unigramas. 9.3.1 Modelo generativo de n-gramas Para que estos modelos den realmente una distribución de probabilidad sobre todas las frases posibles, es necesario tener también un modelo de la longitud de las frases. Una manera es definir \\(P(N)\\), que es la distribución sobre la longitud de frase. Sin embargo, la manera más común es introducir dos símbolos nuevos Para inicio de frase usamos \\(&lt;s&gt;\\) Para fin de frase usamos \\(&lt;/s&gt;\\) De esta manera, el proceso de generación de frases en unigramas es el siguiente: Comenzamos con el símbolo \\(&lt;s&gt;\\). Comenzando con \\(i = 1\\): Escogemos una palabra \\(w_i\\) del vocabulario \\(w^1\\) a \\(w^N\\) y \\(&lt;/s&gt;\\). Estas probabilidades las denotamos como \\(P(w^1), P(w^2),\\ldots, P(w^N)\\) y \\(P(&lt;/s&gt;)\\), y todas estas probabilides deben sumar 1. Si escogimos \\(&lt;/s&gt;\\), terminamos la frase. En otro caso, regresamos a 2 con \\(i=i+1\\). La probabilidad de escoger la frase \\(P(&lt;s&gt;w_1w_2\\cdots w_n&lt;/s&gt;)\\) es entonces \\(P(&lt;s&gt;w_1w_2\\cdots w_n&lt;/s&gt;) = P(w_1)P(w_2)\\cdots P(w_n)P(&lt;/s&gt;).\\) Por construcción, las probabilidades sobre todas las frases suman 1. Necesitamos definir todas las probabilidades \\[P(w)\\] donde \\(w\\) es una palabra o \\(&lt;/s&gt;\\). Ejercicio Supón que solo hay dos palabras en nuestro vocabulario \\(a\\) y \\(b\\). Calcula cuál es la probabilidad de obtener una frase de tamaño 1, 2, 3, etc. Verifica que estas probabilidades suman 1. (supón que están definidos \\(p(a),p(b),p(&lt;/s&gt;)\\) y suman 1). Para bigramas el proceso de generación es el siguiente: Comenzamos con el símbolo \\(&lt;s&gt;\\). Escogemos una palabra \\(w_1\\) según las probabilidades ya definidas \\[P(w^1|&lt;s&gt;), P(w^2|&lt;s&gt;), \\ldots, P(w^N|&lt;s&gt;)\\] sobre el vocabulario. Estas probabilidades suman 1. Comenzando con \\(i=1\\), Escogemos una palabra \\(w_{i+1}\\) según las probabilidades ya definidas \\[P(w^1|w_i), P(w^2|w_i), \\ldots, P(w^N|w_i)\\] sobre el vocabulario. Estas probabilidades suman 1. 4. Si la palabra escogida es \\(&lt;/s&gt;\\) terminamos. Si no, repetimos 3 con \\(i=i+1\\). La probabilidad de una frase dada dado el modelo de bigramas es entonces: \\[P(&lt;s&gt;w_1\\cdots w_n &lt;/s&gt;) = P(w_1|&lt;s&gt;)P(w_2|w_1)P(w_3|w_2) P(w_4|w_3) \\cdots P(w_n|w_{n-1})P(&lt;/s&gt;|w_n).\\] Por definición, la suma de probabilidades sobre todas las frases es 1. Necesitamos definir entonces todas la probablidades \\[p(w|z),\\] donde \\(z\\) es una palabra o \\(&lt;s&gt;\\), y \\(w\\) es una palabra o \\(&lt;/s&gt;\\). Para trigramas podemos agregar un símbolo \\(&lt;s&gt;&lt;s&gt;\\) al inicio. Esto nos permite definir de manera simple el proceso de generación como sigue: Comenzamos con los dos símbolos \\(&lt;s&gt;&lt;s&gt;\\). Comenzando con \\(i=1\\): Cada palabra nueva \\(w_i\\) se escoge según las probabilidades \\(P(w|w_1w_2)\\) sobre \\(w\\) en el vocabulario (\\(w^1\\), \\(w^N\\)). Estas probabilidades deben sumar uno. Terminamos cuando la palabra nueva escogida es \\(&lt;/s&gt;\\). Si no, repetimos 2 con \\(i=i+1\\) Nótese que en la primera elección escogemos de \\(p(w|&lt;s&gt;&lt;s&gt;)\\) y luego de \\(p(w|&lt;s&gt; w^1)\\). La probabilidad de una frase dada dado el modelo de bigramas es entonces: \\[P(&lt;s&gt;&lt;s&gt;w_1\\cdots w_n &lt;/s&gt;) = P(w_1|&lt;s&gt;&lt;s&gt;)P(w_2|&lt;s&gt;w_1)P(w_3|w_1w_2) P(w_4|w_2w_3) \\cdots P(&lt;/s&gt;|w_{n-1}w_n).\\] El modelo de unigramas puede ser deficiente para algunas aplicaciones, y las probabilidades calculadas con este modelo pueden estar muy lejos ser razonables. Por ejemplo, el modelo de unigramas da la misma probabilidad a la frase un día y a la frase día un, aunque la ocurrencia en el lenguaje de estas dos frases es muy distinta. Ejemplo Supongamos que consideramos el modelo de bigramas, y queremos calcular la probabilidad de la frase “el perro corre.” Agregamos inicio y final de frase para obtener \\(&lt;s&gt; el \\, perro \\, corre &lt;/s&gt;\\) Y ahora pondríamos: \\[P(el,perro,corre) = P(el|&lt;s&gt;)P(perro \\,|\\, el)P(corre \\,|\\, perro)P(&lt;/s&gt;|corre)\\] Nótese que aún para frases más largas, sólo es necesario definir \\(P(w|z)\\) para cada para de palabras del vocabulario \\(w,z\\), lo cual es un simplificación considerable. La probabilidad de bigramas también se puede escribir como, usando las convenciones de caracteres de inicio y de final de frase, como \\[P(w_1\\cdots w_n) = \\prod_{j=1}^n P(w_j | w_{j -1}).\\] Y así podemos seguir. Por ejemplo, con el modelo de trigramas, \\[P(w_1\\cdots w_n) = P(w_3|w_1w_2) P(w_4|w_2w_3) \\cdots P(w_n| w_{n-2} w_{n-1}).\\] En este caso, usamos dos símbolos de inicio de frase \\(&lt;s&gt; &lt;s&gt;\\) para que la fórmula tenga sentido. Observación: los modelos de n-gramas son aproximaciones útiles, y fallan en la modelación de dependencias de larga distancia. Por ejemplo, en la frase “Tengo un gato en mi casa, y es de tipo persa” tendríamos que considerar n-gramas imprácticamente largos para modelar correctamente la ocurrencia de la palabra “persa” al final de la oración. 9.4 Modelo de n-gramas usando conteos Supongamos que tenemos una colección de textos o frases del lenguaje que nos interesa. ¿Cómo podemos estimar las probabilidades del tipo \\(P(w|a)\\) \\(P(w|a,b)\\)? El enfoque más simple es estimación por máxima verosimilitud, que en esto caso implica estimar con conteos de ocurrencia en el lenguaje. Si queremos estimar \\(P(w|z)\\) (modelo de bigramas), entonces tomamos nuestra colección, y calculamos: \\(N(z,w)\\) = número de veces que aparece \\(z\\) seguida de \\(w\\) \\(N(z)\\) = número de veces que aparece \\(z\\). \\(P(w|z) = \\frac{N(z,w)}{N(z)}\\) Nótese que estas probabilidades en general son chicas (pues el vocabulario es grande), de forma que conviene usar las log probabilidades para hacer los cálculos y evitar underflows. Calculamos entonces usando: \\(\\log{P(w|z)} = \\log{N(zw)} - \\log{N(z)}\\) ¿Cómo se estimarían las probabilidades para el modelo de unigramas y trigramas? Ejercicio Considera la colección de textos “un día muy soleado,” “un día muy lluvioso,” “un ejemplo muy simple de bigramas.” Estima las probabilidades \\(P(muy|&lt;s&gt;)\\), \\(P(día | un)\\), \\(P(simple | muy)\\) usando conteos. Ejemplo Comenzamos por limpiar nuestra colección de texto, creando también tokens adicionales para signos de puntuación. En este caso solo tenemos dos textos: normalizar &lt;- function(texto, vocab = NULL){ # minúsculas texto &lt;- tolower(texto) # varios ajustes texto &lt;- gsub(&quot;\\\\s+&quot;, &quot; &quot;, texto) texto &lt;- gsub(&quot;\\\\.[^0-9]&quot;, &quot; _punto_ &quot;, texto) texto &lt;- gsub(&quot; _s_ $&quot;, &quot;&quot;, texto) texto &lt;- gsub(&quot;\\\\.&quot;, &quot; _punto_ &quot;, texto) texto &lt;- gsub(&quot;[«»¡!¿?-]&quot;, &quot;&quot;, texto) texto &lt;- gsub(&quot;;&quot;, &quot; _punto_coma_ &quot;, texto) texto &lt;- gsub(&quot;\\\\:&quot;, &quot; _dos_puntos_ &quot;, texto) texto &lt;- gsub(&quot;\\\\,[^0-9]&quot;, &quot; _coma_ &quot;, texto) texto &lt;- gsub(&quot;\\\\s+&quot;, &quot; &quot;, texto) texto } corpus_mini &lt;- c(&quot;Este es un ejemplo: el perro corre, el gato escapa. Este es un número 3.1416, otro número es 1,23.&quot;, &quot;Este es otro ejemplo. &quot; ) normalizar(corpus_mini) ## [1] &quot;este es un ejemplo _dos_puntos_ el perro corre _coma_ el gato escapa _punto_ este es un número 3 _punto_ 1416 _coma_ otro número es 1,23 _punto_ &quot; ## [2] &quot;este es otro ejemplo _punto_ &quot; Y ahora construimos, por ejemplo, los bigramas que ocurren en cada texto ejemplo &lt;- tibble(txt = corpus_mini) |&gt; mutate(id = row_number()) |&gt; mutate(txt = normalizar(txt)) bigrams_ejemplo &lt;- ejemplo |&gt; unnest_tokens(bigramas, txt, token = &quot;ngrams&quot;, n = 2) |&gt; group_by(bigramas) |&gt; tally() knitr::kable(bigrams_ejemplo) bigramas n coma el 1 coma otro 1 dos_puntos el 1 punto 1416 1 punto este 1 1,23 punto 1 1416 coma 1 3 punto 1 corre coma 1 ejemplo dos_puntos 1 ejemplo punto 1 el gato 1 el perro 1 es 1,23 1 es otro 1 es un 2 escapa punto 1 este es 3 gato escapa 1 número 3 1 número es 1 otro ejemplo 1 otro número 1 perro corre 1 un ejemplo 1 un número 1 9.5 Notas de periódico: modelo de n-gramas simples. En los siguientes ejemplos, utilizaremos una colección de noticias cortas en español (de España). library(tidyverse) ruta &lt;- &quot;../datos/noticias/ES_Newspapers.txt&quot; if(!file.exists(ruta)){ periodico &lt;- read_lines(file= &quot;https://es-noticias.s3.amazonaws.com/Es_Newspapers.txt&quot;, progress = FALSE) write_lines(periodico, ruta) } else { periodico &lt;- read_lines(file= ruta, progress = FALSE) } length(periodico) ## [1] 309918 periodico[1:2] ## [1] &quot;En este sentido, señala que «no podemos consentir» que se repita «el malogrado caso del Centro de Transportes de Benavente, donde la falta de control ha supuesto un cúmulo de irregularidades que rozan lo delictivo».&quot; ## [2] &quot;\\&quot;Cuando acabe la experiencia con el Inter no me quedaré en Italia, sino que espero ir a España, porque mi objetivo es ganar los títulos de los tres campeonatos más competitivos del mundo\\&quot;, afirmó el que fuera entrenador del Barcelona B, y añadió: \\&quot;me falta Liga\\&quot;.&quot; periodico_df &lt;- tibble(txt = periodico) |&gt; mutate(id = row_number()) |&gt; mutate(txt = normalizar(txt)) Adicionalmente, seleccionamos una muestra para hacer las demostraciones (puedes correrlo con todo el corpus): set.seed(123) muestra_ind &lt;- sample(1:nrow(periodico_df), 1e5) periodico_m &lt;- periodico_df[muestra_ind, ] Y calculamos las frecuencias de todos unigramas, bigramas y trigramas: conteo_ngramas &lt;- function(corpus, n = 1, vocab_df = NULL){ token_nom &lt;- paste0(&#39;w_n_&#39;, rev(seq(1:n)) - 1) token_cond &lt;- token_nom[-length(token_nom)] # añadir inicio de frases inicio &lt;- paste(rep(&quot;_s_ &quot;, n - 1), collapse = &quot;&quot;) # añadir fin de frases fin &lt;- &quot; _ss_&quot; ngramas_df &lt;- corpus |&gt; mutate(txt = paste(inicio, txt, fin)) |&gt; unnest_tokens(ngrama, txt, token = &quot;ngrams&quot;, n = n) frec_ngramas &lt;- ngramas_df |&gt; group_by(ngrama) |&gt; summarise(num = length(ngrama)) |&gt; separate(ngrama, token_nom, sep=&quot; &quot;) |&gt; group_by(across(all_of(token_cond))) |&gt; mutate(denom = sum(num)) |&gt; ungroup() |&gt; mutate(log_p = log(num) - log(denom)) frec_ngramas } mod_uni &lt;- conteo_ngramas(periodico_m, n = 1) mod_bi &lt;- conteo_ngramas(periodico_m, n = 2) mod_tri &lt;- conteo_ngramas(periodico_m, n = 3) mod_uni |&gt; arrange(desc(num)) |&gt; head(100) |&gt; knitr::kable() w_n_0 num denom log_p de 426481 6894607 -2.782927 coma 380344 6894607 -2.897419 la 261913 6894607 -3.270482 punto 248139 6894607 -3.324506 el 212825 6894607 -3.478025 que 210647 6894607 -3.488311 en 183614 6894607 -3.625659 y 158689 6894607 -3.771549 a 130347 6894607 -3.968295 los 108758 6894607 -4.149370 ss 100000 6894607 -4.233325 del 84899 6894607 -4.397032 se 77632 6894607 -4.486515 las 68042 6894607 -4.618370 un 66732 6894607 -4.637810 por 62876 6894607 -4.697330 con 60358 6894607 -4.738201 una 49207 6894607 -4.942459 para 48948 6894607 -4.947736 no 46051 6894607 -5.008745 su 42489 6894607 -5.089250 ha 41228 6894607 -5.119377 al 39885 6894607 -5.152495 es 35381 6894607 -5.272320 lo 27222 6894607 -5.534469 más 27140 6894607 -5.537486 como 26150 6894607 -5.574646 este 16660 6894607 -6.025484 pero 15365 6894607 -6.106403 sus 14892 6894607 -6.137671 han 14145 6894607 -6.189134 o 13919 6894607 -6.205240 esta 12189 6894607 -6.337961 también 12089 6894607 -6.346199 ya 12056 6894607 -6.348932 dos 11364 6894607 -6.408044 años 11314 6894607 -6.412454 entre 11124 6894607 -6.429390 le 11083 6894607 -6.433082 dos_puntos 10785 6894607 -6.460338 desde 10293 6894607 -6.507031 sobre 9659 6894607 -6.570605 año 9355 6894607 -6.602584 si 9324 6894607 -6.605903 fue 9224 6894607 -6.616686 sin 8968 6894607 -6.644832 está 8861 6894607 -6.656835 hasta 8563 6894607 -6.691044 todo 8409 6894607 -6.709192 muy 8354 6894607 -6.715754 cuando 8240 6894607 -6.729494 según 8018 6894607 -6.756806 son 7992 6894607 -6.760054 porque 7548 6894607 -6.817212 gobierno 7402 6894607 -6.836745 euros 7310 6894607 -6.849252 hay 7274 6894607 -6.854188 ser 7198 6894607 -6.864692 parte 6866 6894607 -6.911913 tiene 6676 6894607 -6.939976 punto_coma 6558 6894607 -6.957809 así 6489 6894607 -6.968386 todos 6424 6894607 -6.978454 además 6403 6894607 -6.981728 tras 6223 6894607 -7.010243 aunque 6020 6894607 -7.043407 tres 5965 6894607 -7.052586 durante 5872 6894607 -7.068300 sido 5845 6894607 -7.072908 me 5823 6894607 -7.076679 ayer 5774 6894607 -7.085130 españa 5725 6894607 -7.093652 000 5671 6894607 -7.103129 uno 5532 6894607 -7.127945 presidente 5488 6894607 -7.135931 sólo 5388 6894607 -7.154320 día 5372 6894607 -7.157294 ahora 5353 6894607 -7.160838 donde 5344 6894607 -7.162520 pasado 5318 6894607 -7.167397 después 5287 6894607 -7.173244 hace 5206 6894607 -7.188683 millones 5188 6894607 -7.192146 partido 5036 6894607 -7.221883 otros 4843 6894607 -7.260960 vez 4787 6894607 -7.272591 primera 4778 6894607 -7.274473 ante 4682 6894607 -7.294769 hoy 4663 6894607 -7.298836 contra 4643 6894607 -7.303134 había 4602 6894607 -7.312004 puede 4590 6894607 -7.314615 cada 4516 6894607 -7.330868 equipo 4484 6894607 -7.337979 personas 4479 6894607 -7.339095 e 4464 6894607 -7.342450 gran 4439 6894607 -7.348066 ni 4418 6894607 -7.352808 nos 4363 6894607 -7.365335 antes 4352 6894607 -7.367859 mod_bi |&gt; arrange(desc(num)) |&gt; head(100) |&gt; knitr::kable() w_n_1 w_n_0 num denom log_p punto ss 93073 248139 -0.9806049 de la 64367 426481 -1.8909667 en el 34475 183614 -1.6726013 en la 29086 183614 -1.8425788 de los 26980 426481 -2.7604720 coma que 22964 380344 -2.8071483 a la 22943 130347 -1.7371872 que se 19073 210647 -2.4019100 coma el 19019 380344 -2.9956376 de las 16151 426481 -3.2735859 s el 15308 100000 -1.8767946 coma y 15277 380344 -3.2147277 coma en 14330 380344 -3.2787209 que el 13521 210647 -2.7459397 coma la 13421 380344 -3.3442555 punto el 12165 248139 -3.0154261 a los 12116 130347 -2.3756732 lo que 11115 27222 -0.8957299 s la 10394 100000 -2.2639415 que la 10203 210647 -3.0275020 coma pero 9406 380344 -3.6997283 con el 9033 60358 -1.8994090 y el 9022 158689 -2.8672803 coma con 8573 380344 -3.7924584 por la 8400 62876 -2.0129328 por el 8387 62876 -2.0144816 punto la 8349 248139 -3.3918473 en los 8314 183614 -3.0948949 punto en 8114 248139 -3.4203981 que no 8111 210647 -3.2569626 con la 7989 60358 -2.0222279 y la 7970 158689 -2.9912618 de que 7956 426481 -3.9816415 coma de 7696 380344 -3.9003754 a las 7561 130347 -2.8471967 coma se 7363 380344 -3.9446086 s en 7271 100000 -2.6212764 de un 7268 426481 -4.0720867 coma a 7076 380344 -3.9843673 en un 6681 183614 -3.3135681 coma ha 6528 380344 -4.0649755 en las 6237 183614 -3.3823364 coma por 6218 380344 -4.1136278 de su 6073 426481 -4.2517151 se ha 6037 77632 -2.5540725 y que 5964 158689 -3.2812049 punto 000 5671 248139 -3.7786236 coma los 5604 380344 -4.2175955 que los 5555 210647 -3.6354853 de una 5551 426481 -4.3415897 en su 5532 183614 -3.5022863 la que 5432 261913 -3.8757050 el que 5285 212825 -3.6955976 coma como 5281 380344 -4.2769606 que en 5084 210647 -3.7240854 no se 4974 46051 -2.2255251 que coma 4896 210647 -3.7617652 coma según 4827 380344 -4.3668509 y de 4796 158689 -3.4991641 punto y 4447 248139 -4.0217594 a su 4350 130347 -3.4000243 coma no 4341 380344 -4.4729714 que ha 4336 210647 -3.8832315 en una 4263 183614 -3.7628626 los que 4256 108758 -3.2407955 para el 4160 48948 -2.4652434 coma un 4072 380344 -4.5369418 más de 4060 27140 -1.8998257 con un 4056 60358 -2.7000962 coma aunque 3931 380344 -4.5721823 y en 3922 158689 -3.7003446 punto los 3898 248139 -4.1535255 millones de 3866 5188 -0.2941279 s los 3784 100000 -3.2743885 para la 3766 48948 -2.5647451 todos los 3617 6424 -0.5743960 uno de 3593 5532 -0.4315619 punto no 3562 248139 -4.2436669 punto punto 3559 248139 -4.2445095 para que 3535 48948 -2.6280452 coma al 3524 380344 -4.6814794 coma lo 3511 380344 -4.6851752 coma ya 3499 380344 -4.6885989 a un 3480 130347 -3.6231678 ha sido 3430 41228 -2.4865574 es el 3417 35381 -2.3374120 ya que 3379 12056 -1.2719827 y los 3363 158689 -3.8541129 de este 3310 426481 -4.8586196 es que 3303 35381 -2.3713438 coma una 3255 380344 -4.7608838 coma para 3229 380344 -4.7689036 de sus 3219 426481 -4.8864971 además coma 3192 6403 -0.6961190 en este 3190 183614 -4.0528148 el presidente 3136 212825 -4.2175221 con los 3134 60358 -2.9579834 después de 3129 5287 -0.5245375 y a 3092 158689 -3.9381282 se han 3079 77632 -3.2273748 ¿Qué palabra es más probable que aparezca después de en, la palabra la o la palabra el? mod_tri |&gt; arrange(desc(num)) |&gt; head(100) |&gt; knitr::kable() w_n_2 w_n_1 w_n_0 num denom log_p s s el 15308 100000 -1.8767946 s s la 10394 100000 -2.2639415 s s en 7271 100000 -2.6212764 s s los 3784 100000 -3.2743885 s s por 2915 100000 -3.5353004 en el que 2846 34475 -2.4943199 coma en el 2724 14330 -1.6602539 coma ya que 2619 3499 -0.2896846 uno de los 2616 3593 -0.3173411 coma lo que 2430 3511 -0.3680096 en la que 2426 29086 -2.4840131 millones de euros 2373 3866 -0.4880654 coma en la 2283 14330 -1.8368649 coma que se 2217 22964 -2.3377728 s s las 1949 100000 -3.9378538 s s a 1883 100000 -3.9723039 sin embargo coma 1873 2062 -0.0961350 por lo que 1862 2350 -0.2327641 una de las 1759 2423 -0.3202610 coma mientras que 1661 2155 -0.2603709 coma por lo 1626 6218 -1.3413253 a través de 1584 1958 -0.2119703 punto 000 euros 1574 5671 -1.2817453 s s no 1568 100000 -4.1553693 punto además coma 1482 1867 -0.2309403 s s según 1475 100000 -4.2165122 coma así como 1404 1721 -0.2035802 presidente de la 1381 2186 -0.4592655 a partir de 1361 1738 -0.2445153 el presidente de 1331 3136 -0.8570176 por su parte 1323 2893 -0.7823921 s s de 1305 100000 -4.3389671 que no se 1284 8111 -1.8432410 punto en el 1271 8114 -1.8537870 se trata de 1252 1533 -0.2024843 s s además 1240 100000 -4.3900588 su parte coma 1237 1388 -0.1151748 a pesar de 1222 1322 -0.0786569 s en el 1212 7271 -1.7916219 s s para 1206 100000 -4.4178611 coma y el 1204 15277 -2.5406991 de la ciudad 1197 64367 -3.9847827 de que el 1184 7956 -1.9050278 coma a la 1119 7076 -1.8442733 que se ha 1106 19073 -2.8475238 en los últimos 1104 8314 -2.0190009 coma y que 1069 15277 -2.6596248 coma con el 1068 8573 -2.0828300 coma que ha 1032 22964 -3.1024291 la que se 1032 5432 -1.6608087 s s con 1016 100000 -4.5892968 coma además de 986 1822 -0.6140337 s en la 979 7271 -2.0051175 el que se 962 5285 -1.7036134 el caso de 961 1655 -0.5435819 coma con un 950 8573 -2.1999110 s s y 950 100000 -4.6564635 coma pero no 948 9406 -2.2947486 la guardia civil 938 1027 -0.0906473 de lo que 936 1979 -0.7487315 punto sin embargo 933 1216 -0.2649169 lo que se 923 11115 -2.4884216 s además coma 919 1240 -0.2995805 coma sobre todo 912 1266 -0.3279776 en el caso 905 34475 -3.6400548 que en el 902 5084 -1.7292391 s s un 901 100000 -4.7094202 s s pero 898 100000 -4.7127554 no obstante coma 896 948 -0.0564141 euros punto ss 881 1632 -0.6165039 a la que 878 22943 -3.2631216 coma que no 877 22964 -3.2651761 punto en la 866 8114 -2.2374613 coma con la 860 8573 -2.2994406 el número de 856 1229 -0.3616857 en este sentido 854 3190 -1.3178450 a lo largo 853 1569 -0.6094342 coma de la 839 7696 -2.2162453 s s así 836 100000 -4.7842969 después de que 836 3129 -1.3198401 de que la 828 7956 -2.2626685 y de la 822 4796 -1.7637971 castilla y león 817 833 -0.0193945 s s una 812 100000 -4.8134251 en los que 810 8314 -2.3286619 s s tras 807 100000 -4.8196018 la posibilidad de 801 858 -0.0687432 un total de 800 823 -0.0283445 por parte de 797 1139 -0.3570513 el presidente del 792 3136 -1.3761420 s s al 788 100000 -4.8434274 s s es 785 100000 -4.8472417 s por su 781 2915 -1.3170499 coma y la 775 15277 -2.9812407 punto 00 horas 769 1182 -0.4298722 y en el 767 3922 -1.6318702 parte de la 762 2513 -1.1932860 a los que 741 12116 -2.7942815 s s también 738 100000 -4.9089816 el resto de 736 1279 -0.5526037 Problema de los ceros Podemos ahora evaluar la probabilidad de ocurrencia de textos utilizando las frecuencias que calculamos arriba: n_gramas &lt;- list(unigramas = mod_uni, bigramas = mod_bi, trigramas = mod_tri) log_prob &lt;- function(textos, n_gramas, n = 2, laplace = FALSE, delta = 0.001, vocab_env = NULL){ df &lt;- tibble(id = 1:length(textos), txt = textos) |&gt; mutate(txt = normalizar(txt)) if(!is.null(vocab_env)){ df &lt;- df |&gt; mutate(txt_u = map_chr(txt, ~restringir_vocab(.x, vocab = vocab_env))) |&gt; select(id, txt_u) |&gt; rename(txt = txt_u) } token_nom &lt;- paste0(&#39;w_n_&#39;, rev(seq(1:n)) - 1) df_tokens &lt;- df |&gt; group_by(id) |&gt; unnest_tokens(ngrama, txt, token = &quot;ngrams&quot;, n = n) |&gt; separate(ngrama, token_nom, &quot; &quot;) |&gt; left_join(n_gramas[[n]], by = token_nom) if(laplace){ V &lt;- nrow(n_gramas[[1]]) log_probs &lt;- log(df_tokens[[&quot;num&quot;]] + delta) - log(df_tokens[[&quot;denom&quot;]] + delta*V ) log_probs[is.na(log_probs)] &lt;- log(1/V) } else { log_probs &lt;- df_tokens[[&quot;log_p&quot;]] } log_probs &lt;- split(log_probs, df_tokens$id) sapply(log_probs, mean) } textos &lt;- c(&quot;un día muy soleado&quot;, &quot;este de es ejemplo un&quot;, &quot;este es un ejemplo de&quot;, &quot;esta frase es exotiquísima&quot;) log_prob(textos, n_gramas, n = 1) ## 1 2 3 4 ## -7.988631 -5.458781 -5.458781 NA log_prob(textos, n_gramas, n = 2) ## 1 2 3 4 ## NA -7.824663 -3.722903 NA log_prob(textos, n_gramas, n = 3) ## 1 2 3 4 ## NA NA -2.177098 NA Observaciones: El modelo de unigramas claramente no captura estructura en el orden de palabras. La segunda frase por ejemplo, tiene probabilidad alta porque tiene tokens o palabras comunes, pero la frase en realidad tendría probabilidad muy baja de ocurrir en el lenguaje. Esto parece incorrecto. La cuarta frase tiene probabilidad 0 en todos los modelos porque la palabra exotiquísima no existe en el vocabulario de entrenamiento. Esto parece incorrecto. La primera frase tiene probabilidad 0 en el modelo de bigramas y trigramas, porque nunca encontró alguna serie de tres palabras juntas. Esto parece incorrecto. Incluso el modelo de bigramas puede dar probabilidad cero a frase que deberían ser relativamente comunes, pues el conjunto de frases es muy grande: n &lt;- 2 textos &lt;- &quot;Otro día muy soleado&quot; df &lt;- tibble(id = 1:length(textos), txt = textos) |&gt; mutate(txt = normalizar(txt)) token_nom &lt;- paste0(&#39;w_n_&#39;, rev(seq(1:n)) - 1) df_tokens &lt;- df |&gt; group_by(id) |&gt; unnest_tokens(ngrama, txt, token = &quot;ngrams&quot;, n = n) |&gt; separate(ngrama, token_nom, &quot; &quot;) |&gt; left_join(n_gramas[[n]], by = token_nom) df_tokens ## # A tibble: 3 × 6 ## # Groups: id [1] ## id w_n_1 w_n_0 num denom log_p ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 otro día 53 3995 -4.32 ## 2 1 día muy 15 5372 -5.88 ## 3 1 muy soleado NA NA NA El problema es que no observamos “muy soleado,” y esta frase tendría 0 probabilidad de ocurrir. Esta última observación es importante: cuando no encontramos en nuestros conteos un bigrama (o trigrama, etc.) dado, la probabilidad asignada es 0. Aunque para algunas frases esta asignación es correcta (una sucesión de palabras que casi no puede ocurrir en el lenguaje), muchas veces esto se debe a que los datos son ralos: la mayor parte de las frases posibles no son observadas en nuestro corpus, y es erróneo asignarles probabilidad 0. Más en general, cuando los conteos de bigramas son chicos, sabemos que nuestra estimación por máxima verosimilitud tendrá varianza alta. Estos ceros ocurren de dos maneras: Algunas palabras son nuevas: no las observamos en nuestros datos de entrenamiento. No observamos bigramas, trigramas, etc. específicos (por ejemplo, observamos día, y aburrido pero no observamos día aburrido). Palabras desconocidas Para el primer problema, podemos entrenar nuestro modelo con una palabra adicional \\(&lt;unk&gt;\\), que denota palabras desconocidas. Una estrategia es tomar las palabras con frecuencia baja y sustituirlas por , por ejemplo: vocabulario_txt &lt;- n_gramas[[1]] |&gt; filter(num &gt; 1) |&gt; pull(w_n_0) vocab_env &lt;- new.env() vocab_env[[&quot;_unk_&quot;]] &lt;- 1 for(a in vocabulario_txt){ vocab_env[[a]] &lt;- 1 } nrow(n_gramas[[1]]) ## [1] 137263 sum(n_gramas[[1]]$num) ## [1] 6894607 length(vocab_env) ## [1] 77933 restringir_vocab &lt;- function(texto, vocab_env){ texto_v &lt;- strsplit(texto, &quot; &quot;)[[1]] texto_v &lt;- lapply(texto_v, function(x){ if(x != &quot;&quot;){ en_vocab &lt;- vocab_env[[x]] if(is.null(en_vocab)){ x &lt;- &quot;_unk_&quot; } x } }) texto &lt;- paste(texto_v, collapse = &quot; &quot;) texto } periodico_m_unk &lt;- periodico_m |&gt; mutate(txt_u = map_chr(txt, ~restringir_vocab(.x, vocab_env = vocab_env))) |&gt; select(id, txt_u) |&gt; rename(txt = txt_u) Y ahora podemos reentrenar nuestros modelos: mod_uni &lt;- conteo_ngramas(periodico_m_unk, n = 1) mod_bi &lt;- conteo_ngramas(periodico_m_unk, n = 2) mod_tri &lt;- conteo_ngramas(periodico_m_unk, n = 3) n_gramas_u &lt;- list(mod_uni, mod_bi, mod_tri) textos &lt;- c(&quot;un día muy soleado&quot;, &quot;este de es ejemplo un&quot;, &quot;este es un ejemplo de&quot;, &quot;esta frase es exotiquísima&quot;) log_prob(textos, n_gramas_u, n = 1, vocab_env = vocab_env) ## 1 2 3 4 ## -8.005118 -5.470732 -5.470732 -6.447402 log_prob(textos, n_gramas_u, n = 2, vocab_env = vocab_env) ## 1 2 3 4 ## NA -7.887497 -3.756993 -4.468375 log_prob(textos, n_gramas_u, n = 3, vocab_env = vocab_env) ## 1 2 3 4 ## NA NA -2.232099 NA Y con esto podemos resolver el problema de vocabulario desconocido. Contexto no observado Para el segundo problema, existen técnicas de suavizamiento (que veremos más adelante). El método más simple es el suavizamiento de Laplace, en el que simplemente agregamos una cantidad \\(\\delta\\) a los conteos de unigramas, bigramas, etc. Para unigramas, agregamos \\(\\delta\\) a cada posible unigrama. Si el tamaño del vocabulario es \\(V\\), y el conteo total de tokens es \\(N\\), el nuevo conteo de tokens será entonces \\(N+\\delta V\\). Por ejemplo, la probabilidad para bigramas es: \\[P(w|a) = \\frac{N(aw)}{N(a)} = \\frac{N(aw)}{\\sum_{z\\in V} N(az)},\\] De modo que la estimación suavizada (sumando \\(\\delta\\) a cada bigrama) es \\[P_{L}(w|a) = \\frac{N(aw)+\\delta}{\\sum_{z\\in V} N(az)+\\delta}= \\frac{N(aw) + \\delta}{N(z)+ \\delta V} \\] Para trigramas, \\[P_{L}(w|ab) = \\frac{N(abw)+\\delta}{\\sum_{z\\in V} N(abz)+\\delta}= \\frac{N(abw) + \\delta}{N(ab)+ \\delta V} \\] y así sucesivamente. Observación: este método es útil para introducir la idea de suavizamiento, pero existen otros mejores que veremos más adelante (ver sección 4.5 de (Jurafsky and Martin 2000), o 3.5 en la edición más reciente). Veremos más adelante también cómo escoger hiperparámetros como \\(\\delta\\). log_prob(textos, n_gramas_u, n = 1, laplace = TRUE, delta = 0.01) ## 1 2 3 4 ## -8.004979 -5.470842 -5.470842 -8.337738 log_prob(textos, n_gramas_u, n = 2, laplace = TRUE, delta = 0.01) ## 1 2 3 4 ## -7.337730 -8.021005 -3.894912 -7.558278 log_prob(textos, n_gramas_u, n = 3, laplace = TRUE, delta = 0.01) ## 1 2 3 4 ## -7.907155 -11.252417 -3.477082 -11.252417 Nótese que este suavizamiento cambia considerablemente las probabilides estimadas, incluyendo algunas de ellas con conteos altos (esta técnica dispersa demasiada probabilidad sobre conteos bajos). 9.6 Evaluación de modelos 9.6.1 Generación de texto Una primera idea de qué tan bien funcionan estos modelos es generando frases según las probabilidades que estimamos. Ejemplo: unigramas Generamos algunas frases bajo el modelo de unigramas. Podemos construir una frase comenzando con el token \\(&lt;s&gt;\\), y paramos cuando encontramos \\(&lt;/s&gt;\\). Cada token se escoge al azar según las probablidades \\(P(w)\\). calc_siguiente_uni &lt;- function(texto, n_gramas){ u &lt;- runif(1) unigramas_s &lt;- arrange(n_gramas[[1]], log_p) prob_acum &lt;- cumsum(exp(unigramas_s$log_p)) palabra_no &lt;- match(TRUE, u &lt; prob_acum) as.character(unigramas_s[palabra_no, &quot;w_n_0&quot;]) } texto &lt;- &quot;&quot; fin &lt;- FALSE set.seed(1215) while(!fin){ siguiente &lt;- calc_siguiente_uni(texto, n_gramas) texto &lt;- c(texto, siguiente) if(siguiente == &quot;_ss_&quot;){ fin &lt;- TRUE } } paste(texto, collapse = &quot; &quot;) ## [1] &quot; las que baleares del con del agua en señalándoles si de en ha mismas _punto_ se pp el de _coma_ los del que _punto_ la en reiterado _punto_ cuentan y los _coma_ derechos hoy la _coma_ al talento en buena gobierno sólo de genero aguas uvi a arreos un en en 3 dos de que es del porque que _ss_&quot; 9.6.1.1 Ejemplo: bigramas calc_siguiente_bi &lt;- function(texto, n_gramas){ u &lt;- runif(1) n &lt;- length(texto) anterior &lt;- texto[n] siguiente_df &lt;- filter(n_gramas[[2]], w_n_1 == anterior) |&gt; arrange(log_p) palabra_no &lt;- match(TRUE, u &lt; cumsum(exp(siguiente_df$log_p))) as.character(siguiente_df[palabra_no, &quot;w_n_0&quot;]) } texto &lt;- &quot;_s_&quot; set.seed(4123) fin &lt;- FALSE while(!fin){ siguiente &lt;- calc_siguiente_bi(texto, n_gramas) texto &lt;- c(texto, siguiente) if(siguiente == &quot;_ss_&quot;){ fin &lt;- TRUE } } paste(texto, collapse = &quot; &quot;) ## [1] &quot;_s_ el concierto pueden encontrarse las poblaciones como la mano derecha de la delincuencia y la mañana en la trama de las alarmas en valladolid _coma_ o alemania podría haber aprendido del documento especifique los portavoces que en el 78 de mehr solar construirá tres fronteras y lo que garantiza la fundación personas indicadas todas las encuestadas 11 españoles _punto_ danays llegaba la posibilidad de la lleva puesto sus aliados parlamentarios para otro tipo de la ola que una hermana con su aventura _punto_ un feto de españa en el sip entre operadores turísticos cuya desembocadura _punto_ definitivamente del ipc previsto entregar al final _coma_ la realidad _punto_ el proceso de interposición de algeciras _coma_ el acuerdo con su educación _coma_ porque prevalecen y no han concluido y llegar al trabajo bien _punto_ _ss_&quot; 9.6.1.2 Ejemplo: trigramas calc_siguiente_tri &lt;- function(texto, n_gramas){ u &lt;- runif(1) n &lt;- length(texto) contexto &lt;- texto[c(n,n-1)] siguiente_df &lt;- filter(n_gramas[[3]], w_n_1 == contexto[1], w_n_2 == contexto[2]) |&gt; arrange(log_p) palabra_no &lt;- match(TRUE, u &lt; cumsum(exp(siguiente_df$log_p))) as.character(siguiente_df[palabra_no, &quot;w_n_0&quot;]) } texto &lt;- c(&quot;_s_&quot;,&quot;_s_&quot;) set.seed(4122) fin &lt;- FALSE while(!fin){ siguiente &lt;- calc_siguiente_tri(texto, n_gramas) texto &lt;- c(texto, siguiente) if(siguiente == &quot;_ss_&quot;){ fin &lt;- TRUE } } paste(texto, collapse = &quot; &quot;) ## [1] &quot;_s_ _s_ en relación con su marido figura entre los años a la formación de salida _coma_ despistado por el gran número de asuntos exteriores y economía y hacienda de los siglos xvi y xvii relacionados con la que se descarta y en el paseo de la escuela de comunicación _coma_ o su boda en normandía _punto_ al mismo tiempo _coma_ esquivar el golpe le desplazó casi 60 millones de euros para emprender estas obras supusieron la eliminación de barreras arquitectónicas _punto_ _ss_&quot; Observación: en este ejemplo vemos cómo los textos parecen más textos reales cuando usamos n-gramas más largos. 9.6.2 Evaluación de modelos: perplejidad En general, la mejor manera de hacer la evaluación de un modelo de lenguaje es en el contexto de su aplicación (es decir, el desempeño en la tarea final para el que construimos nuestro modelo): por ejemplo, si se trata de corrección de ortografía, qué tanto da la palabra correcta, o qué tanto seleccionan usuarios palabras del corrector. Sin embargo también podemos hacer una evaluación intrínseca del modelo considerando muestras de entrenamiento y prueba. Una medida usual en este contexto es la perplejidad. Sea \\(P(W)\\) un modelo del lenguaje. Supongamos que observamos un texto \\(W=w_1 w_2\\cdots w_N\\) (una cadena larga, que puede incluír varios separadores \\(&lt;/s&gt;, &lt;s&gt;\\)). La log-perplejidad del modelo sobre este texto es igual a \\[LP(W) = -\\frac{1}{N} \\log P(w_1 w_2 \\cdots w_N),\\] que también puede escribirse como \\[LP(W) = -\\frac{1}{N} \\sum_{i=1}^N \\log P(w_i|w_1 w_2 \\cdots w_{i-1})\\] La perplejidad es igual a \\[PP(W) = e^{LP(W)},\\] que es la medida más frecuentemente reportada en modelos de lenguaje. Observaciones: La log perplejidad es similar a la devianza (negativo de log-verosimilitud) de los datos (palabras) observadas \\(W\\) bajo el modelo \\(P\\). Cuanto más grande es \\(P(W)\\) bajo el modelo, menor es la perplejidad. Mejores modelos tienen valores más bajos de perplejidad. Buscamos modelos que asignen muy baja probabilidad a frases que no son gramáticas, o no tienen sentido, y alta probabilidad a frases que ocurren con frecuencia. Bajo el modelo de bigramas, por ejemplo, tenemos que \\[LP(W) = -\\frac{1}{N} \\sum_{i=1}^N \\log P(w_i|w_{i-1})\\] Y para el modelo de trigramas: \\[LP(W) = -\\frac{1}{N} \\sum_{i=1}^N \\log P(w_i|w_{i-2}w_{i-1})\\] La evaluación de modelos la hacemos calculando la perplejidad en una muestra de textos de prueba, que no fueron utilizados para entrenar los modelos. Para palabras no vistas, entrenamos nuestros modelos sustituyendo palabras no frecuentes con \\(&lt;unk&gt;\\), y a las palabras no vistas les asignamos el token \\(&lt;unk&gt;\\). Ejemplo Podemos usar nuestra función anterior para calcular la perplejidad. Para los datos de entrenamiento vemos que la perplejidad de entrenamiento es mejor para los modelos más complejos: periodico_entrena &lt;- periodico[muestra_ind] textos &lt;- periodico_entrena[1:1000] texto_entrena &lt;- paste(textos, collapse = &quot; &quot;) exp(-log_prob(texto_entrena, n_gramas_u, n = 1, laplace = T)) ## 1 ## 1054.776 exp(-log_prob(texto_entrena, n_gramas_u, n = 2, laplace = T)) ## 1 ## 174.3516 exp(-log_prob(texto_entrena, n_gramas_u, n = 3, laplace = T)) ## 1 ## 105.3819 Pero para la muestra de prueba: periodico_prueba &lt;- periodico[-muestra_ind] textos &lt;- periodico_prueba[1:1000] texto_prueba &lt;- paste(textos, collapse = &quot; &quot;) exp(-log_prob(texto_prueba, n_gramas_u, n = 1, laplace = T)) ## 1 ## 1006.145 exp(-log_prob(texto_prueba, n_gramas_u, n = 2, laplace = T)) ## 1 ## 320.2612 exp(-log_prob(texto_prueba, n_gramas_u, n = 3, laplace = T)) ## 1 ## 1554.325 Y vemos para este ejemplo que el mejor desempeño está dado por el modelo de bigramas, aunque no hemos hecho ningún ajuste ad-hoc del parámetro \\(\\delta\\). Como explicamos arriba, es preferible usar otros algoritmos de suavizamiento. Ejercicio Discute por qué: El modelo de bigramas se desempeña mejor que el de unigramas en la muestra de prueba. Parece que el modelo de trigramas, con nuestra colección de textos chicos, parece “sobreajustar,” y tener mal desempeño sobre la muestra de prueba. 9.7 Suavizamiento de conteos: otros métodos Como discutimos arriba, el primer problema para generalizar a frases o textos que no hemos visto, es que muchas veces no observamos en nuestros datos de entrenamiento ciertos n-gramas con probabiliidad relativamente alta. Arriba propusimos el suavizamiento de Laplace, que es una manera burda de evitar los ceros y se basa en la idea de quitar un poco de probabilidad a los n-gramas observados y redistribuir sobre los no observados. Podemos considerar métodos mejores observando que cuando el contexto es muy grande (por ejemplo, trigramas), es fácil no encontrar 0’s en frases usuales del lenguaje (por ejemplo, encontramos “Una geometría euclideana,” “Una geometría no-euclideana,” pero nunca observamos Una geometría hiperbólica”). En este caso, podríamos bajar a usar los bigramas, y considerar solamente el bigrama “geometría hiperbólica,” que quizá tiene algunas ocurrencias en la muestra de entrenamiento. A este proceso se le llama backoff. Un método popular con mejor desempeño que Laplace es el método de descuento absoluto (da) de Kneser-Ney. Consideremos el caso de bigramas. En primer lugar, este método utiliza interpolación entre bigramas y unigramas, considerando que debemos quitar algo de masa de probabilidad a bigramas observados para distribuir en bigramas no observados. El descuento lo hacemos absoluto (por ejemplo restando \\(d=0.75\\), ver (Jurafsky and Martin 2000)) \\[P_{da}(w|a) = \\frac{N(aw) - d}{N(a)} + \\lambda(a) P(w).\\] Así que reducimos los conteos observados por una fracción \\(d\\), e interpolamos con las probabilidad de ocurrencia de unigramas. \\(\\lambda(a)\\) es tal que la suma de todas las probabilidades es igual a 1: \\(\\sum_w P(w|a) = 1\\). El lado derecho de esta ecuación toma valores positivos siempre y cuando \\(N(w)\\geq 1\\), que suponemos (vocabulario completo, quizá incluyendo unk). Esta primera parte de descuento absoluto pone masa de probabilidad sobre bigramas no vistos. Podemos mejorar si en lugar de considerar \\(P(w)\\) consideramos una cantidad más específica, como sigue: En vez de usar las probabilidades crudas de unigramas \\(P(w)\\), usamos las probabilidades de continuación de unigramas \\(P_{cont}(w)\\), pues aquí \\(w\\) aparece como continuación después de \\(a\\): \\[P_{cont}(w) = \\frac{\\sum_{z\\in V} I(N(zw) &gt;0) } { \\sum_{z, b\\in V} I(N(zb &gt; 0))}.\\] Que simplemente es la probabilidad de que \\(w\\) sea una continuación después de alguna palabra (en el denominador están todas las posibles continuaciones sobre todas las palabras). Verifica que \\(\\sum_{w\\in V} P_{cont} (w) = 1\\), de forma que es una distribución de probabilidad sobre los unigramas. La idea de este método se basa en la siguiente observación: Hay palabras como embargo que principalmente ocurren como continuación de palabras específicas (sin embargo). En general, su probabilidad de continuación es baja. Pero embargo puede tener probabilidad de unigrama alto porque sin embargo ocurre frecuentemente. En una frase como “Este es un día …,” quizá aburrido ocurre menos que embargo, pero aburrido es una continuación más apropiada, pues ocurre más como continuación de otras palabras. Si usamos las probabilidad de continuación, veríamos que \\(P_{cont}(embargo)\\) es baja, pero \\(P_{cont}(aburrido)\\) es más alta, pues aburrido ocurre más como continuación. Usaríamos entonces: \\[P_{da}(w|a) = \\frac{N(aw) - d}{N(a)} + \\lambda(a) P_{cont}(w).\\] Desempeño Cuando construimos modelos grandes, una búsqueda directa en una base de datos usual de n-gramas puede ser lenta (por ejemplo, considera cómo funcionaría el auto-complete o la corrección de ortografía). Hay varias alternativas. Se pueden filtrar los n-gramas menos frecuentes (usando las ideas de arriba para hacer backoff o interpolación con unigramas), y utilizar estructuras apropiadas para encontrar los n-gramas. Otra solución es utilizar algoritmos más simples como stupid backoff (que se usa para colecciones muy grandes de texto), que consiste en usar la probabilidad del (n-1)-grama multiplicado por una constante fija \\(\\lambda\\) si no encontramos el n-grama que buscamos (Ver (Jurafsky and Martin 2000)). Finalmente, una opción es utilizar filtros de bloom para obtener aproximaciones de los conteos. Si la frecuencia de un n-grama \\(w\\) es \\(f\\), por ejemplo, insertamos en el filtro el elemento \\((w, 2^j)\\) para toda \\(j\\) tal que \\(2^j &lt; f\\). Para estimar un conteo de un n-grama observado, simplemente checamos sucesivamente si el elemento \\((w, 2^j)\\) está en el filtro o no, hasta que encontramos una \\(k\\) tal que \\((w, 2^{k+1})\\) no está en el filtro. Nuestra estimación de la frecuencia del n-grama \\(w\\) es entonces \\(2^k \\leq f &lt; 2^{k+1}\\) 9.8 Ejemplo: Corrector de ortografía Como ejemplo del canal ruidoso y el papel que juega los modelos del lenguaje, podemos ver un sistema simple de corrección de ortografía. En nuestro caso, usaremos como \\(P(W)\\) el modelo de bigramas, y el modelo de ruido \\(P(X|W)\\) es simple: lo construimos a partir de palabras \\(p(x|w)\\), y consideramos que dada una palabra \\(w\\), es posible observar \\(x\\), donde \\(w\\) se produce como transformación de \\(x\\) aplicando: Una eliminación de un caracter Inserción de un caracter Sustitución de un caracter Transposición de dos caracteres adyacentes Daremos igual probabilidad a todas estas posiblidades, aunque en la realidad este modelo debería ser más refinado (¿cómo mejorarías este modelo?¿Con qué datos?). En el siguiente paso tendríamos que producir sugerencias de corrección. En caso de encontrar una palabra que no está en el diccionario, podemos producir palabras similares (a cierta distancia de edición), y filtrar aquellas que están en el vocabulario (ver How to write a spelling corrector). generar_candidatos &lt;- function(palabra){ caracteres &lt;- c(letters, &#39;á&#39;, &#39;é&#39;, &#39;í&#39;, &#39;ó&#39;, &#39;ú&#39;, &#39;ñ&#39;) pares &lt;- lapply(0:(nchar(palabra)), function(i){ c(str_sub(palabra, 1, i), str_sub(palabra, i+1, nchar(palabra))) }) eliminaciones &lt;- pares |&gt; map(function(x){ paste0(x[1], str_sub(x[2],2,-1))}) sustituciones &lt;- pares |&gt; map(function(x) map(caracteres, function(car){ paste0(x[1], car, str_sub(x[2], 2 ,-1)) })) |&gt; flatten() inserciones &lt;- pares |&gt; map(function(x){ map(caracteres, function(car) paste0(x[1], car, x[2])) }) |&gt; flatten() transposiciones &lt;- pares |&gt; map(function(x){ paste0(x[1], str_sub(x[2],2,2), str_sub(x[2],1,1), str_sub(x[2],3,-1)) }) c(eliminaciones, sustituciones, transposiciones, inserciones) } candidatos &lt;- generar_candidatos(&quot;solado&quot;) sprintf(&quot;Número de candidatos generados: %0.i&quot;, length(candidatos)) ## [1] &quot;Número de candidatos generados: 462&quot; print(&quot;Algunos candidatos:&quot;) ## [1] &quot;Algunos candidatos:&quot; head(candidatos) ## [[1]] ## [1] &quot;olado&quot; ## ## [[2]] ## [1] &quot;slado&quot; ## ## [[3]] ## [1] &quot;soado&quot; ## ## [[4]] ## [1] &quot;soldo&quot; ## ## [[5]] ## [1] &quot;solao&quot; ## ## [[6]] ## [1] &quot;solad&quot; sugerir &lt;- function(frase, mod_bi){ tokens &lt;- normalizar(frase) |&gt; str_split(&quot; &quot;) |&gt; first() |&gt; tail(2) candidatos &lt;- generar_candidatos(tokens[2]) candidatos_tbl &lt;- mod_bi |&gt; filter(w_n_1 == tokens[1], w_n_0 %in% c(tokens[2], candidatos)) |&gt; mutate(log_posterior = 0 + log_p) # suponemos todos las corrupciones igualmente probabiles arrange(candidatos_tbl, desc(log_posterior)) } sugerir(c(&quot;esto es un echo&quot;), mod_bi) |&gt; head() ## # A tibble: 5 × 6 ## w_n_1 w_n_0 num denom log_p log_posterior ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 un hecho 115 66008 -6.35 -6.35 ## 2 un techo 20 66008 -8.10 -8.10 ## 3 un eco 4 66008 -9.71 -9.71 ## 4 un ocho 4 66008 -9.71 -9.71 ## 5 un pecho 1 66008 -11.1 -11.1 sugerir(&quot;dice que lo hechó&quot;, mod_bi) ## # A tibble: 2 × 6 ## w_n_1 w_n_0 num denom log_p log_posterior ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lo hecho 13 26883 -7.63 -7.63 ## 2 lo echó 5 26883 -8.59 -8.59 sugerir(&quot;un pantalón asul&quot;, mod_bi) ## # A tibble: 1 × 6 ## w_n_1 w_n_0 num denom log_p log_posterior ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pantalón azul 2 48 -3.18 -3.18 Este modelo está basado en bigramas, pero podemos usar también unigramas por ejemplo para producir recomendaciones cuando no encontremos bigramas apropiados. Referencias "],["representación-de-palabras-y-word2vec.html", "10 Representación de palabras y word2vec 10.1 Modelo de red neuronal 10.2 Representación de palabras 10.3 Modelos de word2vec 10.4 Espacio de representación de palabras 10.5 Usos de representaciones distribuidas", " 10 Representación de palabras y word2vec En esta parte empezamos a ver los enfoques más modernos (redes neuronales) para construir modelos de lenguajes y resolver tareas de NLP. Se trata de modelos de lenguaje que incluyen más estructura, son más fáciles de regularizar y de ampliar si es necesario para incluir dependencias de mayor distancia. El método de conteo/suavizamiento de ngramas es simple y funciona bien para algunas tareas, pero podemos construir mejores modelos con enfoques más estructurados, y con más capacidad para aprender aspectos más complejos del lenguaje natural. Si \\(w=w_1w_2\\cdots w_N\\) es una frase, y las \\(w\\) representan palabras, recordemos que un modelo de lenguaje con dependencia de \\(n\\)-gramas consiste de las probabilidades \\[P(w_t | w_{t-1} w_{t-2} \\cdots w_{t-n+1}),\\] (n=2, bigramas, n=3 trigramas, etc.) Y vimos que tenemos problemas cuando observamos sucesiones que no vimos en el corpus de entrenamiento. Este problema se puede “parchar” utilizando técnicas de suavizamiento. Aún para colecciones de entrenamiento muy grandes tenemos que lidiar con este problema. Podemos tomar un enfoque más estructurado pensando en representaciones “distribucionales” de palabras: Asociamos a cada palabra en el vocabulario un vector numérico con \\(d\\) dimensiones, que es su representación distribuida. Expresamos la función de probabilidad como combinaciones de las representaciones vectoriales del primer paso. Aprendemos (máxima verosimiltud posiblemente regularización) simultáneamente los vectores y la manera de combinar estos vectores para producir probabilidades. La idea de este modelo es entonces subsanar la relativa escasez de datos (comparado con todos los trigramas que pueden existir) con estructura. Sabemos que esta es una buena estrategia si la estrucutura impuesta es apropiada. Una de las ideas fundamentales de este enfoque es representar a cada palabra como un vector numérico de dimensión \\(d\\). Esto se llama una representación vectorial distribuida, o también un embedding de palabras. El objeto es entonces abstraer características de palabras (mediante estas representaciones) intentando no perder mucho de su sentido original, lo que nos permite conocer palabras por su contexto, aún cuando no las hayamos observado antes. Ejemplo ¿Cómo puede funcionar este enfoque? Por ejemplo, si vemos la frase “El gato corre en el jardín,” sabemos que una frase probable debe ser también “El perro corre en el jardín,” pero quizá nunca vimos en el corpus la sucesión “El perro corre.” La idea es que como “perro” y “gato” son funcionalmente similares (aparecen en contextos similares en otros tipos de oraciones como el perro come, el gato come, el perro duerme, este es mi gato, etc.), un modelo como el de arriba daría vectores similares a “perro” y “gato,” pues aparecen en contextos similares. Entonces el modelo daría una probabilidad alta a “El perro corre en el jardín.” 10.1 Modelo de red neuronal Podemos entonces construir una red neuronal con 2 capas ocultas como sigue (segimos (Bengio et al. 2003), una de las primeras referencias en usar este enfoque). Usemos el ejemplo de trigramas: Capa de incrustación o embedding. En la primera capa oculta, tenemos un mapeo de las entradas \\(w_1,\\ldots, w_{n-1}\\) a \\(x=C(w_1),\\ldots, C(w_{n-1})\\), donde \\(C\\) es una función que mapea palabras a vectores de dimensión \\(d\\). \\(C\\) también se puede pensar como una matriz de dimensión \\(|V|\\) por \\(d\\). En la capa de entrada, \\[w_{n-2},w_{n-1} \\to x = (C(w_{n-2}), C(w_{n-1})).\\] Capa totalmente conexa. En la siguiente capa oculta tenemos una matriz de pesos \\(H\\) y la función logística (o tangente hiperbólica) \\(\\sigma (z) = \\frac{e^z}{1+e^z}\\), como en una red neuronal usual. En esta capa calculamos \\[z = \\sigma (a + Hx),\\] que resulta en un vector de tamaño \\(h\\). La capa de salida debe ser un vector de probabilidades sobre todo el vocabulario \\(|V|\\). En esta capa tenemos pesos \\(U\\) y hacemos \\[y = b + U\\sigma (z),\\] y finalmente usamos softmax para tener probabilidades que suman uno: \\[p_i = \\frac{\\exp (y_i) }{\\sum_j exp(y_j)}.\\] En el ajuste maximizamos la verosimilitud: \\[\\sum_t \\log \\hat{P}(w_{t,n}|w_{t,n-2}w_{t-n-1}) \\] La representación en la referencia (Bengio et al. 2003) es: Imagen Esta idea original ha sido explotada con éxito, aunque sigue siendo intensivo en cómputo ajustar un modelo como este. Nótese que el número de parámetros es del orden de \\(|V|(nm+h)\\), donde \\(|V|\\) es el tamaño del vocabulario (decenas o cientos de miles), \\(n\\) es 3 o 4 (trigramas, 4-gramas), \\(m\\) es el tamaño de la representacion (cientos) y \\(h\\) es el número de nodos en la segunda capa (también cientos o miles). Esto resulta en el mejor de los casos en modelos con miles de millones de parámetros. Adicionalmente, hay algunos cálculos costosos, como el softmax (donde hay que hacer una suma sobre el vocabulario completo). En el paper original se propone descenso estocástico. Ejemplo Veamos un ejemplo chico de cómo se vería el paso feed-forward de esta red. Supondremos en este ejemplo que los sesgos \\(a,b\\) son iguales a cero para simplificar los cálculos. Consideremos que el texto de entrenamiento es “El perro corre. El gato corre. El león corre. El león ruge.” En este caso, nuestro vocabulario consiste de los 8 tokens \\(&lt;s&gt;\\), el, perro, gato, león, corre, caza \\(&lt;/s&gt;\\). Consideremos un modelo con \\(d=2\\) (representaciones de palabras en 2 dimensiones), y consideramos un modelo de trigramas. Nuestra primera capa es una matriz \\(C\\) de tamaño \\(2\\times 8\\), es decir, un vector de tamaño 2 para cada palabra. Por ejemplo, podríamos tener library(tidyverse) set.seed(63) C &lt;- round(matrix(rnorm(16, 0, 0.1), 2, 8), 2) colnames(C) &lt;- c(&quot;_s_&quot;, &quot;el&quot;, &quot;perro&quot;, &quot;gato&quot;, &quot;león&quot;, &quot;corre&quot;, &quot;caza&quot;, &quot;_ss_&quot;) rownames(C) &lt;- c(&quot;d_1&quot;, &quot;d_2&quot;) C ## _s_ el perro gato león corre caza _ss_ ## d_1 0.13 0.05 0.05 0.04 -0.17 0.04 0.03 -0.02 ## d_2 -0.19 -0.19 -0.11 0.01 0.04 -0.01 0.02 0.02 En la siguiente capa consideremos que usaremos, arbitrariamente, \\(h=3\\) unidades. Como estamos considerando bigramas, necesitamos una entrada de tamaño 4 (representación de un bigrama, que son dos vectores de la matriz \\(C\\), para predecir la siguiente palabra). H &lt;- round(matrix(rnorm(12, 0, 0.1), 3, 4), 2) H ## [,1] [,2] [,3] [,4] ## [1,] -0.04 0.12 -0.09 0.18 ## [2,] 0.09 0.10 0.06 0.08 ## [3,] 0.10 -0.08 -0.07 -0.13 Y la última capa es la del vocabulario. Son entonces 8 unidades, con 3 entradas cada una. La matriz de pesos es: U &lt;- round(matrix(rnorm(24, 0, 0.1), 8, 3), 2) rownames(U) &lt;- c(&quot;_s_&quot;, &quot;el&quot;, &quot;perro&quot;, &quot;gato&quot;, &quot;león&quot;, &quot;corre&quot;, &quot;caza&quot;, &quot;_ss_&quot;) U ## [,1] [,2] [,3] ## _s_ 0.05 -0.15 -0.30 ## el 0.01 0.16 0.15 ## perro -0.14 0.10 0.05 ## gato 0.04 0.09 0.12 ## león 0.06 -0.03 0.02 ## corre -0.01 0.00 -0.02 ## caza 0.10 0.00 0.06 ## _ss_ 0.07 -0.10 0.01 Ahora consideremos cómo se calcula el objetivo con los datos de entrenamiento. El primer trigrama es (_s_, el). La primera capa entonces devuelve los dos vectores correspondientes a cada palabra (concatenado): capa_1 &lt;- c(C[, &quot;_s_&quot;], C[, &quot;el&quot;]) capa_1 ## d_1 d_2 d_1 d_2 ## 0.13 -0.19 0.05 -0.19 La siguiente capa es: sigma &lt;- function(z){ 1 / (1 + exp(-z))} capa_2 &lt;- sigma(H %*% capa_1) capa_2 ## [,1] ## [1,] 0.4833312 ## [2,] 0.4951252 ## [3,] 0.5123475 Y la capa final da y &lt;- U %*% capa_2 y ## [,1] ## _s_ -0.203806461 ## el 0.160905460 ## perro 0.007463525 ## gato 0.125376210 ## león 0.024393066 ## corre -0.015080262 ## caza 0.079073967 ## _ss_ -0.010555858 Y aplicamos softmax para encontrar las probabilidades p &lt;- exp(y)/sum(exp(y)) |&gt; as.numeric() p ## [,1] ## _s_ 0.09931122 ## el 0.14301799 ## perro 0.12267376 ## gato 0.13802588 ## león 0.12476825 ## corre 0.11993917 ## caza 0.13178067 ## _ss_ 0.12048306 Y la probabilidad es entonces p_1 &lt;- p[&quot;perro&quot;, 1] p_1 ## perro ## 0.1226738 Cuya log probabilidad es log(p_1) ## perro ## -2.098227 Ahora seguimos con el siguiente trigrama, que es “(perro, corre).” Necesitamos calcular la probabilidad de corre dado el contexto “el perro.” Repetimos nuestro cálculo: capa_1 &lt;- c(C[, &quot;el&quot;], C[, &quot;perro&quot;]) capa_1 ## d_1 d_2 d_1 d_2 ## 0.05 -0.19 0.05 -0.11 capa_2 &lt;- sigma(H %*% capa_1) capa_2 ## [,1] ## [1,] 0.4877275 ## [2,] 0.4949252 ## [3,] 0.5077494 y &lt;- U %*% capa_2 y ## [,1] ## _s_ -0.202177217 ## el 0.160227709 ## perro 0.006598141 ## gato 0.124982290 ## león 0.024570880 ## corre -0.015032262 ## caza 0.079237709 ## _ss_ -0.010274101 p &lt;- exp(y)/sum(exp(y)) |&gt; as.numeric() p ## [,1] ## _s_ 0.09947434 ## el 0.14292280 ## perro 0.12256912 ## gato 0.13797317 ## león 0.12479193 ## corre 0.11994636 ## caza 0.13180383 ## _ss_ 0.12051845 Y la probabilidad es entonces p_2 &lt;- p[&quot;corre&quot;, 1] log(p_2) ## corre ## -2.120711 Sumando, la log probabilidad es: log(p_1) + log(p_2) ## perro ## -4.218937 y continuamos con los siguientes trigramas del texto de entrenamiento. Creamos una función feed_fow_p &lt;- function(trigrama, C, H, U){ trigrama &lt;- strsplit(trigrama, &quot; &quot;, fixed = TRUE)[[1]] capa_1 &lt;- c(C[, trigrama[1]], C[, trigrama[2]]) capa_2 &lt;- sigma(H %*% capa_1) y &lt;- U %*% capa_2 p &lt;- exp(y)/sum(exp(y)) |&gt; as.numeric() p } feed_fow_dev &lt;- function(trigrama, C, H, U) { p &lt;- feed_fow_p(trigrama, C, H, U) trigrama_s &lt;- strsplit(trigrama, &quot; &quot;, fixed = TRUE)[[1]] log(p)[trigrama_s[3], 1] } Y ahora aplicamos a todos los textos: texto_entrena &lt;- c(&quot;_s_ el perro corre _ss_&quot;, &quot; _s_ el gato corre _ss_&quot;, &quot; _s_ el león corre _ss_&quot;, &quot;_s_ el león caza _ss_&quot;, &quot;_s_ el gato caza _ss_&quot;) entrena_trigramas &lt;- map(texto_entrena, ~tokenizers::tokenize_ngrams(.x, n = 3)[[1]]) |&gt; flatten() |&gt; unlist() entrena_trigramas ## [1] &quot;_s_ el perro&quot; &quot;el perro corre&quot; &quot;perro corre _ss_&quot; &quot;_s_ el gato&quot; ## [5] &quot;el gato corre&quot; &quot;gato corre _ss_&quot; &quot;_s_ el león&quot; &quot;el león corre&quot; ## [9] &quot;león corre _ss_&quot; &quot;_s_ el león&quot; &quot;el león caza&quot; &quot;león caza _ss_&quot; ## [13] &quot;_s_ el gato&quot; &quot;el gato caza&quot; &quot;gato caza _ss_&quot; log_p &lt;- sapply(entrena_trigramas, function(x) feed_fow_dev(x, C, H, U)) sum(log_p) ## [1] -31.21475 Ahora piensa como harías más grande esta verosimilitud. Observa que “perro,” “gato” y “león”” están comunmente seguidos de “corre.” Esto implica que nos convendría que hubiera cierta similitud entre los vectores de estas tres palabras, por ejemplo: C_1 &lt;- C indices &lt;- colnames(C) %in% c(&quot;perro&quot;, &quot;gato&quot;, &quot;león&quot;) C_1[1, indices] &lt;- 3.0 C_1[1, !indices] &lt;- -1.0 C_1 ## _s_ el perro gato león corre caza _ss_ ## d_1 -1.00 -1.00 3.00 3.00 3.00 -1.00 -1.00 -1.00 ## d_2 -0.19 -0.19 -0.11 0.01 0.04 -0.01 0.02 0.02 La siguiente capa queremos que extraiga el concepto “animal” en la palabra anterior, o algo similar, así que podríamos poner en la unidad 1: H_1 &lt;- H H_1[1, ] &lt;- c(0, 0, 5, 0) H_1 ## [,1] [,2] [,3] [,4] ## [1,] 0.00 0.00 5.00 0.00 ## [2,] 0.09 0.10 0.06 0.08 ## [3,] 0.10 -0.08 -0.07 -0.13 Nótese que la unidad 1 de la segunda capa se activa cuando la primera componente de la palabra anterior es alta. En la última capa, podríamos entonces poner U_1 &lt;- U U_1[&quot;corre&quot;, ] &lt;- c(4.0, -2, -2) U_1[&quot;caza&quot;, ] &lt;- c(4.2, -2, -2) U_1 ## [,1] [,2] [,3] ## _s_ 0.05 -0.15 -0.30 ## el 0.01 0.16 0.15 ## perro -0.14 0.10 0.05 ## gato 0.04 0.09 0.12 ## león 0.06 -0.03 0.02 ## corre 4.00 -2.00 -2.00 ## caza 4.20 -2.00 -2.00 ## _ss_ 0.07 -0.10 0.01 que captura cuando la primera unidad se activa. Ahora el cálculo completo es: log_p &lt;- sapply(entrena_trigramas, function(x) feed_fow_dev(x, C_1, H_1, U_1)) sum(log_p) ## [1] -23.53883 Y logramos aumentar la verosimilitud considerablemente. Compara las probabilidades: feed_fow_p(&quot;el perro&quot;, C, H, U) ## [,1] ## _s_ 0.09947434 ## el 0.14292280 ## perro 0.12256912 ## gato 0.13797317 ## león 0.12479193 ## corre 0.11994636 ## caza 0.13180383 ## _ss_ 0.12051845 feed_fow_p(&quot;el perro&quot;, C_1, H_1, U_1) ## [,1] ## _s_ 0.03493901 ## el 0.04780222 ## perro 0.03821035 ## gato 0.04690264 ## león 0.04308502 ## corre 0.33639351 ## caza 0.41087194 ## _ss_ 0.04179531 feed_fow_p(&quot;el gato&quot;, C, H, U) ## [,1] ## _s_ 0.09957218 ## el 0.14289131 ## perro 0.12246787 ## gato 0.13795972 ## león 0.12480659 ## corre 0.11993921 ## caza 0.13183822 ## _ss_ 0.12052489 feed_fow_p(&quot;el gato&quot;, C_1, H_1, U_1) ## [,1] ## _s_ 0.03489252 ## el 0.04769205 ## perro 0.03813136 ## gato 0.04679205 ## león 0.04298749 ## corre 0.33663831 ## caza 0.41117094 ## _ss_ 0.04169529 Observación: a partir de este principio, es posible construir arquitecturas más refinadas que tomen en cuenta, por ejemplo, relaciones más lejanas entre partes de oraciones (no solo el contexto del n-grama), ver por ejemplo el capítulo 10 del libro de Deep Learning de Goodfellow, Bengio y Courville. Abajo exploramos una parte fundamental de estos modelos: representaciones de palabras, y modelos relativamente simples para obtener estas representaciones. 10.2 Representación de palabras Un aspecto interesante de el modelo de arriba es que nos da una representación vectorial de las palabras, en la forma de los parámetros ajustados de la matriz \\(C\\). Esta se puede entender como una descripción numérica de cómo funciona una palabra en el contexto de su n-grama. Por ejemplo, deberíamos encontrar que palabras como “perro” y “gato” tienen representaciones similares. La razón es que cuando aparecen, las probabilidades sobre las palabras siguientes deberían ser similares, pues estas son dos palabras que se pueden usar en muchos contextos compartidos. También podríamos encontrar que palabras como perro, gato, águila, león, etc. tienen partes o entradas similares en sus vectores de representación, que es la parte que hace que funcionen como “animal mamífero” dentro de frases. Veremos que hay más razones por las que es interesante esta representación. 10.3 Modelos de word2vec Si lo que principalmente nos interesa es obtener la representación vectorial de palabras, más recientemente se descubrió que es posible simplificar considerablemente el modelo de arriba para poder entrenarlo mucho más rápido, y obtener una representación que en muchas tareas se desempeña bien ((Mikolov et al. 2013)). Hay dos ideas básicas que se pueden usar para reducir la complejidad del entrenamiento (ver más en (Goodfellow, Bengio, and Courville 2016) y (Mikolov et al. 2013): Eliminar la segunda capa oculta: modelo de bag-of-words continuo y modelo de skip-gram. Cambiar la función objetivo (minimizar devianza/maximizar verosimilitud) por una más simple, mediante un truco que se llama negative sampling. Como ya no es de interés central predecir la siguiente palabra a partir de las anteriores, en estos modelos intentamos predecir la palabra central a partir de las que están alrededor. 10.3.1 Arquitectura continuous bag-of-words La entrada es igual que en el modelo completo. En primer lugar, simplificamos la segunda capa oculta pondiendo en \\(z\\) el promedio de los vectores \\(C(w_{n-2}), C(w_{n-1})\\). La última capa la dejamos igual por el momento: Imagen El modelo se llama bag-of-words porque todas las entradas de la primera capa oculta contribuyen de la misma manera en la salida, independientemente del orden. Aunque esto no suena como buena idea para construir un modelo de lenguaje, veremos que resulta en una representación adecuada para algunos problemas. En la primera capa oculta, tenemos un mapeo de las entradas \\(w_1,\\ldots, w_{n-1}\\) a \\(x=C(w_1),\\ldots, C(w_{n-1})\\), donde \\(C\\) es una función que mapea palabras a vectores de dimensión \\(d\\). \\(C\\) también se puede pensar como una matriz de dimensión \\(|V|\\) por \\(d\\). En la capa de entrada, \\[w_{n-2},w_{n-1} \\to x = (C(w_{n-2}), C(w_{n-1})).\\] En la siguiente “capa” oculta simplemente sumamos las entradas de \\(x\\). Aquí nótese que realmente no hay parámetros. Finalmente, la capa de salida debe ser un vector de probabilidades sobre todo el vocabulario \\(|V|\\). En esta capa tenemos pesos \\(U\\) y hacemos \\[y = b + U\\sigma (z),\\] y finalmente usamos softmax para tener probabilidades que suman uno: \\[p_i = \\frac{\\exp (y_i) }{\\sum_j exp(y_j)}.\\] En el ajuste maximizamos la verosimilitud sobre el corpus. Por ejemplo, para una frase, su log verosimilitud es: \\[\\sum_t \\log \\hat{P}(w_{t,n}|w_{t,n+1} \\cdots w_{t-n-1}) \\] 10.3.2 Arquitectura skip-grams Otro modelo simplificado, con más complejidad computacional pero mejores resultados (ver (Mikolov et al. 2013)) que el bag-of-words, es el modelo de skip-grams. En este caso, dada cada palabra que encontramos, intentamos predecir un número fijo de las palabras anteriores y palabras posteriores (el contexto es una vecindad de la palabra). Imagen La función objetivo se defina ahora (simplificando) como suma sobre \\(t\\): \\[-\\sum_t \\sum_{ -2\\leq j \\leq 2, j\\neq 0} \\log P(w_{t-j} | w_t)\\] (no tomamos en cuenta dónde aparece exactamente \\(w_{t-j}\\) en relación a \\(w_t\\), simplemente consideramos que está en su contexto), donde \\[\\log P(w_{t-j}|w_t) = u_{t-j}^tC(w_n) - \\log\\sum_k \\exp{u_{k}^tC(w_n)}\\] Todavía se propone una simplificación adicional que resulta ser efectiva: 10.3.3 Muestreo negativo La siguiente simplificación consiste en cambiar la función objetivo. En word2vec puede usarse “muestreo negativo.” Para empezar, la función objetivo original (para contexto de una sola palabra) es \\[E = -\\log \\hat{P}(w_{a}|w_{n}) = -y_{w_a} + \\log\\sum_j \\exp(y_j),\\] donde las \\(y_i\\) son las salidas de la penúltima capa. La dificultad está en el segundo término, que es sobre todo el vocabulario en incluye todos los parámetros del modelo (hay que calcular las parciales de \\(y_j\\)’s sobre cada una de las palabras del vocabulario). La idea del muestreo negativo es que si \\(w_a\\) está en el contexto de \\(w_{n}\\), tomamos una muestra de \\(k\\) palabras \\(v_1,\\ldots v_k\\) al azar (2-50, dependiendo del tamaño de la colección), y creamos \\(k\\) “contextos falsos” \\(v_j w_{n}\\), \\(j=1\\ldots,k\\). Minimizamos en lugar de la observación de arriba \\[E = -\\log\\sigma(y_{w_a}) + \\sum_{j=1}^k \\log\\sigma(y_j),\\] en donde queremos maximizar la probabilidad de que ocurra \\(w_a\\) vs. la probabilidad de que ocurra alguna de las \\(v_j\\). Es decir, solo buscamos optimizar parámetros para separar lo mejor que podamos la observación de \\(k\\) observaciones falsas, lo cual implica que tenemos que mover un número relativamente chico de parámetros (en lugar de todos los parámetros de todas las palabras del vocabulario). Las palabras “falsas” se escogen según una probabilidad ajustada de unigramas (se observó empíricamente mejor desempeño cuando escogemos cada palabra con probabilidad proporcional a \\(P(w)^{3/4}\\), en lugar de \\(P(w)\\), ver (Mikolov et al. 2013)). Ejemplo if(!require(wordVectors)){ devtools::install_github(&quot;bmschmidt/wordVectors&quot;) } ## pillar (1.6.4 -&gt; 1.7.0) [CRAN] ## magrittr (2.0.1 -&gt; 2.0.3) [CRAN] ## fansi (0.5.0 -&gt; 1.0.3) [CRAN] ## withr (2.4.3 -&gt; 2.5.0) [CRAN] ## tidyselect (1.1.1 -&gt; 1.1.2) [CRAN] ## vctrs (0.3.8 -&gt; 0.4.1) [CRAN] ## tzdb (0.2.0 -&gt; 0.3.0) [CRAN] ## tibble (3.1.6 -&gt; 3.1.7) [CRAN] ## rlang (0.4.12 -&gt; 1.0.2) [CRAN] ## glue (1.6.0 -&gt; 1.6.2) [CRAN] ## cli (3.1.0 -&gt; 3.3.0) [CRAN] ## crayon (1.4.2 -&gt; 1.5.1) [CRAN] ## clipr (0.7.1 -&gt; 0.8.0) [CRAN] ## readr (2.1.1 -&gt; 2.1.2) [CRAN] ## * checking for file ‘/tmp/Rtmp70So7D/remotes483e7e08e684/bmschmidt-wordVectors-7f1914c/DESCRIPTION’ ... OK ## * preparing ‘wordVectors’: ## * checking DESCRIPTION meta-information ... OK ## * cleaning src ## * checking for LF line-endings in source and make files and shell scripts ## * checking for empty or unneeded directories ## * building ‘wordVectors_2.0.tar.gz’ library(wordVectors) library(tidyverse) ruta &lt;- &quot;../datos/noticias/ES_Newspapers.txt&quot; if(!file.exists(ruta)){ periodico &lt;- read_lines(file= &quot;https://es-noticias.s3.amazonaws.com/Es_Newspapers.txt&quot;, progress = FALSE) write_lines(periodico, ruta) } else { periodico &lt;- read_lines(file= ruta, progress = FALSE) } normalizar &lt;- function(texto, vocab = NULL){ # minúsculas texto &lt;- tolower(texto) # varios ajustes texto &lt;- gsub(&quot;\\\\s+&quot;, &quot; &quot;, texto) texto &lt;- gsub(&quot;\\\\.[^0-9]&quot;, &quot; _punto_ &quot;, texto) texto &lt;- gsub(&quot; _s_ $&quot;, &quot;&quot;, texto) texto &lt;- gsub(&quot;\\\\.&quot;, &quot; _punto_ &quot;, texto) texto &lt;- gsub(&quot;[«»¡!¿?-]&quot;, &quot;&quot;, texto) texto &lt;- gsub(&quot;;&quot;, &quot; _punto_coma_ &quot;, texto) texto &lt;- gsub(&quot;\\\\:&quot;, &quot; _dos_puntos_ &quot;, texto) texto &lt;- gsub(&quot;\\\\,[^0-9]&quot;, &quot; _coma_ &quot;, texto) texto &lt;- gsub(&quot;\\\\s+&quot;, &quot; &quot;, texto) texto } periodico_df &lt;- tibble(txt = periodico) |&gt; mutate(id = row_number()) |&gt; mutate(txt = normalizar(txt)) Construimos un modelo con vectores de palabras de tamaño 100, skip-grams de tamaño 4, y ajustamos con muestreo negativo de tamaño 20: if(!file.exists(&#39;./salidas/noticias_w2v.txt&#39;)){ tmp &lt;- tempfile() # tokenización write_lines(periodico_df$txt, tmp) prep &lt;- prep_word2vec(tmp, destination = &#39;./salidas/noticias_w2v.txt&#39;, bundle_ngrams = 2) } ## Beginning tokenization to text file at ./salidas/noticias_w2v.txt ## Prepping /tmp/Rtmp70So7D/file483e6918dbe ## Starting training using file ./salidas/noticias_w2v.txt ## Words processed: 100K Vocab size: 73K Words processed: 200K Vocab size: 124K Words processed: 300K Vocab size: 168K Words processed: 400K Vocab size: 209K Words processed: 500K Vocab size: 247K Words processed: 600K Vocab size: 281K Words processed: 700K Vocab size: 314K Words processed: 800K Vocab size: 346K Words processed: 900K Vocab size: 376K Words processed: 1000K Vocab size: 406K Words processed: 1100K Vocab size: 434K Words processed: 1200K Vocab size: 462K Words processed: 1300K Vocab size: 489K Words processed: 1400K Vocab size: 515K Words processed: 1500K Vocab size: 540K Words processed: 1600K Vocab size: 565K Words processed: 1700K Vocab size: 590K Words processed: 1800K Vocab size: 613K Words processed: 1900K Vocab size: 637K Words processed: 2000K Vocab size: 661K Words processed: 2100K Vocab size: 684K Words processed: 2200K Vocab size: 706K Words processed: 2300K Vocab size: 729K Words processed: 2400K Vocab size: 750K Words processed: 2500K Vocab size: 771K Words processed: 2600K Vocab size: 792K Words processed: 2700K Vocab size: 813K Words processed: 2800K Vocab size: 834K Words processed: 2900K Vocab size: 854K Words processed: 3000K Vocab size: 873K Words processed: 3100K Vocab size: 893K Words processed: 3200K Vocab size: 913K Words processed: 3300K Vocab size: 932K Words processed: 3400K Vocab size: 951K Words processed: 3500K Vocab size: 970K Words processed: 3600K Vocab size: 989K Words processed: 3700K Vocab size: 1007K Words processed: 3800K Vocab size: 1026K Words processed: 3900K Vocab size: 1044K Words processed: 4000K Vocab size: 1062K Words processed: 4100K Vocab size: 1080K Words processed: 4200K Vocab size: 1098K Words processed: 4300K Vocab size: 1115K Words processed: 4400K Vocab size: 1132K Words processed: 4500K Vocab size: 1150K Words processed: 4600K Vocab size: 1167K Words processed: 4700K Vocab size: 1184K Words processed: 4800K Vocab size: 1201K Words processed: 4900K Vocab size: 1218K Words processed: 5000K Vocab size: 1235K Words processed: 5100K Vocab size: 1252K Words processed: 5200K Vocab size: 1268K Words processed: 5300K Vocab size: 1285K Words processed: 5400K Vocab size: 1301K Words processed: 5500K Vocab size: 1317K Words processed: 5600K Vocab size: 1333K Words processed: 5700K Vocab size: 1349K Words processed: 5800K Vocab size: 1364K Words processed: 5900K Vocab size: 1380K Words processed: 6000K Vocab size: 1395K Words processed: 6100K Vocab size: 1411K Words processed: 6200K Vocab size: 1426K Words processed: 6300K Vocab size: 1441K Words processed: 6400K Vocab size: 1456K Words processed: 6500K Vocab size: 1471K Words processed: 6600K Vocab size: 1486K Words processed: 6700K Vocab size: 1501K Words processed: 6800K Vocab size: 1516K Words processed: 6900K Vocab size: 1530K Words processed: 7000K Vocab size: 1545K Words processed: 7100K Vocab size: 1560K Words processed: 7200K Vocab size: 1575K Words processed: 7300K Vocab size: 1589K Words processed: 7400K Vocab size: 1604K Words processed: 7500K Vocab size: 1618K Words processed: 7600K Vocab size: 1632K Words processed: 7700K Vocab size: 1646K Words processed: 7800K Vocab size: 1661K Words processed: 7900K Vocab size: 1675K Words processed: 8000K Vocab size: 1689K Words processed: 8100K Vocab size: 1703K Words processed: 8200K Vocab size: 1717K Words processed: 8300K Vocab size: 1731K Words processed: 8400K Vocab size: 1744K Words processed: 8500K Vocab size: 1758K Words processed: 8600K Vocab size: 1771K Words processed: 8700K Vocab size: 1785K Words processed: 8800K Vocab size: 1798K Words processed: 8900K Vocab size: 1812K Words processed: 9000K Vocab size: 1825K Words processed: 9100K Vocab size: 1839K Words processed: 9200K Vocab size: 1852K Words processed: 9300K Vocab size: 1865K Words processed: 9400K Vocab size: 1878K Words processed: 9500K Vocab size: 1892K Words processed: 9600K Vocab size: 1905K Words processed: 9700K Vocab size: 1918K Words processed: 9800K Vocab size: 1931K Words processed: 9900K Vocab size: 1943K Words processed: 10000K Vocab size: 1956K Words processed: 10100K Vocab size: 1969K Words processed: 10200K Vocab size: 1982K Words processed: 10300K Vocab size: 1995K Words processed: 10400K Vocab size: 2008K Words processed: 10500K Vocab size: 2020K Words processed: 10600K Vocab size: 2033K Words processed: 10700K Vocab size: 2045K Words processed: 10800K Vocab size: 2057K Words processed: 10900K Vocab size: 2070K Words processed: 11000K Vocab size: 2082K Words processed: 11100K Vocab size: 2094K Words processed: 11200K Vocab size: 2107K Words processed: 11300K Vocab size: 2119K Words processed: 11400K Vocab size: 2131K Words processed: 11500K Vocab size: 2143K Words processed: 11600K Vocab size: 2156K Words processed: 11700K Vocab size: 2168K Words processed: 11800K Vocab size: 2180K Words processed: 11900K Vocab size: 2192K Words processed: 12000K Vocab size: 2204K Words processed: 12100K Vocab size: 2216K Words processed: 12200K Vocab size: 2227K Words processed: 12300K Vocab size: 2239K Words processed: 12400K Vocab size: 2251K Words processed: 12500K Vocab size: 2263K Words processed: 12600K Vocab size: 2274K Words processed: 12700K Vocab size: 2286K Words processed: 12800K Vocab size: 2298K Words processed: 12900K Vocab size: 2310K Words processed: 13000K Vocab size: 2321K Words processed: 13100K Vocab size: 2333K Words processed: 13200K Vocab size: 2344K Words processed: 13300K Vocab size: 2355K Words processed: 13400K Vocab size: 2367K Words processed: 13500K Vocab size: 2378K Words processed: 13600K Vocab size: 2390K Words processed: 13700K Vocab size: 2401K Words processed: 13800K Vocab size: 2412K Words processed: 13900K Vocab size: 2424K Words processed: 14000K Vocab size: 2435K Words processed: 14100K Vocab size: 2446K Words processed: 14200K Vocab size: 2457K Words processed: 14300K Vocab size: 2469K Words processed: 14400K Vocab size: 2479K Words processed: 14500K Vocab size: 2490K Words processed: 14600K Vocab size: 2502K Words processed: 14700K Vocab size: 2513K Words processed: 14800K Vocab size: 2524K Words processed: 14900K Vocab size: 2535K Words processed: 15000K Vocab size: 2546K Words processed: 15100K Vocab size: 2557K Words processed: 15200K Vocab size: 2568K Words processed: 15300K Vocab size: 2579K Words processed: 15400K Vocab size: 2590K Words processed: 15500K Vocab size: 2600K Words processed: 15600K Vocab size: 2611K Words processed: 15700K Vocab size: 2622K Words processed: 15800K Vocab size: 2633K Words processed: 15900K Vocab size: 2644K Words processed: 16000K Vocab size: 2654K Words processed: 16100K Vocab size: 2665K Words processed: 16200K Vocab size: 2676K Words processed: 16300K Vocab size: 2687K Words processed: 16400K Vocab size: 2697K Words processed: 16500K Vocab size: 2708K Words processed: 16600K Vocab size: 2719K Words processed: 16700K Vocab size: 2729K Words processed: 16800K Vocab size: 2739K Words processed: 16900K Vocab size: 2750K Words processed: 17000K Vocab size: 2760K Words processed: 17100K Vocab size: 2771K Words processed: 17200K Vocab size: 2781K Words processed: 17300K Vocab size: 2792K Words processed: 17400K Vocab size: 2802K Words processed: 17500K Vocab size: 2813K Words processed: 17600K Vocab size: 2823K Words processed: 17700K Vocab size: 2833K Words processed: 17800K Vocab size: 2844K Words processed: 17900K Vocab size: 2854K Words processed: 18000K Vocab size: 2864K Words processed: 18100K Vocab size: 2875K Words processed: 18200K Vocab size: 2885K Words processed: 18300K Vocab size: 2896K Words processed: 18400K Vocab size: 2906K Words processed: 18500K Vocab size: 2916K Words processed: 18600K Vocab size: 2926K Words processed: 18700K Vocab size: 2936K Words processed: 18800K Vocab size: 2946K Words processed: 18900K Vocab size: 2956K Words processed: 19000K Vocab size: 2966K Words processed: 19100K Vocab size: 2975K Words processed: 19200K Vocab size: 2986K Words processed: 19300K Vocab size: 2995K Words processed: 19400K Vocab size: 3005K Words processed: 19500K Vocab size: 3015K Words processed: 19600K Vocab size: 3025K Words processed: 19700K Vocab size: 3035K Words processed: 19800K Vocab size: 3045K Words processed: 19900K Vocab size: 3055K Words processed: 20000K Vocab size: 3065K Words processed: 20100K Vocab size: 3075K Words processed: 20200K Vocab size: 3085K Words processed: 20300K Vocab size: 3095K Words processed: 20400K Vocab size: 3104K Words processed: 20500K Vocab size: 3114K Words processed: 20600K Vocab size: 3124K Words processed: 20700K Vocab size: 3133K Words processed: 20800K Vocab size: 3143K Words processed: 20900K Vocab size: 3153K Words processed: 21000K Vocab size: 3162K ## Vocab size (unigrams + bigrams): 1814048 ## Words in train file: 21011278 ## Words written: 100K Words written: 200K Words written: 300K Words written: 400K Words written: 500K Words written: 600K Words written: 700K Words written: 800K Words written: 900K Words written: 1000K Words written: 1100K Words written: 1200K Words written: 1300K Words written: 1400K Words written: 1500K Words written: 1600K Words written: 1700K Words written: 1800K Words written: 1900K Words written: 2000K Words written: 2100K Words written: 2200K Words written: 2300K Words written: 2400K Words written: 2500K Words written: 2600K Words written: 2700K Words written: 2800K Words written: 2900K Words written: 3000K Words written: 3100K Words written: 3200K Words written: 3300K Words written: 3400K Words written: 3500K Words written: 3600K Words written: 3700K Words written: 3800K Words written: 3900K Words written: 4000K Words written: 4100K Words written: 4200K Words written: 4300K Words written: 4400K Words written: 4500K Words written: 4600K Words written: 4700K Words written: 4800K Words written: 4900K Words written: 5000K Words written: 5100K Words written: 5200K Words written: 5300K Words written: 5400K Words written: 5500K Words written: 5600K Words written: 5700K Words written: 5800K Words written: 5900K Words written: 6000K Words written: 6100K Words written: 6200K Words written: 6300K Words written: 6400K Words written: 6500K Words written: 6600K Words written: 6700K Words written: 6800K Words written: 6900K Words written: 7000K Words written: 7100K Words written: 7200K Words written: 7300K Words written: 7400K Words written: 7500K Words written: 7600K Words written: 7700K Words written: 7800K Words written: 7900K Words written: 8000K Words written: 8100K Words written: 8200K Words written: 8300K Words written: 8400K Words written: 8500K Words written: 8600K Words written: 8700K Words written: 8800K Words written: 8900K Words written: 9000K Words written: 9100K Words written: 9200K Words written: 9300K Words written: 9400K Words written: 9500K Words written: 9600K Words written: 9700K Words written: 9800K Words written: 9900K Words written: 10000K Words written: 10100K Words written: 10200K Words written: 10300K Words written: 10400K Words written: 10500K Words written: 10600K Words written: 10700K Words written: 10800K Words written: 10900K Words written: 11000K Words written: 11100K Words written: 11200K Words written: 11300K Words written: 11400K Words written: 11500K Words written: 11600K Words written: 11700K Words written: 11800K Words written: 11900K Words written: 12000K Words written: 12100K Words written: 12200K Words written: 12300K Words written: 12400K Words written: 12500K Words written: 12600K Words written: 12700K Words written: 12800K Words written: 12900K Words written: 13000K Words written: 13100K Words written: 13200K Words written: 13300K Words written: 13400K Words written: 13500K Words written: 13600K Words written: 13700K Words written: 13800K Words written: 13900K Words written: 14000K Words written: 14100K Words written: 14200K Words written: 14300K Words written: 14400K Words written: 14500K Words written: 14600K Words written: 14700K Words written: 14800K Words written: 14900K Words written: 15000K Words written: 15100K Words written: 15200K Words written: 15300K Words written: 15400K Words written: 15500K Words written: 15600K Words written: 15700K Words written: 15800K Words written: 15900K Words written: 16000K Words written: 16100K Words written: 16200K Words written: 16300K Words written: 16400K Words written: 16500K Words written: 16600K Words written: 16700K Words written: 16800K Words written: 16900K Words written: 17000K Words written: 17100K Words written: 17200K Words written: 17300K Words written: 17400K Words written: 17500K Words written: 17600K Words written: 17700K Words written: 17800K Words written: 17900K Words written: 18000K Words written: 18100K Words written: 18200K Words written: 18300K Words written: 18400K Words written: 18500K Words written: 18600K Words written: 18700K Words written: 18800K Words written: 18900K Words written: 19000K Words written: 19100K Words written: 19200K Words written: 19300K Words written: 19400K Words written: 19500K Words written: 19600K Words written: 19700K Words written: 19800K Words written: 19900K Words written: 20000K Words written: 20100K Words written: 20200K Words written: 20300K Words written: 20400K Words written: 20500K Words written: 20600K Words written: 20700K Words written: 20800K Words written: 20900K Words written: 21000K if (!file.exists(&quot;./salidas/noticias_vectors.bin&quot;)) { modelo &lt;- train_word2vec(&quot;./salidas/noticias_w2v.txt&quot;, &quot;./salidas/noticias_vectors.bin&quot;, vectors = 100, threads = 8, window = 4, cbow = 0, iter = 20, negative_samples = 20, min_count = 10) } else { modelo &lt;- read.vectors(&quot;./salidas/noticias_vectors.bin&quot;) } El resultado son los vectores aprendidos de las palabras, por ejemplo vector_gol &lt;- modelo[[&quot;gol&quot;]] |&gt; as.numeric() vector_gol ## [1] -0.389627248 0.048135430 0.501164794 -0.035961468 0.291238993 ## [6] 0.642733335 -0.386596769 0.281559557 -0.199183896 -0.554564893 ## [11] 0.451201737 0.495587140 -0.525584400 0.166191801 -0.180947676 ## [16] 0.034590811 0.731496751 0.259901792 -0.201457486 -0.308042079 ## [21] -0.177875623 -0.220428273 0.408699900 0.001920983 0.011449666 ## [26] -0.718980432 0.153631359 -0.049470965 0.981541216 0.082757361 ## [31] -0.331263602 0.458369821 -0.429754555 0.128275126 -0.421742797 ## [36] 0.596242130 -0.093633644 0.066455603 -0.016802812 -0.301688135 ## [41] 0.079358041 0.446704596 -0.244078919 -0.137954682 0.695054173 ## [46] 0.335903019 0.216709450 0.604890466 -0.538004100 -0.291783333 ## [51] -0.579949379 -0.048889056 0.324184030 -0.055591993 -0.012452535 ## [56] -0.200338170 0.254620761 0.082836255 0.389545202 -0.185363784 ## [61] -0.021011911 0.307440221 0.415608138 0.248776823 -0.139897019 ## [66] 0.008641024 0.235776618 0.324411124 -0.171800703 0.131596789 ## [71] -0.163520932 0.370538741 -0.134094939 -0.193797469 -0.543500543 ## [76] 0.312639445 -0.172534481 -0.115350038 -0.293528855 -0.534602344 ## [81] 0.515545666 0.708557248 0.444676250 -0.054800753 0.388787180 ## [86] 0.483029991 0.281573176 0.434132993 0.441057146 -0.347387016 ## [91] -0.174339339 0.060069371 -0.034651209 0.407196820 0.661161661 ## [96] 0.261399239 -0.089392163 -0.043052837 -0.539683878 0.105241157 10.4 Espacio de representación de palabras Como discutimos arriba, palabras que se usan en contextos similares por su significado o por su función (por ejemplo, “perro” y “gato”“) deben tener representaciones similares, pues su contexto tiende a ser similar. La similitud que usamos el similitud coseno. Podemos verificar con nuestro ejemplo: ejemplos &lt;- modelo |&gt; closest_to(&quot;gol&quot;, n = 5) ejemplos ## word similarity to &quot;gol&quot; ## 1 gol 1.0000000 ## 2 golazo 0.8252912 ## 3 segundo_gol 0.7978025 ## 4 penalti 0.7764458 ## 5 potente_disparo 0.7755037 También podríamos calcular manualmente: Que también podemos calcular como: vector_penalti &lt;- modelo[[&quot;penalti&quot;]] |&gt; as.numeric() cosineSimilarity(modelo[[&quot;gol&quot;]], modelo[[&quot;penalti&quot;]]) ## [,1] ## [1,] 0.7764458 O directamente: norma &lt;- function(x) sqrt(sum(x^2)) sum(vector_gol * vector_penalti) / (norma(vector_gol) * norma(vector_penalti)) ## [1] 0.7764458 Geometría en el espacio de representaciones Ahora consideremos cómo se distribuyen las palabras en este espacio, y si existe estructura geométrica en este espacio que tenga información acerca del lenguaje. Consideremos primero el caso de plurales de sustantivos. Como el contexto de los plurales es distinto de los singulares, nuestro modelo debería poder capturar en los vectores su diferencia. Examinamos entonces cómo son geométricamente diferentes las representaciones de plurales vs singulares Si encontramos un patrón reconocible, podemos utilizar este patrón, por ejemplo, para encontrar la versión plural de una palabra singular, sin usar ninguna regla del lenguaje. Una de las relaciones geométricas más simples es la adición de vectores. Por ejemplo, extraemos la diferencia entre gol y goles: ejemplos &lt;- modelo |&gt; closest_to(&quot;dos&quot;, n = 15) ejemplos ## word similarity to &quot;dos&quot; ## 1 dos 1.0000000 ## 2 tres 0.9666800 ## 3 cuatro 0.9527403 ## 4 cinco 0.9205234 ## 5 siete 0.9024807 ## 6 seis 0.8977667 ## 7 ocho 0.8879153 ## 8 nueve 0.8550580 ## 9 trece 0.8514542 ## 10 catorce 0.8321762 ## 11 diez 0.8133345 ## 12 quince 0.8102052 ## 13 doce 0.8085939 ## 14 once 0.8033385 ## 15 veinte 0.7814970 ejemplos &lt;- modelo |&gt; closest_to(c(&quot;lluvioso&quot;), n = 5) ejemplos ## word similarity to c(&quot;lluvioso&quot;) ## 1 lluvioso 1.0000000 ## 2 caluroso 0.8041209 ## 3 cálido 0.6896448 ## 4 húmedo 0.6866749 ## 5 gélido 0.6660152 ejemplos &lt;- modelo |&gt; closest_to(&quot;presidente&quot;, n = 5) ejemplos ## word similarity to &quot;presidente&quot; ## 1 presidente 1.0000000 ## 2 vicepresidente 0.8412900 ## 3 ex_presidente 0.8321029 ## 4 máximo_representante 0.7781001 ## 5 máximo_dirigente 0.7629962 ejemplos &lt;- modelo |&gt; closest_to(&quot;parís&quot;, n = 5) ejemplos ## word similarity to &quot;parís&quot; ## 1 parís 1.0000000 ## 2 londres 0.9232452 ## 3 nueva_york 0.8464673 ## 4 roma 0.8443222 ## 5 berlín 0.8081766 Y vemos, por ejemplo, que el modelo puede capturar conceptos relacionados con el estado del clima, capitales de países y números - aún cuando no hemos anotado estas funciones en el corpus original. Estos vectores son similares porque tienden a ocurrir en contextos similares. Geometría en el espacio de representaciones Ahora consideremos cómo se distribuyen las palabras en este espacio, y si existe estructura geométrica en este espacio que tenga información acerca del lenguaje. Consideremos primero el caso de plurales de sustantivos. Como el contexto de los plurales es distinto de los singulares, nuestro modelo debería poder capturar en los vectores su diferencia. Examinamos entonces cómo son geométricamente diferentes las representaciones de plurales vs singulares Si encontramos un patrón reconocible, podemos utilizar este patrón, por ejemplo, para encontrar la versión plural de una palabra singular, sin usar ninguna regla del lenguaje. Una de las relaciones geométricas más simples es la adición de vectores. Por ejemplo, extraemos la diferencia entre gol y goles: plural_1 &lt;- modelo[[&quot;goles&quot;]] - modelo[[&quot;gol&quot;]] plural_1 ## A VectorSpaceModel object of 1 words and 100 vectors ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -0.2301596 -0.2543171 -0.04071745 -0.2292878 0.004059255 -0.2283908 ## attr(,&quot;.cache&quot;) ## &lt;environment: 0x560e6743d8e0&gt; que es un vector en el espacio de representación de palabras. Ahora sumamos este vector a un sustantivo en singular, y vemos qué palabras están cercas de esta “palabra sintética”: vector &lt;- modelo[[&quot;partido&quot;]] + plural_1 modelo |&gt; closest_to(vector, n = 5) ## Warning in if (class(formula) == &quot;formula&quot;) {: the condition has length &gt; 1 and ## only the first element will be used ## Warning in if (class(object) == &quot;VectorSpaceModel&quot;) {: the condition has length ## &gt; 1 and only the first element will be used ## word similarity to vector ## 1 partidos 0.7961097 ## 2 goles 0.7589920 ## 3 partidos_disputados 0.6937101 ## 4 últimos_encuentros 0.6788752 ## 5 encuentros 0.6697611 Nótese que la más cercana es justamente el plural correcto, o otros plurales con relación al que buscábamos (como encuentros) Otro ejemplo: plural_1 &lt;- modelo[[&quot;días&quot;]] - modelo[[&quot;día&quot;]] vector &lt;- modelo[[&quot;mes&quot;]] + plural_1 modelo |&gt; closest_to(vector, n = 20) ## Warning in if (class(formula) == &quot;formula&quot;) {: the condition has length &gt; 1 and ## only the first element will be used ## Warning in if (class(object) == &quot;VectorSpaceModel&quot;) {: the condition has length ## &gt; 1 and only the first element will be used ## word similarity to vector ## 1 tres_meses 0.7858109 ## 2 días 0.7708776 ## 3 meses 0.7655628 ## 4 diez_días 0.7199002 ## 5 seis_meses 0.7110105 ## 6 quince_días 0.7092209 ## 7 nueve_meses 0.6903626 ## 8 doce_meses 0.6887811 ## 9 mes 0.6785786 ## 10 18_meses 0.6483637 ## 11 48_horas 0.6392776 ## 12 diez_años 0.6365554 ## 13 años 0.6339559 ## 14 semanas 0.6284049 ## 15 quince_años 0.6281021 ## 16 dos_semanas 0.6147185 ## 17 trimestres 0.6012591 ## 18 días_hábiles 0.5972889 ## 19 veinticinco_años 0.5955164 ## 20 nueves_meses 0.5947687 Veremos ahora cómo funciona para el género de sustantivos: fem_1 &lt;- modelo[[&quot;presidenta&quot;]] - modelo[[&quot;presidente&quot;]] vector &lt;- modelo[[&quot;rey&quot;]] + fem_1 modelo |&gt; closest_to(vector, n = 5) %&gt;% filter(word != &quot;rey&quot;) ## Warning in if (class(formula) == &quot;formula&quot;) {: the condition has length &gt; 1 and ## only the first element will be used ## Warning in if (class(object) == &quot;VectorSpaceModel&quot;) {: the condition has length ## &gt; 1 and only the first element will be used ## word similarity to vector ## 1 reina 0.7402226 ## 2 princesa 0.6662326 ## 3 pía 0.6249812 ## 4 perla 0.6189366 vector &lt;- modelo[[&quot;tío&quot;]] + fem_1 modelo |&gt; closest_to(vector, n = 5) %&gt;% filter(word != &quot;tío&quot;) ## Warning in if (class(formula) == &quot;formula&quot;) {: the condition has length &gt; 1 and ## only the first element will be used ## Warning in if (class(object) == &quot;VectorSpaceModel&quot;) {: the condition has length ## &gt; 1 and only the first element will be used ## word similarity to vector ## 1 dueña 0.7036596 ## 2 hermana 0.6947787 ## 3 abuela 0.6871846 ## 4 tía 0.6850960 10.4.1 Evaluación de calidad de modelos La evaluación de estas aplicaciones puede hacerse por ejemplo, con tareas de analogía, con listas de singular/plurales, de adjetivos/adverbios, masculino/femenino, etc (ver (Mikolov et al. 2013)), (ver por ejemplo https://github.com/tmikolov/word2vec/blob/master/questions-words.txt). Adicionalmente, si se utilizan en alguna tarea downstream, pueden evaluarse en el desempeño de esa tarea particular. Ejercicio: ¿cómo usarías esta geometría para encontrar el país en el que está una capital dada? Observación: falta afinar los parámetros en este modelo. Puedes probar cambiando negative sampling (por ejemplo, incrementa a 40), el número de vectores (50-200, por ejemplo), e incrementando window y el número de iteraciones. Considera también un modelo preentrenado mucho más grande como este. Puedes bajar los vectores de palabras y repetir las tareas mostradas (el formato bin es estándar para la implementación que usamos de word2vec). Podemos visualizar el espacio de representaciones reduciendo dimensionalidad. En este caso, utilizamos tsne: library(tsne) library(ggplot2) library(ggrepel) mat_vs &lt;- modelo@.Data # solo calculamos para las 500 palabras más comunes num_palabras &lt;- 500 set.seed(1203) vs_2 &lt;- tsne(mat_vs[1:num_palabras, ], max_iter = 1000, perplexity = 50) ## Warning in if (class(X) == &quot;dist&quot;) {: the condition has length &gt; 1 and only the ## first element will be used ## sigma summary: Min. : 0.50676082427291 |1st Qu. : 0.659095357760454 |Median : 0.736635035661307 |Mean : 0.75791167716744 |3rd Qu. : 0.841790649429495 |Max. : 1.11657529954154 | ## Epoch: Iteration #100 error is: 19.2455766810395 ## Epoch: Iteration #200 error is: 0.84088184930299 ## Epoch: Iteration #300 error is: 0.808986014121995 ## Epoch: Iteration #400 error is: 0.798502126441899 ## Epoch: Iteration #500 error is: 0.79708951735216 ## Epoch: Iteration #600 error is: 0.796517693130686 ## Epoch: Iteration #700 error is: 0.796283474583645 ## Epoch: Iteration #800 error is: 0.796006303282691 ## Epoch: Iteration #900 error is: 0.795958873280288 ## Epoch: Iteration #1000 error is: 0.795805321392407 set.seed(823) colnames(vs_2) &lt;- c(&quot;V1&quot;, &quot;V2&quot;) df_vs &lt;- as_tibble(vs_2, .name_repair = &quot;check_unique&quot;) %&gt;% mutate(palabra = rownames(mat_vs[1:num_palabras, ])) ggplot(df_vs %&gt;% sample_n(250), aes(x = V1, y = V2, label = palabra)) + geom_point(colour = &#39;red&#39;, alpha = 0.3) + geom_text_repel(size = 2, force = 0.3, segment.alpha = 0.5) ## Warning: ggrepel: 15 unlabeled data points (too many overlaps). Consider ## increasing max.overlaps 10.5 Usos de representaciones distribuidas La idea general de construir representaciones densas de las palabras (en lugar de representaciones ralas, como codificación dummy o one-hot encoding) es una fundamental en varias tareas de procesamiento de lenguaje natural. Se utiliza en clasificación de textos (fast-text es similar al modelo bag-of-words), respuesta de preguntas, traducción automática, recuperación de información, reconocimiento de entidades, etc. Word2vec también puede ser usado para sistemas de recomendación (las palabras son artículos, las canastas son los textos). Puedes ver la implementación original junto con aplicaciones aquí, y aquí. Un método similar es Glove. Referencias "],["referencias.html", "Referencias", " Referencias "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
