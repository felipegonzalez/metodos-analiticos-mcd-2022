[["index.html", "Métodos analíticos, ITAM 2022 Temario Evaluación", " Métodos analíticos, ITAM 2022 Felipe González 2022-02-21 Temario Este curso trata sobre diversas técnicas de análisis de datos, en su mayoría diseñadas para escalar a datos grandes. El enfoque del curso se concentra más en el entendimiento y aplicación de los algoritmos y los métodos, y menos en las herramientas para implementarlos. Análisis de conjuntos frecuentes Algoritmo a-priori Market basket analysis Búsqueda de elementos similares Minhashing para documentos Locality Sensitive Hashing (LSH), joins aproximados Sistemas de recomendación 1 Recomendación por contenido y filtros colaborativos Factorización de matrices y dimensiones latentes Reducción de dimensionalidad: DVS Descomposición en valores singulares Componentes principales Sistemas de recomendación 2 Métodos basados en similitud Mínimos cuadrados alternados Descenso en gradiente estocástico Retroalimentación implícita Recuperación de información Índices invertidos Modelo de espacio vectorial Normalización y similitud Indexado semántico latente Análisis de redes 1 Medidas de centralidad y pagerank Análisis de redes 2 Clustering y comunidades Modelos de lenguaje 1 N-gramas y conteos Aplicaciones Modelos de lenguaje 2 Inmersiones de palabras Modelos básicos de redes neuronales Aplicaciones de modelos de lenguaje Corrección de ortografía, reconocimiento de idiomas Clasificación de textos Métodos generales de clustering Las notas del curso están R, y en algunos casos usamos python o línea de comandos. Puedes usar python también para hacer tareas y ejercicios. Nuestro texto básico es (Leskovec, Rajaraman, and Ullman 2014). Referencias básicas adicionales son (Jurafsky and Martin 2000) (para procesamiento de lenguaje natural), y sparklyr para utlizar la interfaz de R a Spark. Evaluación Tareas semanales (30%) Examen teórico parcial (35%) Trabajo final (35%) Referencias "],["intro.html", "1 Introducción Tipos de soluciones", " 1 Introducción En este curso principalmente consideraremos técnicas para procesar, analizar y entender conjuntos de datos en dimensión alta (es decir, existen muchos atributos relevantes que describen a cada observación). Tareas básicas en este curso son, por ejemplo: Entender si las observaciones forman grupos útiles o interpretables, Construir o seleccionar atributos importantes para describir los datos de maneras más simples o útiles, En general, descubrir otros tipos de estructuras interesantes en datos de dimensión alta. Veremos aplicaciones relacionadas con este problema, como Análisis de market basket (descubrir características o artículos asociados), Sistemas de recomendación (medir similitud entre usuarios o artículos para hacer recomendaciones), Búsqueda de elementos muy similares o duplicados, Análisis de redes y detección de comunidades, Análisis y modelos para lenguaje natural. En general, consideraremos técnicas que son escalables y se aplican a datos masivos, que son los casos donde la dimensionalidad alta realmente puede explotarse efectivamente. Comenzaremos por describir las dificultades del análisis en dimensión alta. Simlitud en dimensión alta Cuando queremos identificar casos similares, o agruparlos por similitud, generalmente estamos pensando alguna tarea que queremos resolver: por ejemplo, recomendar productos o contenido a usuarios similares, detectar imágenes o textos duplicados o reusados, focalizar programas sociales a distintos tipos de hogares o personas, etc. Muchas veces, identificar los atributos correctos y el tipo de similitud produce herramientas útiles para resolver varios problemas en cada área de interés. Por ejemplo, la medida correcta de similitud de usuarios de Netflix tiene varios usos, igual que la identificación de palabras o pasajes de texto que ocurren en contextos similares. Consideramos por ejemplo perfiles de actividad en un sitio (Netflix), productos seleccionados por un comprador en un súper, imágenes o textos. En todos estos casos, las observaciones individuales tienen número muy grande de atributos (qué artículos están o no en una canasta, qué películas vio alguien, número de pixeles, o qué palabras ocurren y en qué orden ocurren). Hay varios problemas que tenemos que manejar en dimensión alta: Distintos atributos pueden ser importantes y otros ser no ser relevantes en ciertos grupos o en general. Por ejemplo: si queremos predecir la siguiente palabra en un texto (o traducirlo), la ausencia o presencia de algunas palabras es importantes, y muchas otras palabras son irrelevantes. Puede ser que muchos atributos no sean de interés para formar grupos útiles o identificar casos similares. Los atributos generalmente tienen relaciones complejas, y no es claro qué medidas de similitud son apropiadas. Medidas de distancia o similitud Consideremos la distancia euclideana en dimensión alta. Si generamos una muestra centrada en el origen, las distancias al origen se ve cómo sigue: library(tidyverse) centro &lt;- rep(0, 50) puntos &lt;- MASS::mvrnorm(n = 5000, mu = centro, Sigma = diag(rep(1, 50))) distancias &lt;- map_dbl(1:nrow(puntos), ~ sqrt(sum((centro - puntos[.x, ])^2))) qplot(distancias, bins = 30) Y vemos que ningún punto está realmente cerca del origen, y las distancias varían alrededor de un valor fijo. Si consideramos todos los posibles pares de puntos, vemos que todos parecen estar más o menos igual de lejos unos de otros: dist_pares &lt;- dist(puntos) |&gt; as.numeric() qplot(dist_pares, bins = 30) En dimensión alta, nuestra intuición muchas veces no funciona muy bien. Por ejemplo, supongamos que en una dimensión tenemos dos grupos claros: library(tidyverse) library(patchwork) library(glue) set.seed(10012) x &lt;- rnorm(100, c( -1,1), c(0.2,0.2)) datos &lt;- tibble(id = 1:100, variable = &quot;x&quot;, x = x) |&gt; mutate(grupo = id %% 2) ggplot(datos, aes(x = x, fill = factor(grupo))) + geom_histogram(bins = 30) Podemos calcular las distancias entre pares: distancias_pares &lt;- datos |&gt; select(-id, -grupo) |&gt; dist() |&gt; as.numeric() ## Warning in dist(select(datos, -id, -grupo)): NAs introduced by coercion g_1 &lt;- qplot(distancias_pares, bins = 30) Ahora agregamos 50 variables adiconales_ datos_aleatorios &lt;- map_df(1:50, function(i){ datos |&gt; select(id, grupo) |&gt; mutate(variable = glue(&quot;x_{i}&quot;), x = rnorm(100, 0, 1)) }) datos_1 &lt;- bind_rows(datos, datos_aleatorios) |&gt; pivot_wider(names_from = variable, values_from = x) Y vemos claramente que hay una estructura de grupos en los datos el la gráfica de la izquierda. Sin embargo, si agregamos variables ruidosas, la estructura no es clara y es difícil de recuperar: distancias_pares_2 &lt;- datos_1 |&gt; select(-id, -grupo) |&gt; dist() |&gt; as.numeric() g_2 &lt;- qplot(distancias_pares_2, bins = 30) g_1 + g_2 Si usamos un método simple de clustering, no recuperamos los grupos originales: grupos_km &lt;- kmeans(datos_1 |&gt; select(contains(&quot;x&quot;)), centers = 2) |&gt; pluck(&quot;cluster&quot;) table(grupos_km, datos_1$grupo) ## ## grupos_km 0 1 ## 1 38 29 ## 2 12 21 Tipos de soluciones Proyección y búsqueda de marginales interesantes En primer lugar, puede ser que aspectos útiles puedan extraerse de algunas marginales particulares \\(P(X_1, X_2)\\). Por ejemplo: Hay muchas variables ruidosas, en el sentido que no presentan estructuras interesantes o no son útiles para la tarea que nos interesa. (estas paso tiende a ser más guiado por teoría). Podemos buscamos regiones \\(P(X_1 = x_1, X_2 = x_2)\\) alrededor de las cuales se acumula alta probabilidad, o de otra manera: podemos buscar modas de marginales con alta densidad. Aplicaciones: análisis de conjuntos frecuentes o canastas, selección de características según varianza. Proyecciones globales Muchas veces podemos reducir dimensionalidad si reexpresamos variables (ya sea linealmente o no), y luego proyectamos (descomposición en valores singulares, PCA, descomposición de matrices) a regiones de alta densidad. Aplicaciones: construcción de índices resumen, sistemas de recomendación, indexado semántico latente en análisis de texto. Descripción de estructura local En algunos casos, los datos pueden ser del tipo donde la estructura local en pequeñas regiones del espacio de entradas es importante, y algunos casos tienden a acumularse en regiones particulares: Duplicados cercanos, búsqueda de vecinos cercanos. Análisis de centralidad en redes, búsqueda de comunidades. Métodos de reducción de dimensionalidad como t-sne y clustering Inmersiones (embeddings) Para algunos tipos de datos, la reducción de dimensionalidad debemos hacerla ad-hoc al problema. Por ejemplo, Redes convolucionales de clasificación de imágenes para obtener representaciones en dimensión baja de imágenes (similitud de imágenes). Construcción de representaciones donde palabras que ocurren en lugares similares son proyectadas a valores similares (inmersiones de palabras, redes neuronales para NLP). "],["frecuentes.html", "2 Análisis de conjuntos frecuentes 2.1 Datos de canastas 2.2 Conjuntos frecuentes 2.3 Monotonicidad de conjuntos frecuentes 2.4 Algoritmo a-priori 2.5 Modelos simples para análisis de canastas 2.6 Soporte teórico y conjuntos frecuentes 2.7 Reglas de asociación 2.8 Dificultades en el análisis de canastas 2.9 Otras medidas de calidad de reglas 2.10 Selección de reglas 2.11 Búsqueda de reglas especializadas 2.12 Visualización de asociaciones 2.13 Otras aplicaciones 2.14 Ejercicios", " 2 Análisis de conjuntos frecuentes Una de las tareas más antiguas de la minería de datos es la búsqueda de conjuntos frecuentes en canastas, o un análisis derivado que se llama análisis de reglas de asociación. Originalmente, pensamos que tenemos una colección grande de tickets de un supermercado. Nos interesa encontrar subconjuntos de artículos (por ejemplo, pan y leche) que ocurren frecuentemente en esos tickets. La idea es que si tenemos estos subconjuntos frecuentes, entonces podemos entender mejor el tipo de compras que hacen los clientes, diseñar mejor promociones y entender potenciales efectos cruzados, reordenar los estantes del supermercado, etc. En general, los conjuntos frecuentes indican asociaciones (y cuantificaciones de la asociación) entre artículos que hay que tomar en cuenta al momento de tomar decisiones. Esto normalmente se llama análisis de market basket. El análisis de subconjuntos frecuentes puede ser utilizado para otros propósitos, como veremos más adelante. 2.1 Datos de canastas Consideremos el siguiente ejemplo chico del paquete arules. Contiene unas \\(10\\) mil canastas observadas en de un supermercado durante un mes, agregadas a \\(169\\) categorías. En muchos casos prácticos, el número de canastas o transacciones puede llegar hasta los miles o millones de millones de transacciones, y el número de artículos puede ser de miles o decenas de miles. data(Groceries) # del paquete arules Groceries ## transactions in sparse format with ## 9835 transactions (rows) and ## 169 items (columns) lista_mb &lt;- as(Groceries, &quot;list&quot;) Estas son tres canastas (tickets) de ejemplo: lista_mb[[2]] ## [1] &quot;tropical fruit&quot; &quot;yogurt&quot; &quot;coffee&quot; lista_mb[[52]] ## [1] &quot;canned beer&quot; lista_mb[[3943]] ## [1] &quot;sausage&quot; &quot;UHT-milk&quot; &quot;flour&quot; &quot;flower (seeds)&quot; Describiremos algunas características típicas de este tipo de datos. En primer lugar, podemos calcular la distribución del número de artículos por canasta, y vemos que es una cantidad relativamente baja en comparación al número total de artículos existentes: sprintf(&quot;Número de canastas: %s&quot;, length(lista_mb)) ## [1] &quot;Número de canastas: 9835&quot; num_items &lt;- sapply(lista_mb, length) sprintf(&quot;Promedio de artículos por canasta: %.3f&quot;, mean(num_items)) ## [1] &quot;Promedio de artículos por canasta: 4.409&quot; qplot(num_items, binwidth = 1) Podemos hacer una tabla con las canastas y examinar los artículos más frecuentes: canastas_tbl &lt;- tibble( canasta_id = 1:length(lista_mb), articulos = lista_mb) canastas_tbl ## # A tibble: 9,835 × 2 ## canasta_id articulos ## &lt;int&gt; &lt;list&gt; ## 1 1 &lt;chr [4]&gt; ## 2 2 &lt;chr [3]&gt; ## 3 3 &lt;chr [1]&gt; ## 4 4 &lt;chr [4]&gt; ## 5 5 &lt;chr [4]&gt; ## 6 6 &lt;chr [5]&gt; ## 7 7 &lt;chr [1]&gt; ## 8 8 &lt;chr [5]&gt; ## 9 9 &lt;chr [1]&gt; ## 10 10 &lt;chr [2]&gt; ## # … with 9,825 more rows canastas_tbl$articulos[[1]] ## [1] &quot;citrus fruit&quot; &quot;semi-finished bread&quot; &quot;margarine&quot; ## [4] &quot;ready soups&quot; num_canastas &lt;- nrow(canastas_tbl) articulos_frec &lt;- canastas_tbl |&gt; unnest(cols = articulos) |&gt; group_by(articulos) |&gt; summarise(n = n()) |&gt; mutate(prop = n / num_canastas) |&gt; arrange(desc(n)) articulos_frec |&gt; mutate(across(where(is_double), ~ round(.x, 3))) |&gt; DT::datatable() ggplot(articulos_frec, aes(x = prop)) + geom_histogram(binwidth = 0.01) + xlab(&quot;Proporción de canastas&quot;) + ylab(&quot;Número de artículos&quot;) Y vemos que hay algunos pocos artículos que ocurren a tasas muy altas en las canastas. La mayoría tiene tasas de ocurrencia baja, y muchos ocurren en una fracción pequeña de las transacciones. Un primer análisis que podríamos considerar es el de canastas completas que ocurren frecuentemente. ¿Qué tan útil crees que puede ser este análisis? colapsar_canasta &lt;- function(x, sep = &quot;-&quot;){ # convierte cada canasta a una cadena x |&gt; as.character() |&gt; sort() |&gt; paste(collapse = &quot;-&quot;) } canastas_conteo &lt;- canastas_tbl |&gt; mutate(canasta_str = map_chr(articulos, colapsar_canasta)) |&gt; group_by(canasta_str) |&gt; summarise(n = n(), .groups = &quot;drop&quot;) |&gt; mutate(prop = round(n /num_canastas, 5)) |&gt; arrange(desc(n)) nrow(canastas_conteo) ## [1] 7011 Y aquí vemos las canastas más frecuentes: DT::datatable(canastas_conteo |&gt; head(n = 100) |&gt; mutate_if(is.numeric, ~ round(.x, 4))) Hay algunas canastas (principalmente canastas que contienen solo un artículo) que aparecen con frecuencia considerable (alrededor de \\(1\\%\\) o \\(2\\%\\)), pero las canastas están bastante dispersas en el espacio de posibles canastas (que es gigantesco: ¿puedes calcularlo?). Debido a esta dispersión este análisis es de utilidad limitada. Datos de canastas El tamaño de las canastas normalmente es chico (por ejemplo de \\(1\\) a \\(30\\) artículos distintos). El número total de artículos típicamente no es muy grande (de cientos a cientos de miles, por ejemplo). El número de canastas puede ser mucho mayor (en algunos casos miles de millones) y quizá no pueden leerse completas en memoria. La mayoría de los artículos ocurre con frecuencias relativamente bajas, aunque unos cuantos tienen frecuencia alta. El número de canastas distintas es alto, y hay pocas canastas frecuentes. El último inciso señala que encontrar canastas frecuentes no será muy informativo. En lugar de eso buscamos conjuntos de artículos (que podríamos llamar subcanastas) que forman parte de muchas canastas. 2.2 Conjuntos frecuentes Un enfoque simple y escalable para analizar estas canastas es el de los conjuntos frecuentes (frequent itemsets). Conjuntos frecuentes Consideramos un conjunto de artículos \\(I = \\{s_1,s_2,\\ldots, s_k\\}\\). El soporte de \\(I\\) lo definimos como la proporción de canastas que contienen (al menos) estos artículos: \\[P(I) = \\frac{n(I)}{n},\\] donde \\(n(I)\\) es el número de canastas que contienen todos los artículos de \\(I\\), y \\(n\\) es el número total de canastas. Sea \\(s\\in (0,1)\\). Para este valor fijo \\(s\\), decimos que un conjunto de artículos \\(I\\) es un conjunto frecuente cuando \\(P(I)\\geq s\\). Ejercicio Considera las canastas {1,2,3}, {1,2}, {2,4}, {2,3}. ¿Cuáles son los itemsets frecuentes de soporte &gt; 0.4? Ejemplo Explicamos más adelante la función apriori de arules, pero por lo pronto podemos examinar algunos conjuntos frecuentes de soporte mínimo \\(0.01\\) (como hay alrededor de \\(10000\\) canastas, esto significa que canastas que aparecieron al menos \\(100\\) veces durante el mes): pars &lt;- list(supp = 0.01, target = &quot;frequent itemsets&quot;) ap &lt;- apriori(lista_mb, parameter = pars) length(ap) ## [1] 333 Veamos algunos conjuntos frecuentes de tamaño \\(1\\): ap_1 &lt;- subset(ap, size(ap) == 1) length(ap_1) ## [1] 88 sort(ap_1, by = &quot;support&quot;) |&gt; DATAFRAME() |&gt; head(10) ## items support count ## 88 {whole milk} 0.25551601 2513 ## 87 {other vegetables} 0.19349263 1903 ## 86 {rolls/buns} 0.18393493 1809 ## 84 {soda} 0.17437722 1715 ## 85 {yogurt} 0.13950178 1372 ## 81 {bottled water} 0.11052364 1087 ## 83 {root vegetables} 0.10899847 1072 ## 82 {tropical fruit} 0.10493137 1032 ## 79 {shopping bags} 0.09852567 969 ## 80 {sausage} 0.09395018 924 Algunas de tamaño \\(2\\) y \\(3\\): ap_2 &lt;- subset(ap, size(ap) == 2) length(ap_2) ## [1] 213 sort(ap_2, by = &quot;support&quot;) |&gt; head(10) |&gt; DATAFRAME() ## items support count ## 301 {other vegetables,whole milk} 0.07483477 736 ## 300 {rolls/buns,whole milk} 0.05663447 557 ## 298 {whole milk,yogurt} 0.05602440 551 ## 291 {root vegetables,whole milk} 0.04890696 481 ## 290 {other vegetables,root vegetables} 0.04738180 466 ## 297 {other vegetables,yogurt} 0.04341637 427 ## 299 {other vegetables,rolls/buns} 0.04260295 419 ## 286 {tropical fruit,whole milk} 0.04229792 416 ## 295 {soda,whole milk} 0.04006101 394 ## 293 {rolls/buns,soda} 0.03833249 377 ap_3 &lt;- subset(ap, size(ap) == 3) length(ap_3) ## [1] 32 sort(ap_3, by=&quot;support&quot;) |&gt; head() |&gt; DATAFRAME() ## items support count ## 327 {other vegetables,root vegetables,whole milk} 0.02318251 228 ## 332 {other vegetables,whole milk,yogurt} 0.02226741 219 ## 333 {other vegetables,rolls/buns,whole milk} 0.01789527 176 ## 322 {other vegetables,tropical fruit,whole milk} 0.01708185 168 ## 331 {rolls/buns,whole milk,yogurt} 0.01555669 153 ## 320 {tropical fruit,whole milk,yogurt} 0.01514997 149 También podemos ver qué itemsets incluyen algún producto particular, por ejemplo ap_berries &lt;- subset(ap, items %pin% &quot;berries&quot;) length(ap_berries) ## [1] 4 sort(ap_berries, by =&quot;support&quot;) |&gt; head() |&gt; DATAFRAME() ## items support count ## 48 {berries} 0.03324860 327 ## 99 {berries,whole milk} 0.01179461 116 ## 97 {berries,yogurt} 0.01057448 104 ## 98 {berries,other vegetables} 0.01026945 101 ap_soda &lt;- subset(ap, items %pin% &quot;soda&quot;) length(ap_soda) ## [1] 28 sort(ap_soda, by =&quot;support&quot;) |&gt; head() |&gt; DATAFRAME() ## items support count ## 84 {soda} 0.17437722 1715 ## 295 {soda,whole milk} 0.04006101 394 ## 293 {rolls/buns,soda} 0.03833249 377 ## 294 {other vegetables,soda} 0.03274021 322 ## 276 {bottled water,soda} 0.02897814 285 ## 292 {soda,yogurt} 0.02735130 269 Observaciones: Si hay \\(m\\) artículos, entonces el número de posibles itemsets es de \\(2^m -1\\). Este es un número típicamente muy grande. En nuestro ejemplo, existen unos \\(7\\times 10^{50}\\) posibles itemsets. El número de itemsets de un tamaño fijo, por ejemplo \\(k=5\\), también puede ser muy grande ( \\(169 \\choose 5\\) es del orden de mil millones). Si existe un gran número canastas, contar todas las posibles subcanastas que ocurren es poco factible si lo hacemos por fuerza bruta: requeríamos usar tablas en disco que son relativamente lentas, y quizá no podremos mantener en memoria todos los conteos. Sin embargo, en el ejemplo de arriba encontramos solamente 333 itemsets frecuentes: este número es relativamente chico comparado con el número de posibles itemsets. Esto nos da indicios que contando de una manera apropiada puede ser posible encontrar todos los itemsets frecuentes de cualquier orden. Ejemplo Si reducimos el soporte a \\(0.0001\\) (que implica prácticamente que queremos contar todos los itemsets que ocurren), obtenemos: pars_2 &lt;- list(supp = 0.0001, target=&quot;frequent itemsets&quot;, maxtime = 0, maxlen = 6) ap_todos &lt;- apriori(lista_mb, parameter = pars_2) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## NA 0.1 1 none FALSE TRUE 0 1e-04 1 ## maxlen target ext ## 6 frequent itemsets TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 0 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s]. ## sorting and recoding items ... [169 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 ## Warning in apriori(lista_mb, parameter = pars_2): Mining stopped (maxlen ## reached). Only patterns up to a length of 6 returned! ## done [1.62s]. ## sorting transactions ... done [0.00s]. ## writing ... [10945131 set(s)] done [1.82s]. ## creating S4 object ... done [2.56s]. Entonces el número de itemsets que obtenemos (longitud menor o igual a \\(5\\)) es length(ap_todos) ## [1] 10945131 que es órdenes de magnitud más grande que el conjunto de todas las transacciones. En este ejemplo chico, el cálculo de esta colección (hasta canastas de tamaño 6) puede requierir menos de unos 2Gb de memoria (8Gb pueden no son ser suficientes para encontrar los de tamaño \\(8\\), \\(9\\) y \\(10\\)). Puedes ver entonces que para conjuntos de transacciones masivos, contar todos los itemsets generalmente será un proceso muy lento si no es que más bien infactible. 2.3 Monotonicidad de conjuntos frecuentes Consideramos el problema de encontrar los conjuntos frecuentes. Como discutimos arriba en las características de los datos de canastas, suponemos que La lista de transacciones es muy grande, y no puede leerse completa en memoria, Sin embargo, para una sola canasta, es posible calcular de manera relativamente rápida todos los subconjuntos de tamaño \\(k\\) (para \\(k=1,2,3,4\\), por ejemplo). Por ejemplo, si una canasta tiene \\(10\\) artículos, hay \\(\\binom{10}{3}\\) subcanastas de tamaño 3, \\(\\binom{10}{3} = 210\\). Calcular estos subconjuntos es relativamente rápido comparado con leer de disco una transacción. Finalmente, suponemos que el número de itemsets frecuentes es relativamente chico, debido a que el número de articulos que son más frecuentes es relativamente bajo (lo cual también depende de que escojamos un soporte suficientemente alto). Bajo estas características, el principio básico que hace posible hacer los conteos de itemsets frecuentes es el siguiente: Monotonicidad de itemsets Sea \\(s\\) un nivel de soporte mínimo fijo. Si un itemset \\(I\\) es frecuente, entonces todos sus subconjuntos son itemsets frecuentes. Equivalentemente, si algún subconjunto de un itemset no es frecuente, entonces el itemset no puede ser frecuente. Así que a priori, no es necesario examinar o contar itemsets que contienen al menos un subconjunto que no sea frecuente. Este hecho, junto con la selección de un soporte mínimo para los itemsets frecuentes, es el que hace que la búsqueda y conteo de itemsets frecuentes sea un problema factible, pues podemos descartar una gran cantidad de artículos o itemsets a priori de manera simple, y no es necesario contar todo. La demostración es como sigue: Sea \\(n(I)\\) el número de canastas que contiene \\(I\\), y supongamos que \\(\\tfrac{n(I)}{n}&gt;s\\) (\\(I\\) es un conjunto frecuente). Sea ahora \\(J\\subset I\\). Entonces cualquier canasta que contiene los artículos de \\(I\\) contiene también los artículos de \\(J\\) (que son menos), de forma que \\(n(J)\\geq n(I)\\). Como \\(\\tfrac{n(I)}{n}&gt;s\\), entonces \\(J\\) es un conjunto frecuente. Ejemplo En nuestro ejemplo anterior, el número total de itemsets de tamaño \\(2\\) es length(subset(ap_todos, size(ap_todos) == 2)) ## [1] 9636 Comparamos con los pares frecuentes cuando el soporte es \\(1\\%\\): length(subset(ap, size(ap) == 2)) ## [1] 213 una diferencia de casi dos ordenes de magnitud. 2.4 Algoritmo a-priori Para entender cómo aplicamos monotonicidad, consideremos cómo calcularíamos los pares frecuentes. Primero calculamos los artículos frecuentes (itemsets de tamaño \\(1\\)), que son los artículos que aparecen en al menos una proporción \\(s\\) de las canastas. (Contar candidatos) Esto requiere recorrer el archivo de transacciones y contar todos los artículos. (Podar) Examinamos los conteos y seleccionamos aquellos artículos que son frecuentes. Por el principio de monotonicidad, ningún par frecuente puede contener un artículo no frecuente. Así que para calcular pares: (Contar candidatos) Recorremos el archivo de transacciones. Para cada transacción, solo contamos pares candidatos cuyos dos artículos son artículos frecuentes (del paso anterior) (Podar) Examinamos los conteos y seleccionamos aquellos pares que son frecuentes. Nótese que este algoritmo requiere dos pasadas sobre el conjunto de transacciones. Ejercicio Aplica este algoritmo para las canastas {1,2,3}, {1,8}, {2,4}, {2,3,6,7,8}, {2,3,8}, {1,7,8}, {1,2,3,5}, {2,3}. (soporte &gt; 0.3) Observaciones En este algoritmo, no es necesario leer todas las transacciones a la vez, podemos procesarlas por bloques, por ejemplo. Usamos una pasada de los datos para cada tamaño de itemset frecuente. En la primera pasada del algoritmo (artículos frecuentes), típicamente no es un problema mantener el conteo de todos los artículos en memoria (hay relativamente pocos artículos). Si en la segunda pasada no usáramos monotonicidad, tendríamos que mantener conteos de todos los posibles pares (que son del orden \\(m^2\\), donde \\(m\\) es el número de artículos). Mantener este conteo en memoria podría ser difícil si el número de artículos es grande. Sin embargo, el número de artículos frecuentes generalmente es considerablemente menor. Para itemsets de tamaño más grande, el algoritmo original a priori (Agrawal and Srikant 1994) es: Algoritmos a-priori Sea \\(L_1\\) el conjunto de itemsets frecuentes de tamaño 1. Para obtener \\(L_k\\), el conjunto de itemsets frecuentes de tamaño \\(k\\): Sea \\(C_k\\) el conjunto de candidatos de tamaño \\(k\\), construido a partir de \\(L_{k-1}\\) (monotonicidad). Para cada transacción \\(t\\), Calculamos \\(S_t\\), que son los candidatos en \\(C_k\\) que están en \\(t\\). Agregamos 1 a cada conteo de los candidatos en \\(S_t\\). Filtramos los elementos de \\(C_k\\) que tengan conteo mayor que el soporte definido para obtener \\(L_k\\) Seguimos hasta que encontramos que algún \\(L_k\\) es vacío (no hay itemsets frecuente), o para alguna \\(k\\) fija. Observaciones: Este algoritmo se puede implementar de distintas maneras, por ejemplo: Hay distintas maneras de generar el conjunto \\(C_{k}\\) de candidatos. El paper original sugiere (suponiendo que los artículos siempre están ordenados en los itemsets) hacer un join de \\(L_{k-1}\\) consigo misma. Por ejemplo, para generar los tríos candidatos \\(C_3\\) a partir de \\(L_2\\) hacemos SELECT A.item1, A.item2, B.item2 FROM L2 AS A, L2 AS B WHERE A.item1 = B.item1, A.item2 &lt; B.item2 donde los itemsets estén ordenados en las canastas (por índice o lexicográficamente). Hay también distintas maneras de calcular \\(C_t\\) para cada transacción. El paper original sugiere una estructura de árbol para encontrar los subconjuntos de \\(t\\) que están en \\(C_k\\), y no es neceasrio calcular \\(C_k\\). Ver también (Borgelt 2004). Más detalles de la implementación de los algoritmos (incluyendo algunos más modernos como FPGrowth, que está implementado en spark) se puede encontrar en (Leskovec, Rajaraman, and Ullman 2014) y en (Borgelt 2004). FPGrowth construye una representación eficiente de árbol para los itemsets frecuentes y con esto evita el paso de construcción de candidatos (aunque tiene que mantener en memoria el árbol, que puede ser una estructura grande). 2.5 Modelos simples para análisis de canastas Podemos entender mejor el comportamiento de este análisis con algunos modelos simples para datos de canastas. En primer lugar, pensamos que los datos están en forma de codificación dummy (aunque no usemos esta representación para los datos reales, podemos considerarlo teóricamente). Una canasta es entonces un renglón de ceros y unos, dependendiendo qué artículos están o no en la canasta: \\[X= (X_1,X_2,\\ldots, X_m)\\] donde \\(X_i = 1\\) si el artículo \\(i\\) está en la canasta, y \\(X_i=0\\) si no está. Podríamos pensar entonces en construir modelos para la conjunta de las canastas \\[P(X_1=x_1,X_2=x_2,\\ldots, X_m=x_m)\\] Ejemplo Por ejemplo, si los items son 1-camisa, 2-pantalones, 3-chamarra, podríamos tener las dos transacciones \\(X = (1,0,0)\\), para alguien que solo compró una camisa \\(X = (1,0,1)\\), para alguien que solo compró camisa y chamarra Y podemos inventar una conjunta para todas las canastas, por ejemplo ## # A tibble: 8 × 4 ## p c ch prob ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 ## 2 0 0 1 0.105 ## 3 0 1 0 0.186 ## 4 0 1 1 0.186 ## 5 1 0 0 0.186 ## 6 1 0 1 0.186 ## 7 1 1 0 0.105 ## 8 1 1 1 0.047 A partir de esta conjunta podemos calcular cualquier probabilidad que nos interese. Por ejemplo, la probabilidad de que alguien compre una camisa dado que compró un pantalón es: prob_cp &lt;- filter(probs, p == 1 &amp; c == 1) |&gt; pull(prob) |&gt; sum() prob_p &lt;- filter(probs, p == 1) |&gt; pull(prob) |&gt; sum() (prob_cp / prob_p) |&gt; round(2) ## [1] 0.29 Como discutimos arriba, intentar estimar esta conjunta usando simples conteos de canastas no funciona, pues hay \\(2^n\\) posibles canastas, e incluso cuando \\(n\\) no es tan grande (por ejemplo \\(200\\)) es un número muy grande. Tenemos dos caminos (o una combinación de ellos): podemos hacer supuestos acerca de esta conjunta (y checar si son apropiados), o concentrarnos en estimar solamente algunas de sus características. Más adelante veremos algunas técnicas basadas en modelos (por ejemplo embeddings de productos y canastas) que pueden ser útiles para el problema de encontrar asociaciones entre artículos. Por el momento, en market basket tomamos el segundo camino: estimar sólo algunas cantidades de la conjunta de ocurrencia de artículos. La simplificación de market basket es concentrarnos en algunas marginales que involucren a pocos artículos de esta distribución, que tienen una forma como \\[P(X_{i}=1,X_{j}=1),\\] que es la probabilidad de que el conjunto \\({i,j}\\) aparezca en una canasta dada, o en los términos de market basket, el soporte del itemset \\({i,j}\\). La búsqueda de itemsets frecuentes se traduce entonces en buscar marginales de este tipo que no involucren muchas variables y que tengan valores altos - buscamos modas en las marginales de la distribución de las canastas. Modelo de artículos independientes Por otro lado, si hacemos supuestos acerca de la conjunta es posible ajustar modelos a los datos de canastas. En primer lugar, podemos considerar el modelo simplista que establece que la aparición o no de cada artículo es independiente del resto: \\[P(X_1=x_1,\\ldots, X_m=x_m) =\\prod_m P(X_j=x_j)\\] Y adicionalmente, suponemos que la probabilidad de cada artículo es fija dada por \\[P(X_j=1)=p_j\\]. Este modelo no es realista, pero podemos usarlo para entender algunos aspectos de nuestros algoritmos de conjuntos frecuentes. El soporte (bajo el modelo teórico) de un conjunto de \\(k\\) artículos es \\[P(X_{s_1}=1,X_{s_2}=1,\\ldots, X_{s_k}=1)=p_{s_1}p_{s_2}\\cdots p_{s_k}\\] Podemos ver qué pasa si simulamos transacciones bajo este modelo simple. Primero definimos una función para simular canastas con probabilidades dadas para los artículos simular_transacciones &lt;- function(n_items, n_trans, prob){ etiquetas &lt;- names(prob) canastas &lt;- map(seq(1, n_trans), function(i){ seleccion &lt;- rbinom(n_items, 1, prob = prob) etiquetas[seleccion == 1] }) canastas } Y ahora simulamos usando las proporciones que encontramos en el conjunto Groceries set.seed(1299) probs_items &lt;- itemFrequency(Groceries) |&gt; sort() trans &lt;- simular_transacciones(n_items = 169, n_trans = 10000, prob = probs_items) ap_indep &lt;- apriori(trans, parameter = list(support = 0.005, target = &quot;frequent itemsets&quot;), control = list(verbose = FALSE)) Por ejemplo, aquí vemos algunos pares frecuentes encontrados por el algoritmo: inspect(subset(ap_indep, support &gt; 0.015 &amp; size(ap_indep)==2)) ## items support count ## [1] {beef, whole milk} 0.0151 151 ## [2] {margarine, whole milk} 0.0160 160 ## [3] {butter, whole milk} 0.0159 159 ## [4] {pork, whole milk} 0.0172 172 ## [5] {domestic eggs, whole milk} 0.0165 165 ## [6] {brown bread, whole milk} 0.0186 186 ## [7] {other vegetables, whipped/sour cream} 0.0159 159 ## [8] {whipped/sour cream, whole milk} 0.0177 177 ## [9] {fruit/vegetable juice, whole milk} 0.0182 182 ## [10] {pip fruit, whole milk} 0.0195 195 ## [11] {canned beer, rolls/buns} 0.0158 158 ## [12] {canned beer, other vegetables} 0.0160 160 ## [13] {canned beer, whole milk} 0.0203 203 ## [14] {newspapers, rolls/buns} 0.0150 150 ## [15] {newspapers, other vegetables} 0.0171 171 ## [16] {newspapers, whole milk} 0.0206 206 ## [17] {bottled beer, rolls/buns} 0.0151 151 ## [18] {bottled beer, other vegetables} 0.0158 158 ## [19] {bottled beer, whole milk} 0.0209 209 ## [20] {other vegetables, pastry} 0.0192 192 ## [21] {pastry, whole milk} 0.0216 216 ## [22] {citrus fruit, soda} 0.0151 151 ## [23] {citrus fruit, rolls/buns} 0.0167 167 ## [24] {citrus fruit, other vegetables} 0.0177 177 ## [25] {citrus fruit, whole milk} 0.0240 240 ## [26] {sausage, soda} 0.0179 179 ## [27] {rolls/buns, sausage} 0.0170 170 ## [28] {other vegetables, sausage} 0.0194 194 ## [29] {sausage, whole milk} 0.0250 250 ## [30] {shopping bags, soda} 0.0172 172 ## [31] {rolls/buns, shopping bags} 0.0150 150 ## [32] {other vegetables, shopping bags} 0.0182 182 ## [33] {shopping bags, whole milk} 0.0258 258 ## [34] {soda, tropical fruit} 0.0180 180 ## [35] {rolls/buns, tropical fruit} 0.0193 193 ## [36] {other vegetables, tropical fruit} 0.0209 209 ## [37] {tropical fruit, whole milk} 0.0266 266 ## [38] {root vegetables, soda} 0.0184 184 ## [39] {rolls/buns, root vegetables} 0.0183 183 ## [40] {other vegetables, root vegetables} 0.0191 191 ## [41] {root vegetables, whole milk} 0.0294 294 ## [42] {bottled water, yogurt} 0.0177 177 ## [43] {bottled water, soda} 0.0202 202 ## [44] {bottled water, rolls/buns} 0.0218 218 ## [45] {bottled water, other vegetables} 0.0227 227 ## [46] {bottled water, whole milk} 0.0287 287 ## [47] {soda, yogurt} 0.0228 228 ## [48] {rolls/buns, yogurt} 0.0245 245 ## [49] {other vegetables, yogurt} 0.0258 258 ## [50] {whole milk, yogurt} 0.0334 334 ## [51] {rolls/buns, soda} 0.0345 345 ## [52] {other vegetables, soda} 0.0344 344 ## [53] {soda, whole milk} 0.0452 452 ## [54] {other vegetables, rolls/buns} 0.0342 342 ## [55] {rolls/buns, whole milk} 0.0462 462 ## [56] {other vegetables, whole milk} 0.0507 507 Estos pares frecuentes no se deben a asociaciones entre los artículos, sino a co-ocurrencia en las canastas. Artículos frecuentes apareceran en pares frecuentes, tríos frecuentes, etc. Comparamos por ejemplo el número de reglas encontradas para los datos reales, contra 20 simulaciones de este modelo: # parámetros de apriori pars &lt;- list(supp = 0.005, target=&quot;frequent itemsets&quot;) # producir 10 simulaciones replicaciones &lt;- map(1:20, function(i){ trans &lt;- simular_transacciones(n_items = 169, n_trans = 10000, prob = probs_items) ap_indep &lt;- apriori(trans, parameter = pars, control = list(verbose = FALSE)) conteos &lt;- table(size(ap_indep)) tibble(rep = i, long = names(conteos), n = as.numeric(conteos)) }) reps_tbl &lt;- bind_rows(replicaciones) Ahora vamos a comparar con el análisis de las canastas reales: # calcular observados ap &lt;- apriori(lista_mb, parameter = pars, control = list(verbose = FALSE)) conteos_obs &lt;- table(size(ap)) obs_tbl &lt;- tibble(long = names(conteos_obs), n = as.numeric(conteos_obs)) ggplot(reps_tbl, aes(x = long, y = n)) + geom_jitter(height = 0) + geom_point(data = obs_tbl, colour = &quot;red&quot;, size = 2) + labs(x = &quot;Tamaño&quot;) Y vemos claramente que el modelo simple está lejos de ajustar los datos que observamos en las canastas de Groceries. Hay muchas más combinaciones frecuentes de tamaño \\(2\\) y \\(3\\) de lo que esperaríamos si los artículos se compraran independientemente, y esto indica asociaciones positivas entre artículos que nos gustaría descubrir. Veremos más adelante cómo identificar este tipo de conjuntos frecuentes. Finalmente, comparamos los itemsets de ambos casos: coinciden &lt;- match(ap, ap_indep) ## Warning in match(x@items, table@items, nomatch = nomatch, incomparables = ## incomparables): Item coding not compatible, recoding item matrices first. coinciden[500:505] ## [1] 323 324 325 NA NA NA inspect(ap[500]) ## items support count ## [1] {pork, rolls/buns} 0.01128622 111 inspect(ap_indep[323]) ## items support count ## [1] {pork, rolls/buns} 0.011 110 sum(!is.na(coinciden)) # contar los matches ## [1] 491 length(ap) ## [1] 1001 length(ap_indep) ## [1] 521 Y vemos que en el análisis de datos reales estamos capturando la mayor parte de los itemsets frecuentes del modelo independiente. Estos itemsets se explican por la frecuencia simple de aparición de cada artículo. 2.6 Soporte teórico y conjuntos frecuentes El tamaño de los datos de transacciones está relacionado con los mínimos soportes que tiene sentido analizar. Supongamos por ejemplo que tenemos un conjunto de \\(n\\) transacciones, y buscamos soporte mínimo de \\(s\\). Consideramos entonces que un conjunto \\(I\\) tiene soporte \\(s_I\\) teórico (el que observaríamos con una muestra muy grande), y nuestro interés es ver qué tan bien podemos identificar como frecuentes aquellos conjuntos que satisfagan \\(s_I &gt; s\\). Notamos primero que el valor esperado de ocurrencias \\(n(I)\\) de un conjunto \\(I\\) en los tiene una distribución binomial con número de pruebas \\(n\\) y probabilidad de observar un éxito de \\(s_I\\). Si suponemos que \\(ns_I \\leq 1\\) y \\(n\\) no es muy chico, entonces la probabilidad de observar \\(I\\) 0 veces es aproximadamente tab_1 &lt;- tibble(ns = seq(0.1, 1, 0.2)) |&gt; mutate(prob_no_obs = ppois(0, ns)) tab_1 ## # A tibble: 5 × 2 ## ns prob_no_obs ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.1 0.905 ## 2 0.3 0.741 ## 3 0.5 0.607 ## 4 0.7 0.497 ## 5 0.9 0.407 De forma que con valores de \\(n\\) y \\(s\\) tales que \\(ns &lt;1\\) , típicamente no observaremos una buena parte de todos los conjuntos frecuentes que buscamos encontrar. Supongamos entonces que \\(ns\\geq 5\\), y queremos aproximar la probabilidad de encontrar una canasta \\(I\\) tal que \\(s_I = (1+\\alpha) s\\) (es decir, canastas con soporte de al menos \\(\\alpha\\)% más que el soporte mínimo elegido). Entonces usando la aproximación normal a la binomial, la probabilidad de capturar a \\(I\\) correctamente como frecuente es aproximadamente \\[P \\left (Z &gt; \\frac{\\sqrt{n}(s - s_I)}{\\sqrt{s_I(1-s_I)}} \\right ) = P \\left (Z &gt; \\frac{\\sqrt{n}(s - (1+\\alpha)s)}{\\sqrt{s_I(1-s_I)}} \\right )\\approx P \\left (Z &gt; -\\frac{\\alpha}{\\sqrt{1+\\alpha}}\\sqrt{ns}\\right )\\] crossing(ns = seq(5, 100, 5), alpha = c(0.10, 0.15, 0.20, 0.25)) |&gt; mutate(prob_capturar = map2_dbl(ns, alpha, \\(ns, alpha) pnorm(sqrt(ns) * alpha/sqrt(1-alpha)))) |&gt; ggplot(aes(x = ns, y = prob_capturar, colour = factor(alpha))) + geom_line() + geom_point() + ylab(&quot;Recall/Sensibilidad&quot;) Como ejercicio, puedes calcular la probabilidad de capturar incorrectamente como frecuente un conjunto \\(I\\) con \\(s_I = s/(1+\\alpha)\\) (la tasa de falsos positivos). En general, vemos que tomar \\(ns\\) menor que 10 no necesariamente es buena idea Así que si queremos capturar con probabilidad al menos 90% las canastas cuyo soporte es 20% mayor al soporte mínimo, necesitamos tomar \\(s\\) tal que \\(ns&gt;30\\), por ejemplo. Usualmente queremos hacer el soporte mínimo \\(s\\) un poco más chico, para que capturemos con alta probabilidad aquellos conjuntos de soporte teórico \\(s\\) (aunque esto implica también que capturaremos más conjuntos de menor soporte que el teórico). La elección de punto de corte es más o menos arbitraria (lo cual es una dificultad del análisis de conjuntos frecuentes), pero es importante recordar que si hacemos \\(s\\) mucho más chico que estas recomendaciones, capturaremos por azar muchos conjuntos cuya frecuencia teórica es mucho más baja de lo que esperábamos. Finalmente, recordamos que cuanto más chico tomemos el soporte, más conjuntos frecuentes encontraremos, y el procesamiento puede tomar mucho tiempo. Ejemplo Podemos experimentar con el modelo de independencia: simular_num_reglas &lt;- function(n_trans, prob, pars){ n_items &lt;- length(prob) etiquetas &lt;- 1:n_items if(!is.null(names(prob))) { etiquetas &lt;- names(prob) } trans &lt;- map(1:n_trans, function(i){ etiquetas[which(rbinom(n_items, 1, prob = prob) == 1)] }) ap_indep &lt;- apriori(trans, parameter = pars, control = list(verbose = FALSE)) conteos &lt;- table(size(ap_indep)) tibble(long = names(conteos), n = as.numeric(conteos)) } # fijamos para el ejemplo probs_items &lt;- sort(itemFrequency(Groceries)) pars_1 &lt;- list(support = 0.005, target = &quot;frequent itemsets&quot;) simular_1 &lt;- function(n_trans){ sim_tbl &lt;- simular_num_reglas(n_trans = n_trans, prob = probs_items, pars = pars_1) sim_tbl } set.seed(125) n_trans &lt;- c(10, 25, 50, 100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600, 51200) reps &lt;- tibble(n_trans = rep(n_trans, each = 5)) |&gt; mutate(conteos = map(n_trans, simular_1)) |&gt; mutate(ns = n_trans * 0.005) reps_long_2 &lt;- reps |&gt; unnest(cols = conteos) |&gt; filter(long == 2) ggplot(reps_long_2, aes(x=factor(ns), y = n)) + geom_boxplot() + ylab(&quot;Número de conjuntos frecuentes detectados (k=2)&quot;) + xlab(&quot;Soporte crudo (ns)&quot;) + labs(subtitle = &quot;Soporte min 0.005&quot;) Observaciones Para conjuntos de transacciones chicos no podemos capturar muchos conjuntos frecuentes teóricos porque no observamos suficientes transacciones. Para niveles medios de número de transacciones, podemos obtener resultados ruidosos: muchos subconjuntos que identificamos como frecuentes no lo son realmente (es variación muestral). Cuanto tenemos un número suficientemente alto de transacciones, el número de subconjuntos frecuentes encontrados se estabiliza en el valor teórico verdadero del número de conjuntos frecuentes que existen. 2.7 Reglas de asociación Aunque algunas veces lo único que nos interesa es la co-ocurrencia de artículos (por ejemplo, para entender qué artículos se podrían ver potencialmente afectados por acciones en otros artículos que están en el mismo itemset frecuente), otras veces nos interesa entender qué artículos están asociados a lo largo de canastas por otros factores, como tipo de cliente, tipo de ocasión o propósito (por ejemplo, hora del día, hacer un pastel, promociones, decisiones de organización de estantes), etc. Con este propósito podemos organizar la información de los itemsets frecuentes en términos de reglas de asociación. Un ejemplo de una regla de asociación es: Entre las personas que compran leche y pan, un \\(40\\%\\) compra también yogurt Una regla de asociación es una relación de la forma \\(I\\to j\\), donde \\(I\\) es un conjunto de artículos (el antecedente) y \\(j\\) es un artículo (el consecuente). Definimos la confianza de esta regla como \\[\\hat{P}(I\\to j) = \\hat{P}(j|I) = \\frac{n(I\\cup {j})}{n(I)} = \\hat{P}(I\\cup j)/\\hat{P}(I), \\] es decir, la proporción de canastas que incluyen al itemset \\(I\\cup {j}\\) entre las canastas que incluyen al itemset \\(I\\). La confianza siempre está entre 0 y 1. Observaciones: Por monotonicidad, si \\(J\\) es un conjunto de artículos más grande que \\(I\\) (es decir \\(I\\subset J\\)), entonces \\(n(J) \\leq n(I)\\): cualquier canasta que contiene a \\(J\\) también contiene a \\(I\\), y puede haber algunas canastas que contienen a \\(I\\) no contienen a \\(J\\). Bajo el modelo de items independientes, todas las confianzas satisfacen \\(P(I\\to j)=P(j)\\) (la confianza simplemente es la probabilidad de observar el artículo \\(j\\), independientemente del antecedente). Confianza alta no necesariamente significa una asociación de los items: si el consecuente \\(j\\) tiene soporte alto, entonces podemos obtener confianza alta aunque no haya asociación. Ejemplo En nuestro ejemplo anterior, el soporte de {whole milk,yogurt} es de \\(0.0560\\), el soporte de {whole milk} es \\(0.2555\\), así que la confianza de la regla \\(whole milk \\to yogurt\\) es \\(\\frac{0.0560}{0.2555}=\\) 0.22 Podemos usar la confianza para filtrar reglas que tienen alta probabilidad de cumplirse: Ejemplo pars &lt;- list(supp = 0.01, confidence = 0.20, target=&quot;rules&quot;, ext = TRUE, minlen = 2) reglas &lt;- apriori(lista_mb, parameter = pars) Podemos examinar algunas de las reglas: inspect(reglas[1:10,]) ## lhs rhs support confidence coverage ## [1] {hard cheese} =&gt; {whole milk} 0.01006609 0.4107884 0.02450432 ## [2] {butter milk} =&gt; {other vegetables} 0.01037112 0.3709091 0.02796136 ## [3] {butter milk} =&gt; {whole milk} 0.01159126 0.4145455 0.02796136 ## [4] {ham} =&gt; {whole milk} 0.01148958 0.4414062 0.02602949 ## [5] {sliced cheese} =&gt; {whole milk} 0.01077783 0.4398340 0.02450432 ## [6] {oil} =&gt; {whole milk} 0.01128622 0.4021739 0.02806304 ## [7] {onions} =&gt; {other vegetables} 0.01423488 0.4590164 0.03101169 ## [8] {onions} =&gt; {whole milk} 0.01209964 0.3901639 0.03101169 ## [9] {berries} =&gt; {yogurt} 0.01057448 0.3180428 0.03324860 ## [10] {berries} =&gt; {other vegetables} 0.01026945 0.3088685 0.03324860 ## lift count ## [1] 1.607682 99 ## [2] 1.916916 102 ## [3] 1.622385 114 ## [4] 1.727509 113 ## [5] 1.721356 106 ## [6] 1.573968 111 ## [7] 2.372268 140 ## [8] 1.526965 119 ## [9] 2.279848 104 ## [10] 1.596280 101 En la siguiente tabla, coverage es el soporte del antecedente (lhs = left hand side). Agregamos también el error estándar de la estimación de confidence (que es una proporción basada en el número de veces que se observa el antecedente): df_1 &lt;- sort(reglas, by = &quot;confidence&quot;) |&gt; DATAFRAME() df_2 &lt;- df_1 |&gt; select(LHS, RHS, coverage, confidence, support) |&gt; head(100) |&gt; mutate(lhs.base = num_canastas * coverage) |&gt; mutate(conf.ee = sqrt(confidence * (1 - confidence) / lhs.base)) |&gt; mutate_if(is.numeric, ~ round(.x, 2)) DT::datatable(df_2 |&gt; select(-lhs.base)) Observaciones: Nota que estas tres cantidades están ligadas en cada canasta por \\(coverage\\times confidence = support\\). Usa un argumento de probabilidad condicional para mostrarlo. Muchas de las reglas con confianza alta tienen como consecuente un artículo de soporte alto (por ejemplo, whole milk), como explicamos arriba. Nótese también que las reglas con confianza más alta tienden a tener soporte bajo. Esto lo discutiremos más adelante. Ejercicio Para un mismo consecuente (por ejemplo whole milk), examina cómo varían los valores de confidence. ¿A qué crees que se deba esto? Es natural que artículos frecuentes ocurran en muchas canastas juntas, es decir, que reglas formadas con ellas tengan confianza relativamente alta. Por ejemplo, la regla pan -&gt; verduras podría tener confianza y soporte alto, pero esto no indica ninguna asociación especial entre estos artículos. La razón puede ser que verduras es un artículo muy común. Podemos refinar las reglas de asociación considerando qué tan diferente es \\(P(j|I)\\) de \\(P(j)\\). La primera cantidad es la probabilidad de observar el item \\(j\\) bajo la información de que la canasta contiene a \\(I\\). Si esta cantidad no es muy diferente a \\(P(j)\\), entonces consideramos que esta regla no tiene mucho interés. El lift o intéres de una regla \\(I\\to j\\) se define como \\[L(I\\to j) = \\frac{\\hat{P}({j}|I)}{\\hat{P}({j})},\\] es decir, la confianza de la regla \\(I\\to j\\) dividida entre la proporción de canastas que contienen \\(j\\). En nuestro ejemplo, veamos dos reglas con interés muy distinto: df_1 &lt;- arrange(df_1, desc(lift)) df_1[c(4, nrow(df_1)),] |&gt; select(LHS, RHS, coverage, confidence, lift) ## LHS RHS coverage confidence lift ## 4 {beef} {root vegetables} 0.05246568 0.3313953 3.0403668 ## 231 {soda} {whole milk} 0.17437722 0.2297376 0.8991124 La primera regla tiene un interés mucho más alto que la segunda, lo que indica una asociación más importante entre los dos artículos. Observaciones Cuando decimos que un grupo de artículos están asociados, generalmente estamos indicando que forma alguna regla de asociación con lift alto. En principio también podría haber reglas con lift muy por debajo de uno, y eso también indica una asociación (por ejemplo coca y pepsi). Pero el método de itemsets frecuentes no es muy apropiado para buscar estas reglas, pues precisamente esas reglas tienden a tener soporte y confianza bajas. El valor del lift también puede escribirse (deméstralo) como \\[ \\frac{\\hat{P}(I\\cup\\{j\\})}{\\hat{P}(I)\\hat{P}({j})},\\] Cuando los artículos son independientes, esta cantidad está cercana a \\(1\\). Es una medida de qué tan lejos de independencia están la ocurrencia de los itemsets \\(I\\) y \\(j\\). Ejemplo df_1 &lt;- sort(reglas, by = &quot;lift&quot;) |&gt; DATAFRAME() En esta tabla, coverage es el soporte del antecedente (lhs = left hand side): df_2 &lt;- df_1 |&gt; select(LHS, RHS, coverage, lift, confidence, support) |&gt; head(100) |&gt; mutate_if(is.numeric, ~ round(.x, 2)) DT::datatable(df_2) Las reglas de asociación se calculan comenzando por calcular los itemsets frecuentes según el algoritmo a priori explicado arriba. Para encontrar las reglas de asociación hacemos: Para cada itemset frecuente \\(f\\), tomamos como candidatos a consecuentes los artículos \\(i\\) de \\(f\\) Si la confianza \\(\\frac{\\hat{P}(I)}{\\hat{P}(I-\\{j\\})}\\) es mayor que la confianza mínima, agregamos la regla de asociación \\(I\\to j\\). Con este proceso encontramos todas las reglas \\(I\\to j\\) tales que \\(I\\cup\\{j\\}\\) es un itemset frecuente. 2.8 Dificultades en el análisis de canastas El análisis de canastas es un método rápido y simple que nos da varias maneras de explorar las relaciones entre los artículos. Sin embargo, hay varias dificultades en su aplicación. Número de reglas y itemsets Muchas veces encontramos un número muy grande de itemsets o reglas. Hay varias maneras de filtrar estas reglas según el propósito. Si filtramos mucho, perdemos reglas que pueden ser interesantes. Si filtramos poco, es difícil entender globalmente los resultados del análisis. Un punto de vista es producir una cantidad de reglas para procesar posteriormente con queries: por ejemplo, si nos interesa entender las relaciones de berries con otros artículos, podemos filtrar las reglas encontradas y examinarlas más fácilmente. Cortes estrictos en el filtrado Cuando seleccionamos valores mínimos de soporte, confianza y/o lift, estas decisiones son más o menos arbitrarias. Distintos analistas pueden llegar a resultados distintos, incluso cuando el propósito del análisis sea similar, y en ocasiones hay que iterar el análisis para encontrar valores adecuados que den conjuntos razonables con resultados interesantes. Este último concepto es subjetivo. Redundancia de reglas Existe alguna redundancia en las reglas que encontramos. Por ejemplo, podríamos tener {yogurt, berries} -&gt; {whipped cream}, pero también {yogurt} -&gt; {whipped cream}. Este traslape de reglas hace también difícil entender conjuntos grandes de reglas. Variabilidad de medidas de calidad Un problema del análisis clásico de soporte-confianza-lift es la variabilidad de las estimaciones de confianza y lift. Cuando comenzamos poniendo valores de soporte y confianza relativamente bajos, encontramos muchas reglas. Muchas de estas reglas son ruidosas (en un número más grande de transacciones las descalificaríamos). Intentamos muchas veces filtrar u ordenar por lift, para considerar las reglas más interesantes Sin embargo, encontramos entonces que muchas reglas de lift y/o confianza altas son aquellas que tienen soporte bajo y consecuentes poco frecuentes. Como veremos más adelante, esto se debe muchas veces a error de estimación. Los valores más grandes de lift generalmente son sobreestimaciones, por la naturaleza del análisis basado en cortes. Si regresamos a incrementar soporte y confianza, potencialmente perdemos reglas interesantes. Veamos cómo se comportan confianza y lift para el modelo donde no hay asociaciones. Utilizamos el modelo de independencia que explicamos arriba. Obsérvese que en este modelo todas las confianzas teóricas son iguales a la frecuencia del consecuente, y todos los valores teóricos de lift son \\(1\\): pars &lt;- list(support=0.002, confidence = 0.0, target=&quot;rules&quot;, ext = TRUE, minlen = 2) sims_reglas &lt;- map(1:10, function(i){ trans &lt;- simular_transacciones(n_items = 169, n_trans = 10000, prob = probs_items) ap_random &lt;- apriori(trans, parameter = pars, control = list(verbose = FALSE)) ap_random }) Y notamos que conforme el soporte de la regla es más bajo, hay más variabilidad en las estimaciones del confianza y lift. En este caso utilizamos plot(subset(sims_reglas[[4]], rhs %pin% &quot;whole milk&quot;), measure=c(&quot;support&quot;,&quot;confidence&quot;), shading = &quot;lift&quot;, engine = &quot;plotly&quot;) ## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter. plot(subset(sims_reglas[[4]], rhs %pin% &quot;whole milk&quot;), measure=c(&quot;support&quot;,&quot;lift&quot;), shading = &quot;confidence&quot;, engine = &quot;plotly&quot;) ## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter. El valor de confianza y de lift puede ser altamente variable para reglas con soporte bajo. Podemos tomar dos caminos: Cuando hagamos el soporte más bajo, incrementamos el valor de lift mínimo. Esto evita que obtengamos demasiadas reglas que no representan interacciones reales entre los artículos. Podemos usar otras medidas que tomen en cuenta la variabilidad de las estimaciones. Por ejemplo, hyper-lift y hyper-confidence están basados en modelos simples (como el que vimos arriba), que filtran aquellos valores de calidad que están en las colas de las distribuciones de los modelos simples. 2.9 Otras medidas de calidad de reglas Hay una gran cantidad de medidas de interés de reglas que se han propuesto desde que se usa el análisis de canasta. Aquí discutimos hyper-lift y hyper-confidence, que toman en cuenta el soporte de las reglas para proponer puntos de corte (Hahsler and Hornik 2008). Explicamos aquí el hyper-lift para una regla \\(i\\to j\\). Consideramos el modelo de independencia (lo pensamos como el modelo nulo), fijando las probabilidades de ocurrencia de los artículos según los datos (como hicimos en los ejemplos de arriba) y el número de transacciones. Bajo este modelo, el número de ocurrencias \\(X_{\\{i,j\\}}\\) de el itemset \\(\\{i,j\\}\\) es una variable aleatoria con distribución conocida (binomial). Esta distribución representa la variación que podemos observar en los conteos de \\(\\{i,j\\}\\) bajo distintas muestras de transacciones del mismo tamaño. La idea básica del hyperlift es comparar el conteo \\(n(\\{i,j\\})\\) con la cola superior de la distribución de \\(X_{i,j}\\) bajo el supuesto de independencia, poniendo \\[HL(I\\to j) = \\frac{n(\\{I,j\\})}{Q_\\delta (X_{I,j})},\\] donde \\(Q_\\delta\\) es tal que \\(P(X_{I,j} &lt; Q_\\delta (X_{I,j}))\\approx \\delta\\). Tomamos por ejemplo \\(\\delta=0.99\\). De esta forma, \\(HL&gt;1\\) sólo cuando el conteo observado \\(n(\\{i,j\\})\\) está en la cola superior del conteo bajo la hipótesis nula de independencia. Esto toma en cuenta la variabilidad de los conteos (que es grande en términos relativos cuando el soporte es bajo). Observaciones: El modelo de independencia que se usa en el paquete arules es una variación del que vimos aquí, ver los detalles en (Hahsler and Hornik 2008). Los valores de hyper-lift no son realmente comparables a los de lift, son dos medidas de calidad de asociación diferentes, pero similares en cuanto a lo que quieren capturar. Hyper-lift bajo hipótesis de independencia Veamos cómo se comporta el hyper-lift simulando datos con el modelo de independencia: trans &lt;- simular_transacciones(n_items = 169, n_trans = 10000, prob = probs_items) ap_random &lt;- apriori(trans, parameter = list(support=0.001, confidence = 0.10, target=&quot;rules&quot;, ext = TRUE, minlen = 2), control = list(verbose = FALSE)) agregar_hyperlift &lt;- function(reglas, trans){ quality(reglas) &lt;- cbind(quality(reglas), hyper_lift = interestMeasure(reglas, measure = &quot;hyperLift&quot;, transactions = trans)) reglas } ap_random &lt;- agregar_hyperlift(ap_random, trans) Vemos claramente que la gran mayoría de reglas obtenidas ahora tienen hyper-lift menor que uno plot(ap_random, measure=c(&quot;lift&quot;,&quot;hyper_lift&quot;), shading = &quot;support&quot;) ## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter. Cortando en un valor relativamente bajo de hyper-lift, vemos que nos deshacemos correctamente de casi todas las reglas: length(ap_random) ## [1] 3591 length(subset(ap_random, lift &gt; 1)) ## [1] 2643 length(subset(ap_random, hyper_lift &gt; 1)) ## [1] 130 Hyper-lift para datos de canastas Ahora aplicamos a los datos reales pars &lt;- list(supp = 0.002, confidence = 0.10, target=&quot;rules&quot;, ext = TRUE, minlen = 2) reglas &lt;- apriori(lista_mb, parameter = pars, control = list(verbose=FALSE)) reglas &lt;- agregar_hyperlift(reglas, Groceries) length(reglas) ## [1] 8332 Vemos que podemos cortar niveles de hyper-lift donde obtenemos reglas de soporte relativamente alto. plot(reglas, measure=c(&quot;lift&quot;,&quot;hyper_lift&quot;), shading = &quot;support&quot;) ## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter. Si cortamos en valores que dan un número similar de reglas, por ejemplo: filtradas_hl &lt;- subset(reglas, hyper_lift &gt; 2) filtradas_lift &lt;- subset(reglas, lift &gt; 3.7) length(filtradas_hl) ## [1] 439 length(filtradas_lift) ## [1] 480 Vemos que las reglas cortadas con hyper-lift tienen mejores valores de soporte: library(patchwork) soportes &lt;- qqplot( quality(filtradas_hl)$support, quality(filtradas_lift)$support, plot.it = FALSE) |&gt; as_tibble() g_soporte &lt;- ggplot(soportes, aes(x, y)) + geom_abline(colour = &quot;red&quot;) + geom_point(alpha = 0.5) + xlab(&quot;Soporte (filtro Hyperlift)&quot;) + ylab(&quot;Soporte (filtro Lift)&quot;) + coord_fixed(xlim = c(0, 0.03), ylim = c(0, 0.03)) + ggtitle(&quot;Soporte&quot;) soportes &lt;- qqplot( quality(filtradas_hl)$lift, quality(filtradas_lift)$lift, plot.it = FALSE) |&gt; as_tibble() g_lift &lt;- ggplot(soportes, aes(x, y)) + geom_abline(colour = &quot;red&quot;) + geom_point(alpha = 0.5) + xlab(&quot;Lift (filtro Hyperlift)&quot;) + ylab(&quot;Lift (filtro Lift)&quot;) + coord_fixed() + ggtitle(&quot;Lift&quot;) g_soporte + g_lift + plot_annotation( subtitle = &quot;Soporte y Lift con filtro de reglas por lift vs. hyperlift&quot;) La distribución de valores de lift no es tan diferente, de forma que esta medida de calidad no se degrada en el conjunto de reglas que encontramos: En resumen, al utilizar hyper-lift para filtrar reglas en lugar de lift obtenemos reglas de mejor calidad: Descartamos más reglas de soporte bajo que tienen lift alto por azar. Los valores de soporte de las reglas tienden a ser más altos. Los valores de lift son comparables 2.10 Selección de reglas Ahora discutiremos cómo seleccionar itemsets frecuentes y reglas. Filtrar con todos estos criterios (soporte, confianza, soporte del antecedente, lift) no es simple, y depende de los objetivos del análisis. Recordemos también que estos análisis están basados justamente en cortes “duros” de los datos, más o menos arbitrarios, y por lo tanto pueden los resultados son variables. Hay varias maneras de conducir el análisis. Dos tipos útiles son: Itemsets de alta frecuencia: en este enfoque buscamos reglas con soporte y confianza relativamente altos. Generalmente están asociados a productos muy frecuentes, y nos indica potencial de interacción entre los artículos. Este análisis es más una reexpresión de la información contenida en los itemsets frecuentes. En este caso, podemos filtrar con soporte alto, para evitar estimaciones ruidosas (por ejemplo, soporte mínimo de 300 canastas). Interacciones altas: en este enfoque donde buscamos entender nichos. Consideramos valores de soporte y confianza más bajos, pero con valores de lift/hyper-lift alto. Este análisis es más útil para entender, por ejemplo, propósitos de compras, convivencia de artículos, tipos de comprador, etc. Colección de reglas para hacer querys: la colección de reglas puede ser más grande, e incluir por ejemplo resultados de distintas corridas de market basket (incluyendo los dos enfoques de arriba). Las reglas se examinan seleccionando antecedentes o consecuentes, valores altos de soporte, etc, según la pregunta particular que se quiera explorar. Ejemplo: canastas grandes Para entender las canastas grandes, podemos variar valores de soporte y confianza para encontrar un número manejable de reglas. pars &lt;- list(support = 0.02, confidence = 0.20, minlen = 2, target=&quot;rules&quot;, ext = TRUE) reglas_1 &lt;- apriori(lista_mb, parameter = pars) Esta elección de parámetros resulta en 72. Podemos ordenar por hyper-lift: plot(reglas_1, colors=c(&quot;red&quot;, &quot;gray&quot;), engine = &quot;plotly&quot;) reglas_1 &lt;- agregar_hyperlift(reglas_1, lista_mb) DT::datatable(DATAFRAME(sort(reglas_1, by = &quot;hyper_lift&quot;)) |&gt; select(-count, -coverage) |&gt; mutate_if(is.numeric, ~ round(.x, 2))) Observaciones: conforme bajamos en esta tabla (ordenada por soporte), las estimaciones de confianza y lift son menos precisas. 2.11 Búsqueda de reglas especializadas Otra manera de usar este análisis es intenando buscar asociaciones más fuertes (lift o hyper-lift más alto), aún cuando sacrificamos soporte. Por su naturaleza, este tipo de análisis puede resultar en reglas más ruidosas (malas estimaciones de confianza y lift), pero es posible filtrar valores más altos de estas cantidades para encontrar reglas útiles. Comenzamos con un soporte y confianza más bajas pars &lt;- list(support = 0.001, confidence = 0.1, minlen = 2, target=&quot;rules&quot;, ext = TRUE) b_reglas &lt;- apriori(lista_mb, parameter = pars) b_reglas &lt;- agregar_hyperlift(b_reglas, lista_mb) Y ahora filtramos con valores más grandes de hyper-lift. Podemos filtrar adicionalmente con coverage para obtener reglas que aplican con más frecuencia: b_reglas ## set of 32783 rules b_reglas_lift &lt;- subset(b_reglas, hyper_lift &gt; 2.5 &amp; size(b_reglas) &lt; 4 &amp; coverage &gt; 0.01) b_reglas_lift &lt;- sort(b_reglas_lift, by = &quot;hyper_lift&quot;) DT::datatable(DATAFRAME(b_reglas_lift) |&gt; select(-count, -coverage) |&gt; mutate_if(is.numeric, ~ round(.x, 3))) 2.12 Visualización de asociaciones Tener una visión amplia del market basket analysis es difícil (típicamente, funciona mejor como un resultado al que se le hacen querys, o uno donde filtramos cuidadosamente algunas reglas que puedan ser útiles). Así que muchas veces ayuda visualizar los pares con asociación alta: Construimos todas las reglas con un antecedente y un consecuente. Filtramos las reglas con hyper-lift relativamente alto (por ejemplo &gt; \\(1.5\\), pero hay que experimentar). Representamos como una gráfica donde los nodos son artículos, y las aristas son relaciones de lift alto. Usamos algún algoritmo para representar gráficas basado en fuerza, usando como peso el lift. 2.12.1 Ejemplo En nuestro caso, podríamos tomar (ajustando parámetros para no obtener demasiadas reglas o demasiado pocas) b_reglas_lift &lt;- subset(b_reglas, hyper_lift &gt; 1.75 &amp; confidence &gt; 0.05) reglas_f &lt;- subset(b_reglas_lift, size(b_reglas_lift) == 2) library(tidygraph) library(ggraph) df_reglas &lt;- reglas_f |&gt; DATAFRAME() |&gt; rename(from = LHS, to = RHS) df_reglas$weight &lt;- log(df_reglas$hyper_lift) nrow(df_reglas) ## [1] 94 graph_1 &lt;- as_tbl_graph(df_reglas) |&gt; mutate(centrality = centrality_degree(mode = &quot;all&quot;)) ggraph(graph_1, layout = &quot;fr&quot;, start.temp=100) + geom_edge_link(aes(alpha=lift), colour = &quot;red&quot;, arrow = arrow(length = unit(4, &quot;mm&quot;))) + geom_node_point(aes(size = centrality, colour = centrality)) + geom_node_text(aes(label = name), size=4, colour = &quot;gray20&quot;, repel=TRUE) + theme_graph(base_family = &quot;sans&quot;) Para gráficas más grandes, es mejor usar software especializado para investigar las redes que obtenemos (como Gephi): b_reglas_lift &lt;- subset(b_reglas, hyper_lift &gt; 1.5 &amp; coverage &gt; 0.01) reglas_f &lt;- subset(b_reglas_lift, size(b_reglas_lift) == 2) length(reglas_f) ## [1] 251 reglas_f |&gt; DATAFRAME() |&gt; rename(source = LHS, target = RHS) |&gt; select(-count) |&gt; write_csv(file = &quot;./salidas/reglas.csv&quot;) Para el análisis de canastas grandes: reglas_f2 &lt;- subset(reglas_1, hyper_lift &gt; 1.3, confidence &gt; 0.4) df_reglas &lt;- reglas_f2 |&gt; DATAFRAME() |&gt; rename(from = LHS, to = RHS) df_reglas$weight &lt;- log(df_reglas$hyper_lift) graph_1 &lt;- as_tbl_graph(df_reglas) |&gt; mutate(centrality = centrality_degree(mode = &quot;all&quot;)) ggraph(graph_1, layout = &quot;fr&quot;, start.temp=100) + geom_edge_link(aes(alpha=hyper_lift), colour = &quot;red&quot;, arrow = arrow(length = unit(4, &quot;mm&quot;))) + geom_node_point(aes(size = centrality, colour = centrality)) + geom_node_text(aes(label = name), size=4, colour = &quot;gray20&quot;, repel=TRUE) + theme_graph(base_family = &quot;sans&quot;) 2.13 Otras aplicaciones Análisis de tablas de variables categóricas: podemos considerar una tabla con varias variables categóricas. Una canasta son los valores que toman las variables. Por ejemplo, podríamos encontrar reglas como {hogar = propio, ocupación=profesional} -&gt; ingreso = alto. Puedes ver más de este análisis en (Hastie, Tibshirani, and Friedman 2017), por ejemplo, sección 14.2. Conceptos relacionados: si los artículos son palabras y las canastas documentos (tweets, por ejemplo), este tipo de análisis (una vez que eliminamos las palabras más frecuentes, que no tienen significado como artículos, preposiciones, etc.), puede mostrar palabras que co-ocurren para formar conceptos relacionados. Plagiarismo: si los artículos son documentos y los canastas oraciones, el análisis de canastas puede encontrar documentos que contienen las mismas oraciones. Si varias canastas (oraciones) “contienen” los mismos artículos (documentos), entonces esas oraciones son indicadores de plagio 2.14 Ejercicios Considera los datos de datos/recetas. Lee los datos, asegúrate que puedes filtrar por tipo de cocina, y que puedes aplicarles la función apriori de arules (o cualquier otra herramienta que estés utilizando). Calcula la frecuencia de todos los artículos (ingredientes). El resto de este ejercicio lo haremos a principio de la siguiente clase. Acerca de los datos: Cada receta es una canasta, y los artículos son los ingredientes que usan. Puedes consultar el artículo original aquí. Haz algunos experimentos el ejemplo 2.12.1 que vimos en clase: incrementa/decrementa hyperlift, incrementa/decrementa soporte. ¿Qué pasa con las gráficas resultantes y el número de reglas? (Opcional) Muchas veces el análisis de canastas puede hacerse con una muestra de transacciones. Leer secciones 6.4.1 a 6.4.4 de (Leskovec, Rajaraman, and Ullman 2014). Referencias "],["similitud.html", "3 Similitud y vecinos cercanos 3.1 Similitud de conjuntos 3.2 Representación de documentos como conjuntos 3.3 Representación matricial 3.4 Minhash y reducción probabilística de dimensionalidad 3.5 Agrupando textos de similitud alta 3.6 Ejemplo: tweets 3.7 Verificar si un nuevo elemento es duplicado 3.8 Controlando la sensibilidad y umbral de similitud 3.9 Distancia euclideana y LSH 3.10 Locality Sensitive Hashing (LSH) 3.11 LSH para imágenes 3.12 Joins por similitud 3.13 Ejemplo: entity matching", " 3 Similitud y vecinos cercanos En esta parte consideraremos la tarea de agrupar eficientemente elementos muy similares en conjuntos datos masivos. Algunos ejemplos de esta tarea son: Encontrar documentos similares en una colección de documentos. Esto puede servir para detectar plagio, deduplicar noticias o páginas web, hacer matching de datos de dos fuentes (por ejemplo, nombres completos de personas), etc. Ver por ejemplo Google News. Encontrar usuarios similares (Netflix), en el sentido de que tienen gustos similares, o películas similares, en el sentido de qe le gustan a las mismas personas. Encontrar imágenes similares en una colección grande, ver por ejemplo Pinterest. Uber: rutas similares que indican (fraude o abusos)[https://eng.uber.com/lsh/]. Deduplicar registros de usuarios de algún servicio (por ejemplo, beneficiarios de programas sociales). Estos problemas no son triviales por dos razones: Los elementos que queremos comparar muchas veces están naturalmente representados en espacios de dimensión alta, y es relativamente costoso comparar un par (documentos, imágenes, usuarios, rutas). Muchas veces es preferible construir una representación más compacta y hacer comparaciones con las versiones comprimidas. Si la colección de elementos es grande (\\(N\\)), entonces el número de pares posibles es del orden de \\(N^2\\), y es muy costoso hacer todas las posibles comparaciones para encontrar los elementos similares (por ejemplo, comparar \\(100\\) mil documentos, con unas \\(10\\) mil comparaciones por segundo, tardaría alrededor de \\(5\\) días). Si tenemos que calcular todas las similitudes, no hay mucho qué hacer. Pero muchas veces nos interesa encontrar pares de similitud alta, o completar tareas más específicas como contar duplicados, etc. En estos casos, veremos que es posible construir soluciones probabilísticas aproximadas para resolver estos problemas de forma escalable. Aunque veremos más adelante métricas de similitud comunes como la dada por la distancia euclideana o distancia coseno, por ejemplo, en esta primera parte nos concentramos en discutir similitud entre pares de textos. Los textos los podemos ver como colecciones de palabras, o de manera más general, como colecciones de cadenas. 3.1 Similitud de conjuntos Muchos de estos problemas de similitud se pueden pensar como problemas de similitud entre conjuntos. Por ejemplo, los documentos son conjuntos de palabras, conjuntos de pares de palabras, sucesiones de caracteres, una película se puede ver como el conjunto de personas a las que les gustó, o una ruta como un conjunto de tramos, etc. Hay muchas medidas que son útiles para cuantificar la similitud entre conjuntos. Una que es popular, y que explotaremos por sus propiedades, es la similitud de Jaccard: La similitud de Jaccard de los conjuntos \\(A\\) y \\(B\\) está dada por \\[sim(A,B) = \\frac{|A\\cap B|}{|A\\cup B|}\\] Esta medida cuantifica qué tan cerca está la unión de \\(A\\) y \\(B\\) de su intersección. Cuanto más parecidos sean \\(A\\cup B\\) y \\(A\\cap B\\), más similares son los conjuntos. En términos geométricos, es el área de la intersección entre el área de la unión. Ejercicio Calcula la similitud de Jaccard entre los conjuntos \\(A=\\{5,2,34,1,20,3,4\\}\\) y \\(B=\\{19,1,2,5\\}\\) library(tidyverse) options(digits = 3) sim_jaccard &lt;- \\(a, b) length(intersect(a, b)) / length(union(a, b)) sim_jaccard(c(0,1,2,5,8), c(1,2,5,8,9)) ## [1] 0.667 sim_jaccard(c(2,3,5,8,10), c(1,8,9,10)) ## [1] 0.286 sim_jaccard(c(3,2,5), c(8,9,1,10)) ## [1] 0 3.2 Representación de documentos como conjuntos Hay varias maneras de representar documentos como conjuntos. Las más simples son: Los documentos son colecciones de palabras, o conjuntos de sucesiones de palabras de tamaño \\(n\\). Los documentos son colecciones de caracteres, o conjuntos de sucesiones de caracteres (cadenas) de tamaño \\(k\\). La primera representación se llama representación de n-gramas, y la segunda representación de k-tejas, o \\(k\\)-shingles. Nótese que en ambos casos, representaciones de dos documentos con secciones parecidas acomodadas en distintos lugares tienden a ser similares. Consideremos una colección de textos cortos: textos &lt;- c(&quot;el perro persigue al gato pero no lo alcanza&quot;, &quot;el gato persigue al perro, pero no lo alcanza&quot;, &quot;este es el documento de ejemplo&quot;, &quot;este no es el documento de los ejemplos&quot;, &quot;documento más corto&quot;, &quot;otros animales pueden ser mascotas&quot;) Abajo mostramos la representacion en bolsa de palabras (1-gramas) y la representación en bigramas (2-gramas) de los primeros dos documentos: # Bolsa de palabras (1-gramas) tokenizers::tokenize_ngrams(textos[1:2], n = 1) |&gt; map(unique) ## [[1]] ## [1] &quot;el&quot; &quot;perro&quot; &quot;persigue&quot; &quot;al&quot; &quot;gato&quot; &quot;pero&quot; &quot;no&quot; ## [8] &quot;lo&quot; &quot;alcanza&quot; ## ## [[2]] ## [1] &quot;el&quot; &quot;gato&quot; &quot;persigue&quot; &quot;al&quot; &quot;perro&quot; &quot;pero&quot; &quot;no&quot; ## [8] &quot;lo&quot; &quot;alcanza&quot; # bigramas tokenizers::tokenize_ngrams(textos[1:2], n = 2) |&gt; map(unique) ## [[1]] ## [1] &quot;el perro&quot; &quot;perro persigue&quot; &quot;persigue al&quot; &quot;al gato&quot; ## [5] &quot;gato pero&quot; &quot;pero no&quot; &quot;no lo&quot; &quot;lo alcanza&quot; ## ## [[2]] ## [1] &quot;el gato&quot; &quot;gato persigue&quot; &quot;persigue al&quot; &quot;al perro&quot; ## [5] &quot;perro pero&quot; &quot;pero no&quot; &quot;no lo&quot; &quot;lo alcanza&quot; La representación en k-tejas es otra posibilidad: calcular_tejas &lt;- function(x, k = 2){ tokenizers::tokenize_character_shingles(x, n = k, lowercase = FALSE, simplify = TRUE, strip_non_alpha = FALSE) } # 2-tejas calcular_tejas(textos[1:2], k = 2) |&gt; map(unique) ## [[1]] ## [1] &quot;el&quot; &quot;l &quot; &quot; p&quot; &quot;pe&quot; &quot;er&quot; &quot;rr&quot; &quot;ro&quot; &quot;o &quot; &quot;rs&quot; &quot;si&quot; &quot;ig&quot; &quot;gu&quot; &quot;ue&quot; &quot;e &quot; &quot; a&quot; ## [16] &quot;al&quot; &quot; g&quot; &quot;ga&quot; &quot;at&quot; &quot;to&quot; &quot; n&quot; &quot;no&quot; &quot; l&quot; &quot;lo&quot; &quot;lc&quot; &quot;ca&quot; &quot;an&quot; &quot;nz&quot; &quot;za&quot; ## ## [[2]] ## [1] &quot;el&quot; &quot;l &quot; &quot; g&quot; &quot;ga&quot; &quot;at&quot; &quot;to&quot; &quot;o &quot; &quot; p&quot; &quot;pe&quot; &quot;er&quot; &quot;rs&quot; &quot;si&quot; &quot;ig&quot; &quot;gu&quot; &quot;ue&quot; ## [16] &quot;e &quot; &quot; a&quot; &quot;al&quot; &quot;rr&quot; &quot;ro&quot; &quot;o,&quot; &quot;, &quot; &quot; n&quot; &quot;no&quot; &quot; l&quot; &quot;lo&quot; &quot;lc&quot; &quot;ca&quot; &quot;an&quot; &quot;nz&quot; ## [31] &quot;za&quot; # 4-tejas:&quot; calcular_tejas(textos[1:2], k = 4) |&gt; map(unique) ## [[1]] ## [1] &quot;el p&quot; &quot;l pe&quot; &quot; per&quot; &quot;perr&quot; &quot;erro&quot; &quot;rro &quot; &quot;ro p&quot; &quot;o pe&quot; &quot;pers&quot; &quot;ersi&quot; ## [11] &quot;rsig&quot; &quot;sigu&quot; &quot;igue&quot; &quot;gue &quot; &quot;ue a&quot; &quot;e al&quot; &quot; al &quot; &quot;al g&quot; &quot;l ga&quot; &quot; gat&quot; ## [21] &quot;gato&quot; &quot;ato &quot; &quot;to p&quot; &quot;pero&quot; &quot;ero &quot; &quot;ro n&quot; &quot;o no&quot; &quot; no &quot; &quot;no l&quot; &quot;o lo&quot; ## [31] &quot; lo &quot; &quot;lo a&quot; &quot;o al&quot; &quot; alc&quot; &quot;alca&quot; &quot;lcan&quot; &quot;canz&quot; &quot;anza&quot; ## ## [[2]] ## [1] &quot;el g&quot; &quot;l ga&quot; &quot; gat&quot; &quot;gato&quot; &quot;ato &quot; &quot;to p&quot; &quot;o pe&quot; &quot; per&quot; &quot;pers&quot; &quot;ersi&quot; ## [11] &quot;rsig&quot; &quot;sigu&quot; &quot;igue&quot; &quot;gue &quot; &quot;ue a&quot; &quot;e al&quot; &quot; al &quot; &quot;al p&quot; &quot;l pe&quot; &quot;perr&quot; ## [21] &quot;erro&quot; &quot;rro,&quot; &quot;ro, &quot; &quot;o, p&quot; &quot;, pe&quot; &quot;pero&quot; &quot;ero &quot; &quot;ro n&quot; &quot;o no&quot; &quot; no &quot; ## [31] &quot;no l&quot; &quot;o lo&quot; &quot; lo &quot; &quot;lo a&quot; &quot;o al&quot; &quot; alc&quot; &quot;alca&quot; &quot;lcan&quot; &quot;canz&quot; &quot;anza&quot; Observaciones: Los tokens son las unidades básicas de análisis. Los tokens son palabras para los n-gramas (cuya definición no es del todo simple) y caracteres para las k-tejas. Podrían ser también oraciones, por ejemplo. Nótese que en ambos casos es posible hacer algo de preprocesamiento para obtener la representación. Transformaciones usuales son: Eliminar puntuación y/o espacios. Convertir los textos a minúsculas. Esto incluye decisiones acerca de qué hacer con palabras compuestas (por ejemplo, con un guión), palabras que denotan un concepto (Reino Unido, por ejemplo) y otros detalles. Si lo que nos interesa principalmente similitud textual (no significado, o polaridad, etc.) entre documentos, entonces podemos usar \\(k\\)-tejas, con un mínimo de preprocesamiento. Esta representación es simple y flexible en el sentido de que se puede adaptar para documentos muy cortos (mensajes o tweets, por ejemplo), pero también para documentos más grandes. Por estas razones, no concentramos por el momento en \\(k\\)-tejas Tejas (shingles) Sea \\(k&gt;0\\) un entero. Las \\(k\\)-tejas (\\(k\\)-shingles) de un documento d es el conjunto de todas las corridas (distintas) de \\(k\\) caracteres sucesivos. Escogemos \\(k\\) suficientemente grande, de forma que la probabilidad de que una teja particular ocurra en un texto dado sea relativamente baja. Ejemplo Documentos textualmente similares tienen tejas similares: # calcular tejas textos ## [1] &quot;el perro persigue al gato pero no lo alcanza&quot; ## [2] &quot;el gato persigue al perro, pero no lo alcanza&quot; ## [3] &quot;este es el documento de ejemplo&quot; ## [4] &quot;este no es el documento de los ejemplos&quot; ## [5] &quot;documento más corto&quot; ## [6] &quot;otros animales pueden ser mascotas&quot; tejas_doc &lt;- calcular_tejas(textos, k = 4) # calcular similitud de jaccard entre algunos pares sim_jaccard(tejas_doc[[1]], tejas_doc[[2]]) ## [1] 0.773 sim_jaccard(tejas_doc[[1]], tejas_doc[[3]]) ## [1] 0 sim_jaccard(tejas_doc[[4]], tejas_doc[[5]]) ## [1] 0.156 Podemos calcular todas las similitudes: tejas_tbl &lt;- crossing(id_1 = 1:length(textos), id_2 = 1:length(textos)) |&gt; filter(id_1 &lt; id_2) |&gt; mutate(tejas_1 = tejas_doc[id_1], tejas_2 = tejas_doc[id_2]) |&gt; mutate(sim = map2_dbl(tejas_1, tejas_2, ~sim_jaccard(.x, .y))) |&gt; select(id_1, id_2, sim) tejas_tbl ## # A tibble: 15 × 3 ## id_1 id_2 sim ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 2 0.773 ## 2 1 3 0 ## 3 1 4 0.0137 ## 4 1 5 0 ## 5 1 6 0 ## 6 2 3 0 ## 7 2 4 0.0133 ## 8 2 5 0 ## 9 2 6 0 ## 10 3 4 0.6 ## 11 3 5 0.189 ## 12 3 6 0 ## 13 4 5 0.156 ## 14 4 6 0 ## 15 5 6 0 pero nótese que, como señalamos arriba, esta operación será muy costosa incluso si la colección de textos es de tamaño moderado. Si los textos son cortos, entonces basta tomar valores como \\(k=4,5\\), pues hay un total de \\(27^4\\) tejas de tamaño \\(4\\), y el número de tejas de un documento corto (mensajes, tweets) es mucho más bajo que \\(27^4\\) (nota: ¿puedes explicar por qué este argumento no es exactamente correcto?) Para documentos grandes, como noticias o artículos, es mejor escoger un tamaño más grande, como \\(k=9,10\\), pues en documentos largos puede haber cientos de miles de caracteres. Si \\(k\\) fuera más chica entonces una gran parte de las tejas aparecerá en muchos de los documentos, y todos los documentos tendrían similitud alta. Evitamos escoger \\(k\\) demasiado grande, pues entonces los únicos documentos similares tendrían que tener subcadenas largas exactamente iguales. Por ejemplo: “Batman y Robin” y “Robin y Batman” son algo similares si usamos tejas de tamaño 3, pero son muy distintas si usamos tejas de tamaño 8: Ejemplo tejas_1 &lt;- calcular_tejas(&quot;Batman y Robin&quot;, k = 3) tejas_2 &lt;- calcular_tejas(&quot;Robin y Batman&quot;, k = 3) sim_jaccard(tejas_1, tejas_2) ## [1] 0.6 tejas_1 &lt;- calcular_tejas(&quot;Batman y Robin&quot;, k = 8) tejas_2 &lt;- calcular_tejas(&quot;Robin y Batman&quot;, k = 8) sim_jaccard(tejas_1, tejas_2) ## [1] 0 3.3 Representación matricial Podemos usar una matriz binaria para guardar todas las representaciones en k-tejas de nuestra colección de documentos. Puede usarse una representación rala (sparse) si es necesario: dtejas_tbl &lt;- tibble(id = paste0(&quot;doc_&quot;, 1:length(textos)), tejas = tejas_doc) |&gt; unnest(cols = tejas) |&gt; unique() |&gt; mutate(val = 1) |&gt; pivot_wider(names_from = id, values_from = val, values_fill = list(val = 0)) |&gt; arrange(tejas) # opcionalmente ordenamos tejas dtejas_tbl ## # A tibble: 123 × 7 ## tejas doc_1 doc_2 doc_3 doc_4 doc_5 doc_6 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot; al &quot; 1 1 0 0 0 0 ## 2 &quot; alc&quot; 1 1 0 0 0 0 ## 3 &quot; ani&quot; 0 0 0 0 0 1 ## 4 &quot; cor&quot; 0 0 0 0 1 0 ## 5 &quot; de &quot; 0 0 1 1 0 0 ## 6 &quot; doc&quot; 0 0 1 1 0 0 ## 7 &quot; eje&quot; 0 0 1 1 0 0 ## 8 &quot; el &quot; 0 0 1 1 0 0 ## 9 &quot; es &quot; 0 0 1 1 0 0 ## 10 &quot; gat&quot; 1 1 0 0 0 0 ## # … with 113 more rows ¿Cómo calculamos la similitud de Jaccard usando estos datos? Calcular la unión e intersección se puede hacer haciendo OR y AND de las columnas, y entonces podemos calcular la similitud inter_12 &lt;- sum(dtejas_tbl$doc_1 &amp; dtejas_tbl$doc_2) union_12 &lt;- sum(dtejas_tbl$doc_1 | dtejas_tbl$doc_2) similitud &lt;- inter_12/union_12 similitud # comparar con el número que obtuvimos arriba. ## [1] 0.773 El cálculo para todos los documentos podríamos hacerlo (aunque veremos que normalmente no haremos esto si no necesitamos calcular todas las similitudes) con: mat_td &lt;- dtejas_tbl |&gt; select(-tejas) |&gt; as.matrix() |&gt; t() 1 - dist(mat_td, method = &quot;binary&quot;) ## doc_1 doc_2 doc_3 doc_4 doc_5 ## doc_2 0.7727 ## doc_3 0.0000 0.0000 ## doc_4 0.0137 0.0133 0.6000 ## doc_5 0.0000 0.0000 0.1892 0.1556 ## doc_6 0.0000 0.0000 0.0000 0.0000 0.0000 3.4 Minhash y reducción probabilística de dimensionalidad Para una colección grande de documentos la representación binaria de la colección de documentos puede tener un número muy grande de renglones. Puede ser posible crear un número más chico de nuevos features (ojo: aquí los renglones son las “variables,” y los casos son las columnas) con los que sea posible obtener una buena aproximación de la similitud. La idea básica es la siguiente: Escogemos una función al azar (una función hash) que mapea cadenas cortas a un número grande de enteros, de manera existe muy baja probabilidad de colisiones, y no hay correlación entre las cadenas y el valor al que son mapeados. Si un documento tiene tejas \\(T\\), aplicamos la función hash a cada teja de \\(T\\), y calculamos el mínimo de estos valores hash. Repetimos este proceso para varias funciones hash fijas, por ejemplo \\(k= 5\\) Los valores mínimos obtenidos nos dan una representación en dimensión baja de cada documento. Ejemplo textos_tbl &lt;- tibble(doc_id = 1:length(textos), texto = textos) tejas_tbl &lt;- tibble(doc_id = 1:length(textos), tejas = tejas_doc) tejas_tbl ## # A tibble: 6 × 2 ## doc_id tejas ## &lt;int&gt; &lt;list&gt; ## 1 1 &lt;chr [41]&gt; ## 2 2 &lt;chr [42]&gt; ## 3 3 &lt;chr [28]&gt; ## 4 4 &lt;chr [36]&gt; ## 5 5 &lt;chr [16]&gt; ## 6 6 &lt;chr [31]&gt; Creamos una función hash: set.seed(813) generar_hash &lt;- function(){ r &lt;- as.integer(stats::runif(1, 1, 2147483647)) funcion_hash &lt;- function(tejas){ digest::digest2int(tejas, seed = r) } funcion_hash } h_1 &lt;- generar_hash() Y aplicamos la función a cada teja del documento 1, y tomamos el mínimo: hashes_1 &lt;- h_1(tejas_tbl$tejas[[1]]) hashes_1 ## [1] -1318809190 -1534091290 -1401861150 -1601894665 781339434 -519860631 ## [7] 2116727945 -1824301917 -1401861150 -1371561364 -1385084918 -821046029 ## [13] 1766711521 1952075680 569109680 -1412107908 1059544482 -1866546594 ## [19] 2090868926 -455965089 784118133 1421354549 -1397404756 -481987742 ## [25] -1824301917 -1401861150 1157451749 698137483 -623448608 544014704 ## [31] -587926856 1455454405 950121497 -165719415 2119479774 2115904977 ## [37] 1519310299 1652203243 -667865842 1219377605 -1500087628 minhash_1 &lt;- min(hashes_1) minhash_1 ## [1] -1866546594 Consideramos este minhash como un descriptor del documento. Generalmente usamos más de un descriptor. En el siguiente ejemplo usamos 4 funciones hash creadas de manera independiente: hashes &lt;- map(1:4, ~ generar_hash()) docs_firmas &lt;- tejas_tbl |&gt; mutate(firma = map(tejas, \\(lista) map_int(hashes, \\(h) min(h(lista))))) |&gt; select(doc_id, firma) |&gt; unnest_wider(firma, names_sep = &quot;_&quot;) docs_firmas ## # A tibble: 6 × 5 ## doc_id firma_1 firma_2 firma_3 firma_4 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 -2129712961 -2124594172 -2073157333 -1982715048 ## 2 2 -2129712961 -2124594172 -2073157333 -1982715048 ## 3 3 -2139075502 -2093452194 -1959662839 -2048484934 ## 4 4 -2139075502 -2093452194 -1959662839 -2048484934 ## 5 5 -2106131386 -2093452194 -2127642881 -1984764210 ## 6 6 -2115397946 -2087727654 -2074217109 -1986993146 Nótese ahora que en documentos muy similares, varios de los minhashes coinciden. Esto es porque la teja donde ocurrió el mínimo está en los dos documentos. Entonces cuando los las tejas de dos documentos son muy similares, es muy probable que sus minhashes coincidan. ¿Cuál es la probabilidad de que la firma coincida para un documento? Sea \\(h\\) una función hash escogida escogida al azar, y \\(a\\) y \\(b\\) dos documentos dados dadas. Denotamos como \\(f_h\\) la función minhash asociada a \\(h\\). Entonces \\[P(f_h(a) = f_h(b)) = sim(a, b)\\] donde \\(sim\\) es la similitud de Jaccard basada en las tejas usadas. Sean \\(h_1, h_2, \\ldots h_n\\) funciones hash escogidas al azar de manera independiente. Si \\(n\\) es grande, entonces por la ley de los grandes números \\[sim(a,b) \\approx \\frac{|h_j : f_{h_j}{\\pi_j}(a) = f_{h_j}(b)|}{n},\\] es decir, la similitud de Jaccard es aproximadamente la proporción de elementos de las firmas que coinciden. Ahora damos un argumento para demostrar este resultado: Supongamos que el total de tejas de los dos documentos es \\(|A\\cup B|\\), y el número de tejas que tienen en común es \\(|A\\cap B|\\). Sea \\(h\\) la función hash que escogimos al azar. Para fijar ideas, puedes suponer que las tejas están numeradas \\(1,\\ldots, M\\), y la función hash es una permutación aleatoria de estos números. Entonces: El mínimo de \\(h\\) puede ocurrir en cualquier elemento de \\(|A\\cup B|\\) con la misma probabilidad. Los minhashes de \\(a\\) y \\(b\\) coinciden si y sólo si el mínimo de \\(h\\) ocurre en un elemento de \\(|A\\cap B|\\) Por 1 y 2, la probabilidad de que esto ocurra es \\[\\frac{|A\\cap B|}{|A\\cup B|},\\] que es la similitud de Jaccard. Nótese que esto requiere que la familia de donde escogemos nuestra función hash cumple, al menos aproximadamente, las propiedades 1 y 2. Para que 1 ocurra, la familia debe ser suficientemente grande y variada: por ejemplo, esto fallaría si todas las cadenas que comienzan con “a” se mapean a números chicos. Para que ocurra 2, no debe haber colisiones (cadenas distintas que se mapean al mismo valor). Observaciónes: Una familia que cumple de manera exacta estas dos propiedades es la familia de permutaciones que mencionamos arriba: numeramos las tejas, construimos una permutación al azar, y luego aplicamos esta función de permutaciones a los índices de las tejas. La razón por la que esta familia no es utiliza típicamente es porque es costosa si el número de tejas es grande: primero hay que escoger un ordenamiento al azar, y luego es necesario almacenarlo. Muchas veces, se utiliza una función hash con aritmética modular como sigue: sea \\(M\\) el número total de tejas, y sea \\(p\\) un número primo fijo grande (al menos \\(p &gt; M\\)). Numeramos las tejas. Ahora escogemos dos enteros \\(a\\) y \\(b\\) al azar, y hacemos \\[h(x) = (ax + b\\mod p) \\mod M\\] Estas funciones se pueden seleccionar y aplicar rápidamente, y sólo tenemos que almacenar los coeficientes \\(a\\) y \\(b\\). En el enfoque que vimos arriba, utilizamos directamente una función hash de cadenas que está diseñada para cumplir 1 y 2 de manera aproximada. Resumen. Con el método de minhash, representamos a los documentos con un número relativamente chico de atributos numéricos (reducción de dimensionalidad). Esta respresentación tiene la propiedad de que textos muy similares con probabilidad alta coinciden en uno o más de los descriptores. 3.5 Agrupando textos de similitud alta Nuestro siguiente paso es evitar hacer la comparación de todos los pares de descriptores. Para esto hacemos un clustering no exhaustivo basado en los descriptores que acabamos de construir. Recordemos que tenemos docs_firmas ## # A tibble: 6 × 5 ## doc_id firma_1 firma_2 firma_3 firma_4 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 -2129712961 -2124594172 -2073157333 -1982715048 ## 2 2 -2129712961 -2124594172 -2073157333 -1982715048 ## 3 3 -2139075502 -2093452194 -1959662839 -2048484934 ## 4 4 -2139075502 -2093452194 -1959662839 -2048484934 ## 5 5 -2106131386 -2093452194 -2127642881 -1984764210 ## 6 6 -2115397946 -2087727654 -2074217109 -1986993146 Ahora agrupamos documentos que comparten alguna firma. A los grupos que coinciden en cada firma les llamamos cubetas: cubetas_tbl &lt;- docs_firmas |&gt; pivot_longer(contains(&quot;firma_&quot;), &quot;n_firma&quot;) |&gt; mutate(cubeta = paste(n_firma, value)) |&gt; group_by(cubeta) |&gt; summarise(documentos = list(doc_id)) |&gt; mutate(num_docs = map_int(documentos, length)) cubetas_tbl ## # A tibble: 15 × 3 ## cubeta documentos num_docs ## &lt;chr&gt; &lt;list&gt; &lt;int&gt; ## 1 firma_1 -2106131386 &lt;int [1]&gt; 1 ## 2 firma_1 -2115397946 &lt;int [1]&gt; 1 ## 3 firma_1 -2129712961 &lt;int [2]&gt; 2 ## 4 firma_1 -2139075502 &lt;int [2]&gt; 2 ## 5 firma_2 -2087727654 &lt;int [1]&gt; 1 ## 6 firma_2 -2093452194 &lt;int [3]&gt; 3 ## 7 firma_2 -2124594172 &lt;int [2]&gt; 2 ## 8 firma_3 -1959662839 &lt;int [2]&gt; 2 ## 9 firma_3 -2073157333 &lt;int [2]&gt; 2 ## 10 firma_3 -2074217109 &lt;int [1]&gt; 1 ## 11 firma_3 -2127642881 &lt;int [1]&gt; 1 ## 12 firma_4 -1982715048 &lt;int [2]&gt; 2 ## 13 firma_4 -1984764210 &lt;int [1]&gt; 1 ## 14 firma_4 -1986993146 &lt;int [1]&gt; 1 ## 15 firma_4 -2048484934 &lt;int [2]&gt; 2 Ahora filtramos las cubetas que tienen más de un elemento: cubetas_tbl &lt;- cubetas_tbl |&gt; filter(num_docs &gt; 1) cubetas_tbl ## # A tibble: 8 × 3 ## cubeta documentos num_docs ## &lt;chr&gt; &lt;list&gt; &lt;int&gt; ## 1 firma_1 -2129712961 &lt;int [2]&gt; 2 ## 2 firma_1 -2139075502 &lt;int [2]&gt; 2 ## 3 firma_2 -2093452194 &lt;int [3]&gt; 3 ## 4 firma_2 -2124594172 &lt;int [2]&gt; 2 ## 5 firma_3 -1959662839 &lt;int [2]&gt; 2 ## 6 firma_3 -2073157333 &lt;int [2]&gt; 2 ## 7 firma_4 -1982715048 &lt;int [2]&gt; 2 ## 8 firma_4 -2048484934 &lt;int [2]&gt; 2 Y de aquí extraemos pares candidatos que tienen alta probabilidad de ser muy similares: pares_tbl &lt;- cubetas_tbl |&gt; mutate(pares_cand = map(documentos, ~ combn(.x, 2, simplify = FALSE))) |&gt; select(cubeta, pares_cand) |&gt; unnest(pares_cand) |&gt; unnest_wider(pares_cand, names_sep = &quot;_&quot;) pares_tbl ## # A tibble: 10 × 3 ## cubeta pares_cand_1 pares_cand_2 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 firma_1 -2129712961 1 2 ## 2 firma_1 -2139075502 3 4 ## 3 firma_2 -2093452194 3 4 ## 4 firma_2 -2093452194 3 5 ## 5 firma_2 -2093452194 4 5 ## 6 firma_2 -2124594172 1 2 ## 7 firma_3 -1959662839 3 4 ## 8 firma_3 -2073157333 1 2 ## 9 firma_4 -1982715048 1 2 ## 10 firma_4 -2048484934 3 4 pares_tbl &lt;- pares_tbl |&gt; select(-cubeta) |&gt; unique() pares_tbl ## # A tibble: 4 × 2 ## pares_cand_1 pares_cand_2 ## &lt;int&gt; &lt;int&gt; ## 1 1 2 ## 2 3 4 ## 3 3 5 ## 4 4 5 Nótese que con este proceso evitamos hacer todas las comparaciones, y el método tiene complejidad lineal en el tamaño de la colección de documentos. Una vez que tenemos los pares, podemos calcular la similitud exacta de solamente esos documentos: pares_tbl |&gt; left_join(tejas_tbl |&gt; rename(pares_cand_1 = doc_id, texto_1 = tejas)) |&gt; left_join(tejas_tbl |&gt; rename(pares_cand_2 = doc_id, texto_2 = tejas)) |&gt; mutate(score = map2_dbl(texto_1, texto_2, ~ sim_jaccard(.x, .y))) |&gt; select(-contains(&quot;texto&quot;)) ## Joining, by = &quot;pares_cand_1&quot; ## Joining, by = &quot;pares_cand_2&quot; ## # A tibble: 4 × 3 ## pares_cand_1 pares_cand_2 score ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 2 0.773 ## 2 3 4 0.6 ## 3 3 5 0.189 ## 4 4 5 0.156 Si queremos capturar solamente aquellos pares de similitud muy alta, podemos también combinar firmas para formar cubetas donde las dos firmas coinciden: cubetas_tbl &lt;- docs_firmas |&gt; mutate(cubeta = paste(firma_1, firma_2)) |&gt; group_by(cubeta) |&gt; summarise(documentos = list(doc_id)) |&gt; mutate(num_docs = map_int(documentos, length)) cubetas_tbl ## # A tibble: 4 × 3 ## cubeta documentos num_docs ## &lt;chr&gt; &lt;list&gt; &lt;int&gt; ## 1 -2106131386 -2093452194 &lt;int [1]&gt; 1 ## 2 -2115397946 -2087727654 &lt;int [1]&gt; 1 ## 3 -2129712961 -2124594172 &lt;int [2]&gt; 2 ## 4 -2139075502 -2093452194 &lt;int [2]&gt; 2 pares_tbl &lt;- cubetas_tbl |&gt; filter(num_docs &gt; 1) |&gt; mutate(pares_cand = map(documentos, ~ combn(.x, 2, simplify = FALSE))) |&gt; select(cubeta, pares_cand) |&gt; unnest(pares_cand) |&gt; unnest_wider(pares_cand, names_sep = &quot;_&quot;) |&gt; left_join(tejas_tbl |&gt; rename(pares_cand_1 = doc_id, texto_1 = tejas)) |&gt; left_join(tejas_tbl |&gt; rename(pares_cand_2 = doc_id, texto_2 = tejas)) |&gt; mutate(score = map2_dbl(texto_1, texto_2, ~ sim_jaccard(.x, .y))) |&gt; select(-contains(&quot;texto&quot;)) ## Joining, by = &quot;pares_cand_1&quot; ## Joining, by = &quot;pares_cand_2&quot; pares_tbl ## # A tibble: 2 × 4 ## cubeta pares_cand_1 pares_cand_2 score ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 -2129712961 -2124594172 1 2 0.773 ## 2 -2139075502 -2093452194 3 4 0.6 3.6 Ejemplo: tweets Ahora buscaremos tweets similares en una colección de un dataset de kaggle. ruta &lt;- &quot;../datos/FIFA.csv&quot; if(!file.exists(ruta)){ fifa &lt;- read_csv(&quot;https://fifatweets.s3.amazonaws.com/FIFA.csv&quot;) write_csv(fifa, &quot;../datos/FIFA.csv&quot;) } else { fifa &lt;- read_csv(ruta) } ## Rows: 530000 Columns: 16 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (9): lang, Source, Orig_Tweet, Tweet, Hashtags, UserMentionNames, UserM... ## dbl (6): ID, len, Likes, RTs, Followers, Friends ## dttm (1): Date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. tw &lt;- fifa$Tweet tw[1:10] ## [1] &quot;Only two goalkeepers have saved three penalties in penalty shoot out Ricardo vs&quot; ## [2] &quot;scores the winning penalty to send into the quarter finals where they will face Russia&quot; ## [3] &quot;Tonight we have big game&quot; ## [4] &quot;We get stronger Turn the music up now We got that power power&quot; ## [5] &quot;Only two goalkeepers have saved three penalties in penalty shoot out Ricardo vs&quot; ## [6] &quot;We re looking strong going into the knockout stage We caught up with ahead of&quot; ## [7] &quot;am happy for winning Especially since you know we colluded and all Russia eliminates Spain after penalty&quot; ## [8] &quot;When you see me When we feel the same feeling Power power&quot; ## [9] &quot;Kasper Schmeichel takes the final award of the day&quot; ## [10] &quot;After Years Global Puma Ambassador LG Mobile Ambassador CocaCola WorldCup Kookmin Bank UNICEF&quot; set.seed(9192) num_tweets &lt;- 100000 system.time(tejas_doc &lt;- calcular_tejas(tw[1:num_tweets], k = 5)) ## user system elapsed ## 2.610 0.048 2.658 tejas_tbl &lt;- tibble(doc_id = 1:num_tweets, tejas = tejas_doc) hash_f &lt;- map(1:50, ~ generar_hash()) system.time( docs_firmas &lt;- tejas_tbl |&gt; mutate(firma = map(tejas, \\(lista) map_int(hash_f, \\(h) min(h(lista))))) |&gt; select(doc_id, firma)) ## user system elapsed ## 25.292 0.038 25.333 docs_firmas ## # A tibble: 100,000 × 2 ## doc_id firma ## &lt;int&gt; &lt;list&gt; ## 1 1 &lt;int [50]&gt; ## 2 2 &lt;int [50]&gt; ## 3 3 &lt;int [50]&gt; ## 4 4 &lt;int [50]&gt; ## 5 5 &lt;int [50]&gt; ## 6 6 &lt;int [50]&gt; ## 7 7 &lt;int [50]&gt; ## 8 8 &lt;int [50]&gt; ## 9 9 &lt;int [50]&gt; ## 10 10 &lt;int [50]&gt; ## # … with 99,990 more rows La firma de minhashes del primer documento es por ejemplo: docs_firmas$firma[1] ## [[1]] ## [1] -1953455759 -2112001395 -2031432865 -2108080560 -2095602723 -2118858341 ## [7] -2099337985 -1978298401 -2092751862 -2097330616 -1897306653 -2139645671 ## [13] -1981364640 -2081406116 -2127003388 -2138613184 -2146084782 -2082312909 ## [19] -2083329993 -2001255852 -2091106849 -2091293600 -2120806067 -2095725804 ## [25] -2143199361 -2091159512 -2124095195 -2038461037 -2109684884 -2073791594 ## [31] -2129542593 -1904599938 -2118234795 -2056298394 -2105031889 -2113931139 ## [37] -2134167293 -2140440957 -1717523835 -2144768923 -2138027225 -2084388635 ## [43] -2110278901 -2135431955 -2139423769 -2130397390 -2097742891 -2105787557 ## [49] -2101425996 -1941363605 Y probaremos primero hacer cubetas con algunas las firmas (las cuatro primeras por ejemplo): docs_firmas &lt;- docs_firmas |&gt; mutate(cubeta_nombre = map_chr(firma, \\(x) paste(x[1:3], collapse = &quot;-&quot;))) docs_cubetas_tbl &lt;- docs_firmas |&gt; group_by(cubeta_nombre) |&gt; summarise(docs = list(doc_id)) |&gt; mutate(num_docs = map_int(docs, length)) docs_cubetas_filt_tbl &lt;- docs_cubetas_tbl |&gt; filter(num_docs &gt; 1) Y examinamos ahora algunas de las cubetas: docs_ejemplo &lt;- docs_cubetas_filt_tbl$docs[[125]] tw[docs_ejemplo] ## [1] &quot;We got That power power&quot; &quot;We got That power power&quot; ## [3] &quot;We got That power power&quot; docs_ejemplo &lt;- docs_cubetas_filt_tbl$docs[[1658]] length(docs_ejemplo) ## [1] 406 tw[docs_ejemplo][1:2] ## [1] &quot;short story involving Nigeria Argentina and France&quot; ## [2] &quot;short story involving Nigeria Argentina and France&quot; docs_ejemplo &lt;- docs_cubetas_filt_tbl$docs[[4958]] tw[docs_ejemplo] ## [1] &quot;The keeps on serving us more and more drama If you missed any of the great moments from todays fixtures yo&quot; ## [2] &quot;The keeps on serving us more and more drama If you missed any of the great moments from todays fixtures yo&quot; ## [3] &quot;The keeps on serving us more and more drama If you missed any of the great moments from todays fixtures yo&quot; ## [4] &quot;The keeps on serving us more and more drama If you missed any of the great moments from todays fixtures you can watch the highlights on&quot; ## [5] &quot;The keeps on serving us more and more drama If you missed any of the great moments from todays fixtures you can watch the highlights on&quot; Con este método, podemos extraer pares de muy alta similitud (cercano a duplicados) de forma eficiente en colecciones grandes de texto. 3.7 Verificar si un nuevo elemento es duplicado Si tenemos un nuevo entrada, podemos checar si es duplicado calculando su firma, formando la cubetas, y revisando si cae en alguna de las cubetas conocidas. Nótese que tenemos que guardar las funciones hash que usamos para aplicar a los nuevos datos, y repetir el proceso exactamente como procesamos los datos originales. Por ejemplo: nuevos_tweets &lt;- 150000:150005 tw[nuevos_tweets] ## [1] &quot;It should sound the song that was on the Show of the great fountain of Dubai Please play of&quot; ## [2] &quot;PARK CHANYEOL Please play of&quot; ## [3] &quot;What Game Congratulations to on reaching the quarter finals with but commiserations to Marcos&quot; ## [4] &quot;For the fans of the vs rivalry Mbappe Cristiano Ronaldo fan has ended Messis dream of winning the&quot; ## [5] &quot;Messi How far Suarez could not make it to the quarterfinals you know what to do Suarez Roger&quot; ## [6] &quot;Got few daubes all right had to put him in when was painting low Thankfully painting finished another bit later after the&quot; system.time(tejas_nuevas_doc &lt;- calcular_tejas(tw[nuevos_tweets], k = 5)) ## user system elapsed ## 0.001 0.000 0.000 tejas_nuevas_tbl &lt;- tibble(doc_id = nuevos_tweets, tejas = tejas_nuevas_doc) system.time( docs_nuevas_firmas &lt;- tejas_nuevas_tbl |&gt; mutate(firma = map(tejas, \\(lista) map_int(hash_f, \\(h) min(h(lista))))) |&gt; select(doc_id, firma)) ## user system elapsed ## 0.007 0.000 0.006 docs_nuevas_firmas ## # A tibble: 6 × 2 ## doc_id firma ## &lt;int&gt; &lt;list&gt; ## 1 150000 &lt;int [50]&gt; ## 2 150001 &lt;int [50]&gt; ## 3 150002 &lt;int [50]&gt; ## 4 150003 &lt;int [50]&gt; ## 5 150004 &lt;int [50]&gt; ## 6 150005 &lt;int [50]&gt; docs_nuevas_firmas &lt;- docs_nuevas_firmas |&gt; mutate(cubeta_nombre = map_chr(firma, \\(x) paste(x[1:3], collapse = &quot;-&quot;))) docs_cubetas_nuevas_tbl &lt;- docs_nuevas_firmas |&gt; group_by(cubeta_nombre) |&gt; summarise(docs = list(doc_id)) |&gt; mutate(num_docs = map_int(docs, length)) docs_cubetas_nuevas_tbl ## # A tibble: 6 × 3 ## cubeta_nombre docs num_docs ## &lt;chr&gt; &lt;list&gt; &lt;int&gt; ## 1 -2041983150--2147447475--2129777399 &lt;int [1]&gt; 1 ## 2 -2051683951--2053274679--2125528528 &lt;int [1]&gt; 1 ## 3 -2108824716--2032083428--2137254712 &lt;int [1]&gt; 1 ## 4 -2113147634--2140751096--2058622652 &lt;int [1]&gt; 1 ## 5 -2139603446--2140751096--2090996001 &lt;int [1]&gt; 1 ## 6 -2142045021--2054826468--2129777399 &lt;int [1]&gt; 1 Y ahora podemos hacer un semi-join con las cubetas: cand_duplicados_tbl &lt;- docs_cubetas_nuevas_tbl |&gt; semi_join(docs_cubetas_tbl |&gt; select(cubeta_nombre)) ## Joining, by = &quot;cubeta_nombre&quot; cand_duplicados_tbl ## # A tibble: 3 × 3 ## cubeta_nombre docs num_docs ## &lt;chr&gt; &lt;list&gt; &lt;int&gt; ## 1 -2041983150--2147447475--2129777399 &lt;int [1]&gt; 1 ## 2 -2113147634--2140751096--2058622652 &lt;int [1]&gt; 1 ## 3 -2142045021--2054826468--2129777399 &lt;int [1]&gt; 1 Y vemos que tenemos tres candidatos a duplicados. Checamos el primer tweet: cubeta_1 &lt;- cand_duplicados_tbl |&gt; pull(cubeta_nombre) |&gt; pluck(1) doc_1 &lt;- cand_duplicados_tbl |&gt; pull(docs) |&gt; pluck(1) cubeta_1 ## [1] &quot;-2041983150--2147447475--2129777399&quot; doc_1 ## [1] 150004 # en la tabla original de todos los tweets doc_2 &lt;- filter(docs_cubetas_tbl, cubeta_nombre == cubeta_1) |&gt; pull(docs) |&gt; pluck(1) doc_2[1] ## [1] 36839 Checamos los tweets: tw[doc_2[1]] ## [1] &quot;Messi How far Suarez could not make it to the quarterfinals you know what to do Suarez Roger&quot; tw[doc_1] ## [1] &quot;Messi How far Suarez could not make it to the quarterfinals you know what to do Suarez Roger&quot; Y efectivamente detectamos que el nuevo tweet es duplicado. Otra vez, no fue necesario hacer una comparación exhaustiva del nuevo tweet contra nuestra colección inicial grande. Ejercicio: checa los otros candidatos a duplicados que encontramos en este ejemplo. 3.8 Controlando la sensibilidad y umbral de similitud En algunos casos nos pueden interesar encontrar duplicados muy cercanos, y en otros problemas quisiéramos capturar pares con un umbral más bajo de similitud. Usando distinto número de hashes podemos hacer esto. En primer lugar, sabemos que un la probabilidad de que dos minhashes coincidan es igual a la similitud de jaccard de los dos textos correspondientes. Si \\(s\\) es la similitud de Jaccard entre \\(a\\) y \\(b\\), entonces: \\[P(f_h(a) = f_h(b)) = s(a,b) = s\\] Si escogemos \\(k\\) hashes al azar, y definimos candidatos solamente cuando coincidan todos los hashes (construcción AND), la probabilidad de hacer un par candidatos es: \\[P(f_1(a) = f_1(b), \\ldots, f_k(a)=f_k(b)) = s^k\\] Por otro lado, si consideramos un par candidato cuando al menos alguno de los minhashes coinciden (construcción OR), la probabilidad que \\(a\\) y \\(b\\) sean candidatos es: \\[P(f_j(a) = f_j(b) \\, \\textrm{ para alguna } j) = 1-(1-s)^k\\] Podemos graficar estas probabilidades con distintos valores de \\(k\\) k &lt;- 3 prob_captura &lt;- tibble(s = seq(0, 1, 0.01)) |&gt; crossing(tipo = c(&quot;k_hashes_AND&quot;, &quot;k_hashes_OR&quot;)) |&gt; crossing(k = c(1, 3, 5, 10)) |&gt; mutate(prob_captura = ifelse(tipo == &quot;k_hashes_AND&quot;, s^k, 1 - (1 - s)^k)) ggplot(prob_captura, aes(x = s, y = prob_captura, colour = factor(k))) + geom_point() + facet_wrap(~ tipo) Nótese que: Si queremos tener muy alta probabilidad de capturar todos los pares similares, podemos usar la construcción OR con k = 3 para similitud mayor a 0.75 o k = 10 para similitud mayor a 0.3, por ejemplo La desventaja de esto, es que es posible que obtengamos muchos candidatos que realmente no tienen la similitud deseada (falsos positivos). Esto implica más procesamiento. Si solo queremos capturar pares de muy alta similitud (por ejemplo &gt; 0.98), y no es grave que tengamos algunos falsos negativos, podemos usar la construcción AND con 1 o 3 hashes por ejemplo, y obtendremos un número más manejable de pares candidatos para procesar. La desventaja de esto es que es posible obtener algunos falsos negativos. Depende de la aplicación esto puede ser aceptable o no. A partir de un umbral \\(s\\) de similitud para los pares que queremos capturar, podemos usar varios minhashes para afinar el método: Usamos la construcción OR con varios hashes para capturar pares de alta o mediana similitud con mucha confianza. Generalmente es necesario filtrar falsos positivos. Usamos la construcción AND con uno o varios hashes para pares de muy alta similitud con confianza alta. Tendremos menos falsos positivos, pero también es posible tener más falsos negativos (se nos escapan algunos pares muy similares). Nota: es posible combinar estas técnicas para refinar la captura de pares a un nivel de similitud dado haciendo bandas de hashes: por ejemplo, si tenemos 20 hashes podemos hacer 4 bandas de 5 hashes, usamos AND para cada grupo de 5 hashes y OR para las 4 bandas. Para más de esto, revisa nuestro texto (Leskovec, Rajaraman, and Ullman 2014). 3.9 Distancia euclideana y LSH Ahora aplicamos estas ideas para otro tipo de similitud. En este caso, consideramos la distancia euclideana usual en dimensión \\(p\\): \\[d(x,y) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 +\\cdots (x_p - y_p)^2}\\] Y nuestra tarea es encontrar todos los pares tales que su distancia euclideana es muy cercana a 0. Es decir, queremos hacer “clusters” pero sólo de datos muy cercanos, igual que en los ejemplos de similitud de jaccard. Para distancia euclideana nuestros hashes resultan de proyecciones aleatorias rectas fijas en cubetas. La idea general es que tomamos una línea al azar en el espacio de entradas, y la dividimos en cubetas de manera uniforme. El valor hash de un punto \\(x\\) es el número de cubeta donde cae la proyección de \\(x\\). Más especificamente, si escogemos un ancho \\(r\\) de cubeta: Escogemos una dirección al azar \\(v\\) El hash de un punto \\(x\\) se calcula como sigue: Calculamos el tamaño de la proyección \\(x\\) sobre \\(v\\) Dividimos este tamaño entre \\(r\\) el hash la parte entera de este último número Es decir, \\[h(x) = \\left\\lfloor{ \\frac{x\\cdot v}{r}}\\right\\rfloor\\] Ejercicio: haz un dibujo de este proceso, y cómo se calcula el hash de un punto \\(x\\) una vez que tienes \\(r\\) y \\(v\\). Por ejemplo, si \\(v=(1,2)\\), y \\(r = 1\\): library(tidyverse) norma &lt;- function(x) sqrt(sum(x^2)) v &lt;- c(1,2) / norma(c(1,2)) v ## [1] 0.447 0.894 hash_1 &lt;- function(x) floor(sum(x * v) / 1) hash_1(c(5,0)) ## [1] 2 hash_1(c(0,-1)) ## [1] -1 Construimos ahora nuestra función generadora de hashes: gen_hash &lt;- function(p, r){ v &lt;- rnorm(p) v &lt;- v / norma(v) # devolvemos una función que calcula la cubeta: function(x){ floor(sum(x * v) / r) |&gt; as.integer() } } set.seed(823) hash_1 &lt;- gen_hash(2, 1) # los hashes de dos puntos: hash_1(c(4, 7)) ## [1] -7 hash_1(c(-4, 7)) ## [1] 0 # el vector que escogimos es environment(hash_1)$v ## [1] -0.914 -0.405 Ejemplo La siguiente función genera dos clusters de puntos mezclados con puntos distribuidos normales con desviación estándar relativamente grande set.seed(1021) simular_puntos &lt;- function(d = 2, n = 200){ #puntos muy cercanos a (3,3,..., 3): mat_1 &lt;- matrix(rnorm(10 * d, sd = 0.01) + 3, ncol = d) #puntos muy cercanos a (-3,-3,..., -3): mat_2 &lt;- matrix(rnorm(10 * d, sd = 0.01) - 3, ncol = d) # puntos distribuidos alrededor del origen: mat_3 &lt;- matrix(rnorm(n * d, sd = 10), ncol = d) datos_tbl_vars &lt;- rbind(mat_3, mat_1, mat_2) |&gt; as_tibble() |&gt; mutate(id_1 = row_number()) datos_tbl_vars } # diez puntos en cluster 1, diez en cluster 2, y 100 sin cluster: datos_tbl_vars &lt;- simular_puntos(d = 2, n = 100) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ggplot(datos_tbl_vars, aes(x = V1, y= V2)) + geom_jitter(width = 0.5, height = 0.5, alpha = 0.3) Para este ejemplo calculamos las distancias reales: dist_e &lt;- function(x, y){ norma(x - y) } datos_tbl &lt;- datos_tbl_vars |&gt; pivot_longer(-id_1, names_to = &quot;variable&quot;, values_to = &quot;valor&quot;) |&gt; group_by(id_1) |&gt; arrange(variable) |&gt; summarise(vec_1 = list(valor)) system.time( pares_tbl &lt;- datos_tbl |&gt; crossing(datos_tbl |&gt; rename(id_2 = id_1, vec_2 = vec_1)) |&gt; filter(id_1 &lt; id_2) |&gt; mutate(dist = map2_dbl(vec_1, vec_2, dist_e)) ) ## user system elapsed ## 0.024 0.000 0.025 pares_tbl |&gt; head() ## # A tibble: 6 × 5 ## id_1 vec_1 id_2 vec_2 dist ## &lt;int&gt; &lt;list&gt; &lt;int&gt; &lt;list&gt; &lt;dbl&gt; ## 1 1 &lt;dbl [2]&gt; 2 &lt;dbl [2]&gt; 7.07 ## 2 1 &lt;dbl [2]&gt; 3 &lt;dbl [2]&gt; 24.2 ## 3 1 &lt;dbl [2]&gt; 4 &lt;dbl [2]&gt; 29.0 ## 4 1 &lt;dbl [2]&gt; 5 &lt;dbl [2]&gt; 24.9 ## 5 1 &lt;dbl [2]&gt; 6 &lt;dbl [2]&gt; 3.31 ## 6 1 &lt;dbl [2]&gt; 7 &lt;dbl [2]&gt; 16.2 nrow(pares_tbl) ## [1] 7140 Supongamos que queremos encontrar los puntos que están a distancia menor a 1: pares_sim &lt;- pares_tbl |&gt; filter(dist &lt; 1) nrow(pares_sim) ## [1] 103 Ahora veremos cómo encontrar estos 103 pares de puntos cercanos. Cálculo de firmas Usaremos 4 hashes con tamaño de cubeta = 0.2: #generar hashes hash_f &lt;- map(1:4, ~ gen_hash(p = 2, r = 0.2)) # esta es una función de conveniencia: calculador_hashes &lt;- function(hash_f){ function(z) { map_int(hash_f, ~ .x(z)) } } calc_hashes &lt;- calculador_hashes(hash_f) Calculamos las firmas: firmas_tbl &lt;- datos_tbl_vars |&gt; pivot_longer(cols = -id_1, names_to = &quot;variable&quot;, values_to = &quot;valor&quot;) |&gt; group_by(id_1) |&gt; summarise(vec_1 = list(valor)) |&gt; mutate(firma = map(vec_1, ~ calc_hashes(.x))) |&gt; select(id_1, firma) firmas_tbl ## # A tibble: 120 × 2 ## id_1 firma ## &lt;int&gt; &lt;list&gt; ## 1 1 &lt;int [4]&gt; ## 2 2 &lt;int [4]&gt; ## 3 3 &lt;int [4]&gt; ## 4 4 &lt;int [4]&gt; ## 5 5 &lt;int [4]&gt; ## 6 6 &lt;int [4]&gt; ## 7 7 &lt;int [4]&gt; ## 8 8 &lt;int [4]&gt; ## 9 9 &lt;int [4]&gt; ## 10 10 &lt;int [4]&gt; ## # … with 110 more rows firmas_tbl$firma[[1]] ## [1] -88 14 14 -87 firmas_tbl$firma[[2]] ## [1] -91 48 48 -92 Para este ejemplo, consideraremos todos los pares que coinciden en al menos una cubeta (hacemos disyunción o construcción OR de los 4 hashes): cubetas_tbl &lt;- firmas_tbl |&gt; unnest(firma) |&gt; group_by(id_1) |&gt; mutate(hash_no = 1:4) |&gt; mutate(cubeta = paste(hash_no, firma, sep = &quot;/&quot;)) cubetas_tbl ## # A tibble: 480 × 4 ## # Groups: id_1 [120] ## id_1 firma hash_no cubeta ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 -88 1 1/-88 ## 2 1 14 2 2/14 ## 3 1 14 3 3/14 ## 4 1 -87 4 4/-87 ## 5 2 -91 1 1/-91 ## 6 2 48 2 2/48 ## 7 2 48 3 3/48 ## 8 2 -92 4 4/-92 ## 9 3 -24 1 1/-24 ## 10 3 88 2 2/88 ## # … with 470 more rows Ahora agrupamos cubetas y filtramos las que tienen más de un elemento cubetas_tbl &lt;- cubetas_tbl |&gt; group_by(cubeta) |&gt; summarise(ids = list(id_1), n = length(id_1)) |&gt; filter(n &gt; 1) cubetas_tbl ## # A tibble: 79 × 3 ## cubeta ids n ## &lt;chr&gt; &lt;list&gt; &lt;int&gt; ## 1 1/-1 &lt;int [2]&gt; 2 ## 2 1/-13 &lt;int [2]&gt; 2 ## 3 1/-16 &lt;int [4]&gt; 4 ## 4 1/-21 &lt;int [11]&gt; 11 ## 5 1/-28 &lt;int [2]&gt; 2 ## 6 1/-32 &lt;int [2]&gt; 2 ## 7 1/-41 &lt;int [2]&gt; 2 ## 8 1/-44 &lt;int [2]&gt; 2 ## 9 1/-54 &lt;int [2]&gt; 2 ## 10 1/-55 &lt;int [2]&gt; 2 ## # … with 69 more rows Y finalmente, extraemos los pares candidatos: candidatos_tbl &lt;- cubetas_tbl |&gt; mutate(pares_cand = map(ids, ~ combn(.x, 2, simplify = FALSE))) |&gt; select(cubeta, pares_cand) |&gt; unnest(pares_cand) |&gt; unnest_wider(pares_cand, names_sep = &quot;_&quot;) |&gt; select(-cubeta) |&gt; unique() candidatos_tbl ## # A tibble: 219 × 2 ## pares_cand_1 pares_cand_2 ## &lt;int&gt; &lt;int&gt; ## 1 25 34 ## 2 30 85 ## 3 8 33 ## 4 8 40 ## 5 8 80 ## 6 33 40 ## 7 33 80 ## 8 40 80 ## 9 100 111 ## 10 100 112 ## # … with 209 more rows nrow(candidatos_tbl) ## [1] 219 En este caso, seguramente tenemos algunos falsos positivos que tenemos que filtrar, y quizá algunos falsos negativos. Calculamos distancias para todos los pares candidatos (es una lista mucho más corta generalmente): puntos_tbl &lt;- datos_tbl_vars |&gt; pivot_longer(V1:V2) |&gt; group_by(id_1) |&gt; select(-name) |&gt; summarise(punto = list(value)) candidatos_tbl &lt;- candidatos_tbl |&gt; left_join(puntos_tbl |&gt; rename(pares_cand_1 = id_1, punto_1 = punto)) |&gt; left_join(puntos_tbl |&gt; rename(pares_cand_2 = id_1, punto_2 = punto)) ## Joining, by = &quot;pares_cand_1&quot; ## Joining, by = &quot;pares_cand_2&quot; candidatos_tbl ## # A tibble: 219 × 4 ## pares_cand_1 pares_cand_2 punto_1 punto_2 ## &lt;int&gt; &lt;int&gt; &lt;list&gt; &lt;list&gt; ## 1 25 34 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 2 30 85 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 3 8 33 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 4 8 40 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 5 8 80 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 6 33 40 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 7 33 80 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 8 40 80 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 9 100 111 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## 10 100 112 &lt;dbl [2]&gt; &lt;dbl [2]&gt; ## # … with 209 more rows pares_similares_tbl &lt;- candidatos_tbl |&gt; mutate(dist = map2_dbl(punto_1, punto_2, dist_e)) |&gt; filter(dist &lt; 1) nrow(pares_similares_tbl) ## [1] 96 Probando con datos de “gold standard” En este caso, sabemos cuáles son los pares que buscamos, así que podemos evaluar nuestro método: verdadero_pos &lt;- nrow(inner_join(pares_similares_tbl, pares_sim)) ## Joining, by = &quot;dist&quot; verdadero_pos ## [1] 96 sensibilidad &lt;- verdadero_pos / nrow(pares_sim) sensibilidad ## [1] 0.932 precision &lt;- verdadero_pos / nrow(pares_similares_tbl) precision ## [1] 1 Como vemos, la precisión es 1 y la sensibilidad es alta. Nos faltó encontrar una pequeña parte de los pares similares (alrededor de 7 de 103). Si queremos ser mas exhaustivos (con el mayor cómputo que implica), podemos hacer más anchas las cubetas (cambiar \\(r\\) a 1 por ejemplo), y podemos incluir más hashes. ¿Qué pasa si ponemos \\(r = 0.8\\) por ejemplo? Proyección aleatoria en cubetas. Podemos encontrar pares muy similares en datos numéricos usando como hashes proyecciones aleatorias discretizadas en cubetas. Para afinar el umbral de captura y el balance de falsos positivos y falsos negativos, usamos las mismas técnicas mencionadas arriba para minhashing (construcciones OR o AND y combinaciones). 3.10 Locality Sensitive Hashing (LSH) Las dos técnicas que acabamos de ver arriba son tipos de Locality Sensitive Hashing (LSH), donde usamos hashes que tienden a coincidir cuando dos elementos son similares. Esto requiere de dos partes: Una definición de distancia en el espacio original Una familia de hashes que se seleccionan al azar que son sensibles a la localidad en relación a la distancia seleccionada, que formalmente podemos escribir como: Sean \\(d_1&lt;d_2\\) dos valores (que interpretamos como distancias). Una familia \\({\\cal F}\\) es una familia \\(d_1,d_2,p_1,p_2\\), sensible a localidad (con \\(p_1&gt;p_2\\)) cuando para cualquier par de elementos \\(x,y\\), 1. Si \\(d(x,y)\\leq d_1\\), entonces la probabilidad \\(P(f(x)=f(y))\\geq p_1\\). 2. Si \\(d(x,y)\\geq d_2\\), entonces \\(P(f(x)=f(y))\\leq p_2\\) Nótese que las probabilidades están dadas sobre la selección de \\(f\\). Estas condiciones se interpretan como sigue: cuando \\(x\\) y \\(y\\) están suficientemente cerca (\\(d_1\\)), la probabilidad de que sean mapeados al mismo valor por una función \\(f\\) de la familia es alta. Cuando \\(x\\) y \\(y\\) están lejos \\(d_2\\), entonces, la probabilidad de que sean mapeados al mismo valor es baja. Podemos ver una gráfica: En nuestros ejemplos, vimos (puedes intentar demostrarlas): Para documentos, utilizamos distancia de Jaccard de tejas. Las funciones minhash dan una familia sensible a la localidad. Para datos numéricos con distancia euclideana, la familia de proyecciones aleatorias en cubetas das una familia sensible a la localidad (Adicional) La similitud coseno para datos numéricos (donde no nos importa la magnitud sino solo la dirección de lso puntos), que se utiliza a veces en procesamiento de texto, también puede tratarse utilizando proyecciones aleatorias con 2 cubetas (derecha e izquierda) De esta última puedes ver más en Leskovec, Rajaraman, and Ullman (2014). 3.11 LSH para imágenes Consideramos tres imágenes para probar: En espacios de dimensión muy alta, como en imágenes, conviene hacer reducción de dimensionalidad para definir la métrica de distancia y utilizar estos métodos para encontrar vecinos cercanos. library(keras) modelo &lt;- application_vgg16(weights = &#39;imagenet&#39;) ## Loaded Tensorflow version 2.4.0 # obtener la penúltima embed_modelo &lt;- keras_model(inputs = modelo$input, outputs = get_layer(modelo, &quot;fc2&quot;)$output) obtener_pixeles &lt;- function(imagen_ruta){ img &lt;- image_load(imagen_ruta, target_size = c(224,224)) x &lt;- image_to_array(img) array_reshape(x, c(1, dim(x))) } calcular_capa &lt;- function(imagen_ruta){ x &lt;- obtener_pixeles(imagen_ruta) |&gt; imagenet_preprocess_input() embed_modelo |&gt; predict(x) |&gt; as.numeric() } pixeles_1 &lt;- obtener_pixeles(&quot;../datos/imagenes/elefante_1.jpg&quot;) |&gt; as.numeric() pixeles_2 &lt;- obtener_pixeles(&quot;../datos/imagenes/elefante_3.jpg&quot;) |&gt; as.numeric() pixeles_3 &lt;- obtener_pixeles(&quot;../datos/imagenes/leon_1.jpg&quot;) |&gt; as.numeric() Calculamos la distancia pixel a pixel: mean((pixeles_2 - pixeles_1)^2) ## [1] 7040 mean((pixeles_1 - pixeles_3)^2) ## [1] 7064 Calculamos la penúltima capa de nuestro modelo para las imágenes de prueba: features_1 &lt;- calcular_capa(&quot;../datos/imagenes/elefante_1.jpg&quot;) features_2 &lt;- calcular_capa(&quot;../datos/imagenes/elefante_3.jpg&quot;) features_3 &lt;- calcular_capa(&quot;../datos/imagenes/leon_1.jpg&quot;) length(features_1) ## [1] 4096 Nótese ahora que la distancia en nuestro nuevo espacio de imágenes es mucho más chica para los elefantes que entre el león y los elefantes: mean((features_2 - features_1)^2) ## [1] 0.889 mean((features_1 - features_3)^2) ## [1] 3.2 Podemos usar entonces el siguiente proceso: Calculamos para cada imagen la representación dada por la última capa de una red nueronal de clasificación para imagen. Definimos como nuestra medida de distancia entre imagenes la distancia euclideana en la representación del inciso anterior Definimos funciones hash con proyecciones en cubetas como vimos arriba Con estos hashes, podemos encontrar imagenes duplicadas o muy similares. 3.12 Joins por similitud Otro uso de las técnicas del LSH nos permita hacer uniones (joins) por similitud. La idea es la siguiente: Tenemos una tabla A, con una columna A.x que es un texto, por ejemplo, o un vector de números, etc. Tenemos una tabla B, con una columna B.x que es del mismo tipo que A.x Queremos hacer una unión de A con B con la llave x, de forma que queden pareados todos los elementos tales que \\(sim(A.x_i, A.y_j)\\) es chica. Un ejemplo es pegar dos tablas de datos de películas de fuentes distintas mediante el título (que a veces varía en cómo está escrito, de manera que no podemos hacer un join usual), o títulos de pláticas en diferentes conferencias, o juntar registros de personas que pueden tener escrito su nombre de manera un poco diferente o con errores, netc. Usando LSH podemos hacer un join aproximado por similitud. La idea es la misma que antes: Calculamos cubetas de la misma forma para cada tabla (mismos hashes y bandas) Unimos las cubetas de las dos fuentes Los pares candidatos son todos los pares (uno de A y uno de B) que caen en la misma cubeta. Usando criterios adicionales, podemos filtrar falsos positivos. 3.13 Ejemplo: entity matching Ver tarea 4 en el repositorio del curso. Referencias "],["dvs-y-reducción-de-dimensionalidad.html", "4 DVS y reducción de dimensionalidad 4.1 Descomposición aditiva en matrices de rango 1 4.2 Aproximación con matrices de rango 1. 4.3 Aproximación con matrices de rango bajo 4.4 Interpetación de vectores singulares 4.5 Descomposición en valores singulares (SVD o DVS) 4.6 Más de interpretación geométrica 4.7 SVD para películas de netflix 4.8 Componentes principales", " 4 DVS y reducción de dimensionalidad En esta parte veremos técnicas de reducción de dimensionalidad por medio de proyecciones lineales, que buscan preservar la mayor parte de la información contenida en los datos originales. En particular veremos la descomposición en valores singulares, que es una de las más útiles. La descomposición en valores singulares puede verse como un tipo de descomposición aditiva en matrices de rango uno, así que comenzaremos explicando estos conceptos. 4.1 Descomposición aditiva en matrices de rango 1 Supongamos que tenemos una matriz de datos \\(X\\) de tamaño \\(n\\times p\\) (todas son variables numéricas). En los renglones tenemos los casos (\\(n\\)) y las columnas son las variables \\(p\\). Típicamente pensamos que las columnas o variables están todas definidas en una misma escala: por ejemplo, cantidades de dinero, poblaciones, número de eventos, etc. Cuando no es así, entonces normalizamos las variables de alguna forma para no tener unidades. 4.1.1 Matrices de rango 1 Una de las estructuras de datos más simples que podemos imaginar (que sea interesante) para un tabla de este tipo es que se trata de una matriz de datos de rango 1. Es generada por un score de individuos que determina mediante un peso el valor de una variable. Es decir, el individuo \\(i\\) en la variable \\(j\\) es \\[X_{ij} = \\sigma u_i v_j\\] Donde \\(u=(u_1,u_2,\\ldots, u_n)\\) son los scores de los individuos y \\(v = (v_1, v_2, \\ldots, v_p)\\) son los pesos de las variables. Tanto \\(u\\) como \\(v\\) son vectores normalizados, es decir \\(||u||=||v||=1\\). La constante \\(\\sigma\\) nos permite pensar que los vectores \\(u\\) y \\(v\\) están normalizados. Esto se puede escribir, en notación matricial, como \\[X = \\sigma u v^t\\] donde consideramos a \\(u\\) y \\(v\\) como matrices columna. Una matriz de rango uno (o en general de rango bajo) es más simple de analizar. En rango 1, tenemos que entender la variación de \\(n+p\\) datos (componentes de \\(u\\) y \\(v\\)), mientras que en una matriz general tenemos que entender \\(n\\times p\\) datos. Cada variable \\(j\\) de las observaciones es un reescalamiento del índice o score \\(u\\) de las personas por el factor \\(\\sigma v_j\\). Igualmente, cada caso \\(i\\) de las observaciones es un reescalamiento del índice o peso \\(v\\) de las variables por el factor \\(\\sigma u_i\\). \\(u\\) y \\(v\\) representan una dimensión (dimensión latente, componente) de estos datos. Ejemplo: una matriz de rango 1 de preferencias Supongamos que las columnas de \\(X\\) son películas (\\(p\\)), los renglones (\\(n\\)) personas, y la entrada \\(X_{ij}\\) es la afinidad de la persona \\(i\\) por la película \\(j\\). Vamos a suponer que estos datos tienen una estructura ficticia de rango 1, basada en las preferencias de las personas por películas de ciencia ficción. Construimos los pesos de las películas que refleja qué tanto son de ciencia ficción o no. Podemos pensar que cada uno de estos valores el el peso de la película en la dimensión de ciencia ficción. library(tidyverse) peliculas_nom &lt;- c(&#39;Gladiator&#39;,&#39;Memento&#39;,&#39;X-Men&#39;,&#39;Scream&#39;,&#39;Amores Perros&#39;, &#39;Billy Elliot&#39;, &#39;Lord of the Rings&#39;,&#39;Mulholland drive&#39;, &#39;Amelie&#39;,&#39;Planet of the Apes&#39;) # variable latente que describe el contenido de ciencia ficción de cada v &lt;- c(-1.5, -0.5, 4, -1,-3, -3, 0, 1, -0.5, 3.5) normalizar &lt;- function(x){ norma &lt;- sqrt(sum(x^2)) if(norma &gt; 0){ x_norm &lt;- x/norma } else { x_norm &lt;- x } x_norm } v &lt;- normalizar(v) peliculas &lt;- tibble(pelicula = peliculas_nom, v = v) |&gt; arrange(v) peliculas ## # A tibble: 10 × 2 ## pelicula v ## &lt;chr&gt; &lt;dbl&gt; ## 1 Amores Perros -0.420 ## 2 Billy Elliot -0.420 ## 3 Gladiator -0.210 ## 4 Scream -0.140 ## 5 Memento -0.0700 ## 6 Amelie -0.0700 ## 7 Lord of the Rings 0 ## 8 Mulholland drive 0.140 ## 9 Planet of the Apes 0.490 ## 10 X-Men 0.560 Ahora pensamos que tenemos con individuos con scores de qué tanto les gusta la ciencia ficción set.seed(102) u &lt;- rnorm(15, 0, 1) u &lt;- normalizar(u) personas &lt;- data_frame(persona = 1:15, u = u) ## Warning: `data_frame()` was deprecated in tibble 1.1.0. ## Please use `tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. head(personas) ## # A tibble: 6 × 2 ## persona u ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.0422 ## 2 2 0.183 ## 3 3 -0.316 ## 4 4 0.463 ## 5 5 0.289 ## 6 6 0.280 Podemos entonces construir la afinidad de cada persona por cada película (matriz \\(n\\times p\\) ) multiplicando el score de cada persona (en la dimensión ciencia ficción) por el peso de la película (en la dimensión ciencia ficción). Por ejemplo, para una persona, tenemos que su índice es personas$u[2] ## [1] 0.1832537 Esta persona tiene afinidad por la ciencia ficción, así que sus niveles de gusto por las películas son (multiplicando por \\(\\sigma = 100\\), que en este caso es una constante arbitraria seleccionada para el ejemplo): tibble(pelicula=peliculas$pelicula, afinidad= 100*personas$u[2]*peliculas$v) |&gt; arrange(desc(afinidad)) ## # A tibble: 10 × 2 ## pelicula afinidad ## &lt;chr&gt; &lt;dbl&gt; ## 1 X-Men 10.3 ## 2 Planet of the Apes 8.98 ## 3 Mulholland drive 2.57 ## 4 Lord of the Rings 0 ## 5 Memento -1.28 ## 6 Amelie -1.28 ## 7 Scream -2.57 ## 8 Gladiator -3.85 ## 9 Amores Perros -7.70 ## 10 Billy Elliot -7.70 Consideremos otra persona personas$u[15] ## [1] -0.05320133 Esta persona tiene disgusto ligero por la ciencia ficción, y sus scores de las películas son: tibble(pelicula = peliculas$pelicula, afinidad = 100 * personas$u[15] * peliculas$v) ## # A tibble: 10 × 2 ## pelicula afinidad ## &lt;chr&gt; &lt;dbl&gt; ## 1 Amores Perros 2.23 ## 2 Billy Elliot 2.23 ## 3 Gladiator 1.12 ## 4 Scream 0.745 ## 5 Memento 0.372 ## 6 Amelie 0.372 ## 7 Lord of the Rings 0 ## 8 Mulholland drive -0.745 ## 9 Planet of the Apes -2.61 ## 10 X-Men -2.98 Si fuera tan simple el gusto por las películas (simplemente depende si contienen ciencia ficción o no, y si a la persona le gusta o no), la matriz \\(X\\) de observaciones sería \\[X_1 = \\sigma uv^t\\] donde consideramos a \\(u\\) y \\(v\\) como vectores columna. El producto es de una matriz de \\(n\\times 1\\) contra una de \\(1\\times p\\), lo cual da una matriz de \\(n\\times p\\). Podemos calcular como: X = 100 * tcrossprod(personas$u, peliculas$v) # tcrossprod(x,y) da x %*% t(y) colnames(X) &lt;- peliculas$pelicula head(round(X, 1)) ## Amores Perros Billy Elliot Gladiator Scream Memento Amelie ## [1,] -1.8 -1.8 -0.9 -0.6 -0.3 -0.3 ## [2,] -7.7 -7.7 -3.8 -2.6 -1.3 -1.3 ## [3,] 13.3 13.3 6.6 4.4 2.2 2.2 ## [4,] -19.5 -19.5 -9.7 -6.5 -3.2 -3.2 ## [5,] -12.1 -12.1 -6.1 -4.0 -2.0 -2.0 ## [6,] -11.8 -11.8 -5.9 -3.9 -2.0 -2.0 ## Lord of the Rings Mulholland drive Planet of the Apes X-Men ## [1,] 0 0.6 2.1 2.4 ## [2,] 0 2.6 9.0 10.3 ## [3,] 0 -4.4 -15.5 -17.7 ## [4,] 0 6.5 22.7 25.9 ## [5,] 0 4.0 14.2 16.2 ## [6,] 0 3.9 13.7 15.7 O usando una tabla peliculas |&gt; crossing(personas) |&gt; mutate(afinidad = round(100 * u * v, 2)) |&gt; select(persona, pelicula, afinidad) |&gt; pivot_wider(names_from = pelicula, values_from = afinidad) ## # A tibble: 15 × 11 ## persona Amelie `Amores Perros` `Billy Elliot` Gladiator `Lord of the Rings` ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -0.3 -1.77 -1.77 -0.89 0 ## 2 2 -1.28 -7.7 -7.7 -3.85 0 ## 3 3 2.21 13.3 13.3 6.64 0 ## 4 4 -3.24 -19.5 -19.5 -9.73 0 ## 5 5 -2.02 -12.2 -12.2 -6.07 0 ## 6 6 -1.96 -11.8 -11.8 -5.89 0 ## 7 7 -1.47 -8.79 -8.79 -4.4 0 ## 8 8 -0.41 -2.49 -2.49 -1.24 0 ## 9 9 -0.9 -5.39 -5.39 -2.7 0 ## 10 10 -3.11 -18.7 -18.7 -9.34 0 ## 11 11 -2.36 -14.2 -14.2 -7.09 0 ## 12 12 -0.19 -1.13 -1.13 -0.57 0 ## 13 13 1.03 6.15 6.15 3.08 0 ## 14 14 2.08 12.4 12.4 6.23 0 ## 15 15 0.37 2.23 2.23 1.12 0 ## # … with 5 more variables: Memento &lt;dbl&gt;, Mulholland drive &lt;dbl&gt;, ## # Planet of the Apes &lt;dbl&gt;, Scream &lt;dbl&gt;, X-Men &lt;dbl&gt; Nótese que en este ejemplo podemos simplificar mucho el análisis: en lugar de ver la tabla completa, podemos simplemente considerar los dos vectores de índices (pesos y scores), y trabajar como si fuera un problema de una sola dimensión. 4.2 Aproximación con matrices de rango 1. En general, las matrices de datos reales no son de rango 1. Más bien nos interesa saber si se puede hacer una buena aproximación de rango 1. El problema que nos interesa es el inverso: si tenemos la tabla \\(X\\), ¿cómo sabemos si se puede escribir aproximadamente en la forma simple de una matriz de rango uno? Nótese que si lo pudiéramos hacer, esto simplificaría mucho nuestro análisis de estos datos, y obtendríamos información valiosa. Medimos la diferencia entre una matriz de datos general \\(X\\) y una matriz de rango 1 \\(\\sigma uv^t\\) mediante la norma Frobenius: \\[ ||X-\\sigma uv^t||^2_F = \\sum_{i,j} (X_{i,j} - \\sigma u_iv_j)^2\\] Nos interesa resolver \\[\\min_{\\sigma, u,v} || X - \\sigma uv^t ||_F^2\\] donde \\(\\sigma\\) es un escalar, \\(u\\) es un vector columna de tamaño \\(n\\) y \\(v\\) es un vector columna de tamaño \\(p\\). Suponemos que los vectores \\(u\\) y \\(v\\) tienen norma uno. Esto no es necesario - podemos absorber constantes en \\(\\sigma\\). Ejemplo Por ejemplo, la siguiente tabla tiene gastos personales en distintos rubros en distintos años para todo Estados Unidos (en dólares nominales). library(tidyverse) X_arr &lt;- USPersonalExpenditure[, c(1,3,5)] X_arr ## 1940 1950 1960 ## Food and Tobacco 22.200 59.60 86.80 ## Household Operation 10.500 29.00 46.20 ## Medical and Health 3.530 9.71 21.10 ## Personal Care 1.040 2.45 5.40 ## Private Education 0.341 1.80 3.64 En este ejemplo podríamos tener la intuición de que las proporciones de gasto se han mantenido aproximadamente constante en cada año, y que todos los rubros han aumentado debido a la inflación. Podríamos intentar hacer varias normalizaciones para probar esta idea, pero quisiéramos idear una estrategia general. Digamos que el vector \\(u\\) denota los niveles generales de cada rubro (es un vector de longitud 5), y el vector \\(v\\) denota los niveles generales de cada año (un vector de longitud 3). Queremos ver si es razonable aproximar \\[X\\approx uv^t\\] Observación: En este caso, la ecuación de arriba \\(X_{i,j} = u_iv_j\\) expresa que hay niveles generales para cada rubro \\(i\\) a lo largo de todos los años, y para obtener una aproximación ajustamos con un factor \\(v_j\\) de inflación el año \\(j\\) La mejor manera de entender este problema es con álgebra lineal, como veremos más adelante. Por el momento intentemos aproximar directamente, intentando resolver (podemos normalizar \\(u\\) y \\(v\\) más tarde y encontrar la \\(\\sigma\\)): \\[\\min_{u,v} \\sum_{i,j} (X_{i,j} - u_iv_j)^2 = \\min_{u,v} ||X-uv^t||^2_F\\] Observación:Este problema tiene varios mínimos, pues podemos mover constantes de \\(u\\) a \\(v\\) (tiene múltiples soluciones). Hay varias maneras de lidiar con esto (por ejemplo, normalizando). Por el momento, corremos la optimización para encontrar una solución: error &lt;- function(pars){ v &lt;- pars[1:3] u &lt;- pars[4:8] mean((X_arr - tcrossprod(u, v))^2) #tcrossprod da x %*% t(y) } optim_decomp &lt;- optim(rep(0.1, 5 + 3), error, method =&#39;BFGS&#39;) v_años &lt;- optim_decomp$par[1:3] u_rubros &lt;- optim_decomp$par[4:8] La matriz \\(X_1=uv^t\\) que obtuvimos es: X_1 &lt;- tcrossprod(u_rubros, v_años) round(X_1, 1) ## [,1] [,2] [,3] ## [1,] 21.6 58.4 87.8 ## [2,] 11.1 30.1 45.3 ## [3,] 4.7 12.6 18.9 ## [4,] 1.2 3.2 4.8 ## [5,] 0.8 2.2 3.3 Podemos ver qué tan buena es la aproximación: R &lt;- X_arr - X_1 qplot(as.numeric(X_1), as.numeric(as.matrix(X_arr))) + geom_abline(colour=&#39;red&#39;) round(R,2) ## 1940 1950 1960 ## Food and Tobacco 0.60 1.25 -0.98 ## Household Operation -0.65 -1.11 0.90 ## Medical and Health -1.12 -2.87 2.18 ## Personal Care -0.15 -0.77 0.55 ## Private Education -0.46 -0.38 0.36 donde vemos que nuestra aproximación es muy cercana a los datos en la tabla \\(X\\). La descomposición que obtuvimos es de la forma \\[X = uv^t + R\\] donde \\(R\\) tiene norma Frobenius relativamente chica. Observaciones: Este método nos da un ordenamiento de rubros de gasto según su nivel general, y un ordenamiento de años según su nivel general de gasto. tibble(rubro = rownames(X_arr), nivel = u_rubros) |&gt; arrange(desc(nivel)) ## # A tibble: 5 × 2 ## rubro nivel ## &lt;chr&gt; &lt;dbl&gt; ## 1 Food and Tobacco 9.74 ## 2 Household Operation 5.03 ## 3 Medical and Health 2.10 ## 4 Personal Care 0.538 ## 5 Private Education 0.363 tibble(año = colnames(X_arr), nivel = v_años) ## # A tibble: 3 × 2 ## año nivel ## &lt;chr&gt; &lt;dbl&gt; ## 1 1940 2.22 ## 2 1950 5.99 ## 3 1960 9.01 Pudimos explicar estos datos usando esos dos índices (5+3=7 números) en lugar de toda la tabla(5(3)=15 números). Una vez explicado esto, podemos concentrarnos en los patrones que hemos aislado en la matriz \\(R\\). Podríamos repetir buscando una aproximación igual a la que acabomos de hacer para la matriz \\(X\\), o podríamos hacer distintos tipos de análisis. 4.2.1 Suma de matrices de rango 1. La matriz de datos \\(X\\) muchas veces no puede aproximarse bien con una sola matriz de rango 1. Podríamos entonces buscar descomponer los datos en más de una dimensión latente: \\[X = \\sigma_1 u_1v_1^t + \\sigma_2 u_2v_2^t+\\ldots+ \\sigma_k u_kv_k^t\\] Interpretamos cada término del lado derecho de la misma manera (cada uno de ellos es una matriz de rango uno), y los valores finales son una suma ponderada de estos términos. Ejemplo: películas En nuestro ejemplo anterior, claramente debe haber otras dimensiones latentes que expliquen la afinidad por una película. Por ejemplo, quizá podríamos considerar el gusto por películas mainstream vs películas independientes. peliculas_nom &lt;- c(&#39;Gladiator&#39;,&#39;Memento&#39;,&#39;X-Men&#39;,&#39;Scream&#39;,&#39;Amores Perros&#39;, &#39;Billy Elliot&#39;, &#39;Lord of the Rings&#39;,&#39;Mulholland drive&#39;, &#39;Amelie&#39;,&#39;Planet of the Apes&#39;) # variable latente que describe el contenido de ciencia ficción de cada v_1 &lt;- c(-1.5, -0.5, 4, -1,-3, -3, 0, 1, -0.5, 3.5) v_2 &lt;- c(4.1, 0.2, 3.5, 1.5, -3.0, -2.5, 2.0, -4.5, -1.0, 2.6) #mainstream o no v_1 &lt;- normalizar(v_1) v_2 &lt;- normalizar(v_2) peliculas &lt;- tibble(pelicula = peliculas_nom, v_1 = v_1, v_2 = v_2) |&gt; arrange(v_2) peliculas ## # A tibble: 10 × 3 ## pelicula v_1 v_2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mulholland drive 0.140 -0.508 ## 2 Amores Perros -0.420 -0.338 ## 3 Billy Elliot -0.420 -0.282 ## 4 Amelie -0.0700 -0.113 ## 5 Memento -0.0700 0.0226 ## 6 Scream -0.140 0.169 ## 7 Lord of the Rings 0 0.226 ## 8 Planet of the Apes 0.490 0.293 ## 9 X-Men 0.560 0.395 ## 10 Gladiator -0.210 0.462 Y las personas tienen también scores en esta nueva dimensión, que aquí simulamos al azar personas &lt;- personas |&gt; mutate(u_1 = u, u_2 = normalizar(rnorm(15, 0, 1))) |&gt; select(-u) head(personas) ## # A tibble: 6 × 3 ## persona u_1 u_2 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.0422 -0.0623 ## 2 2 0.183 -0.377 ## 3 3 -0.316 0.420 ## 4 4 0.463 -0.401 ## 5 5 0.289 0.127 ## 6 6 0.280 0.180 Por ejemplo, la segunda persona persona le gusta la ciencia ficción, y pero prefiere fuertemente películas independientes. Podemos graficar a las personas según su interés en ciencia ficción y mainstream: ggplot(personas, aes(x = u_1, y=u_2)) + geom_point() + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) + xlab(&#39;Ciencia ficción&#39;)+ ylab(&#39;Mainstream&#39;) Y también podemos graficar las películas ggplot(peliculas, aes(x = v_1, y=v_2, label = pelicula)) + geom_point() + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;)+ xlab(&#39;Ciencia ficción&#39;)+ ylab(&#39;Mainstream&#39;) + geom_text() ¿Cómo calculariamos ahora la afinidad de una persona por una película? Necesitamos calcular (dando el mismo peso a las dos dimensiones) \\[X_{i,j} = \\sigma_1 u_{1,i} v_{1,j} + \\sigma_2 u_{2,i} v_{2,j}\\] Usamos la notación \\(u_{k,i}\\) para denotar la componente \\(i\\) del vector \\(u_k\\). Antes pusimos \\(\\sigma_1=100\\). Supongamos que la siguiente componente es un poco menos importante que la primera. Podriamos escoger \\(\\sigma_2=70\\), por ejemplo. Podríamos hacer library(purrr) library(stringr) personas_larga &lt;- personas |&gt; gather(dimension, u, u_1:u_2) |&gt; separate(dimension, c(&#39;x&#39;,&#39;dim&#39;), &#39;_&#39;) |&gt; select(-x) head(personas_larga) ## # A tibble: 6 × 3 ## persona dim u ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 1 0.0422 ## 2 2 1 0.183 ## 3 3 1 -0.316 ## 4 4 1 0.463 ## 5 5 1 0.289 ## 6 6 1 0.280 peliculas_larga &lt;- peliculas |&gt; gather(dimension, v, v_1:v_2) |&gt; separate(dimension, c(&#39;x&#39;,&#39;dim&#39;), &#39;_&#39;) |&gt; select(-x) head(peliculas_larga) ## # A tibble: 6 × 3 ## pelicula dim v ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Mulholland drive 1 0.140 ## 2 Amores Perros 1 -0.420 ## 3 Billy Elliot 1 -0.420 ## 4 Amelie 1 -0.0700 ## 5 Memento 1 -0.0700 ## 6 Scream 1 -0.140 sigma_df &lt;- data_frame(dim = c(&#39;1&#39;,&#39;2&#39;), sigma = c(100,70)) df_dim &lt;- personas_larga |&gt; left_join(peliculas_larga) |&gt; left_join(sigma_df) |&gt; mutate(afinidad = sigma*u*v) ## Joining, by = &quot;dim&quot; ## Joining, by = &quot;dim&quot; df_agg &lt;- df_dim |&gt; group_by(persona, pelicula) |&gt; summarise(afinidad = round(sum(afinidad),2)) ## `summarise()` has grouped output by &#39;persona&#39;. You can override using the `.groups` argument. df_agg |&gt; spread(pelicula, afinidad) ## # A tibble: 15 × 11 ## # Groups: persona [15] ## persona Amelie `Amores Perros` `Billy Elliot` Gladiator `Lord of the Rings` ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.2 -0.3 -0.54 -2.9 -0.98 ## 2 2 1.69 1.22 -0.27 -16.0 -5.95 ## 3 3 -1.1 3.33 4.99 20.2 6.63 ## 4 4 -0.07 -9.95 -11.5 -22.7 -6.34 ## 5 5 -3.03 -15.2 -14.7 -1.96 2.01 ## 6 6 -3.38 -16.0 -15.3 -0.06 2.84 ## 7 7 -1.07 -7.6 -7.8 -6.02 -0.79 ## 8 8 2.08 4.99 3.74 -11.5 -4.98 ## 9 9 0.95 0.17 -0.76 -10.3 -3.71 ## 10 10 -2.44 -16.6 -17.0 -12.1 -1.35 ## 11 11 -2.01 -13.1 -13.3 -8.52 -0.7 ## 12 12 -0.34 -1.6 -1.52 0.07 0.31 ## 13 13 0.7 5.17 5.33 4.42 0.66 ## 14 14 0.01 6.27 7.3 14.7 4.12 ## 15 15 -3.43 -9.17 -7.27 16.7 7.6 ## # … with 5 more variables: Memento &lt;dbl&gt;, Mulholland drive &lt;dbl&gt;, ## # Planet of the Apes &lt;dbl&gt;, Scream &lt;dbl&gt;, X-Men &lt;dbl&gt; Observación: Piensa qué harías si vieras esta tabla directamente, e imagina cómo simplificaría la comprensión y análisis si conocieras las matrices de rango 1 con las que se construyó este ejemplo. Consideremos la persona 2: filter(personas, persona == 2) ## # A tibble: 1 × 3 ## persona u_1 u_2 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 0.183 -0.377 Que tiene gusto por la ciencia ficción y le gustan películas independientes. Sus afinidades son: filter(df_agg, persona==2) |&gt; arrange(desc(afinidad)) ## # A tibble: 10 × 3 ## # Groups: persona [1] ## persona pelicula afinidad ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Mulholland drive 15.9 ## 2 2 Amelie 1.69 ## 3 2 Planet of the Apes 1.25 ## 4 2 Amores Perros 1.22 ## 5 2 X-Men -0.14 ## 6 2 Billy Elliot -0.27 ## 7 2 Memento -1.88 ## 8 2 Lord of the Rings -5.95 ## 9 2 Scream -7.03 ## 10 2 Gladiator -16.0 Explicaríamos así esta descomposición: Cada persona \\(i\\) tiene un nivel de gusto por ciencia ficción (\\(u_{1,i}\\)) y otro nivel de gusto por películas independientes (\\(u_{2,i}\\)). Cada película \\(j\\) tiene una calificación o peso en la dimensión de ciencia ficción (\\(v_{1,i}\\)) y un peso en la dimensión de independiente (\\(v_{2,i}\\)) La afinidad de una persona \\(i\\) por una película \\(j\\) se calcula como \\[ \\sigma_1 u_{1,i}v_{1,j} + \\sigma_2 u_{2,i}v_{2,j}\\] Una matriz de rango 2 es una suma (o suma ponderada) de matrices de rango 1 Las explicaciones de matrices de rango aplican para cada sumando (ver arriba) En este caso, hay dos dimensiones latentes que explican los datos: preferencia por independientes y preferencia por ciencia ficción. En este ejemplo ficticio estas componentes explica del todo a los datos. 4.3 Aproximación con matrices de rango bajo Nuestro problema generalmente es el inverso: si tenemos la matriz de datos \\(X\\), ¿podemos encontrar un número bajo \\(k\\) de dimensiones de forma que \\(X\\) se escribe (o aproxima) como suma de matrices de \\(k\\) matrices rango 1? Lograr esto sería muy bueno, pues otra vez simplificamos el análisis a solo un número de dimensiones \\(k\\) (muy) menor a \\(p\\), el número de variables, sin perder mucha información (con buen grado de aproximación). Adicionalmente, las dimensiones encontradas pueden mostrar patrones interesantes que iluminan los datos, esqpecialmente en términos de aquellas dimensiones que aportan mucho a la aproximación. En general, buscamos encontrar una aproximación de la matriz \\(X\\) mediante una suma de matrices de rango 1 \\[X \\approx \\sigma_1 u_1v_1^t + \\sigma_2 v_2v_2^t+\\ldots+ \\sigma_k u_kv_k^t.\\] A esta aproximación le llamamos una aproximación de rango \\(k\\). Hay muchas maneras de hacer esto, y probablemente la mayoría de ellas no son muy interesantes. Podemos más concretamente preguntar, ¿cuál es la mejor aproximación de rango \\(k\\) que hay? \\[\\min_{X_k} || X - X_k ||_F^2\\] donde consideramos la distancia entre \\(X\\) y \\(X_k\\) con la norma de Frobenius, que está definida por: \\[|| A ||_F^2 = \\sum_{i,j} a_{i,j}^2\\] y es una medida de qué tan cerca están las dos matrices \\(A\\) y \\(B\\), componente a componente. 4.3.1 Discusión: aproximación de rango 1. Empecemos resolviendo el problema más simple, que es \\[\\min_{\\sigma,u,v} || X - \\sigma uv^t ||_F^2\\] donde \\(\\sigma\\) es un escalar, \\(u\\) es un vector columna de tamaño \\(n\\) y \\(v\\) es un vector columna de tamaño \\(p\\). Suponemos que los vectores \\(u\\) y \\(v\\) tienen norma uno. El objetivo que queremos minimizar es \\[\\sum_{i,j} (X_{i,j} - \\sigma u_iv_j)^2\\] Derivando con respecto a \\(u_i\\) y \\(v_j\\), e igualando a cero, obtenemos (la sigma podemos quitarla en la derivada, pues multiplica todo el lado derecho): \\[\\frac{\\partial}{\\partial u_i} = -2\\sigma\\sum_{j} (X_{i,j} - \\sigma u_iv_j)v_j = 0\\] \\[\\frac{\\partial}{\\partial v_j} = -2\\sigma\\sum_{i} (X_{i,j} - \\sigma u_iv_j)u_i = 0\\] Que simplificando (y usando que la norma de \\(u\\) y \\(v\\) es igual a 1: \\(\\sum_iu_i^2 = \\sum_j v_j^2=1\\)) quedan: \\[\\sum_j X_{i,j}v_j = \\sigma u_i,\\] \\[\\sum_i X_{i,j}u_i =\\sigma v_j,\\] O en forma matricial \\[\\begin{equation} Xv = \\sigma u \\tag{4.1} \\end{equation}\\] \\[\\begin{equation} u^t X= \\sigma v^t. \\tag{4.2} \\end{equation}\\] Podemos resolver este par de ecuaciones para encontrar la solución al problema de optimización de arriba. Este problema tiene varias soluciones (con distintas \\(\\sigma\\)), pero veremos cómo podemos escoger la que de mejor la aproximación (adelanto: escoger las solución con \\(\\sigma^2\\) más grande). A un par de vectores \\((u,v)\\) que cumplen esta propiedad les llamamos vector propio izquierdo (\\(u\\)) y vector propio derecho (\\(v\\)), con valor singular asociado \\(\\sigma\\). Por convención, tomamos \\(\\sigma \\geq 0\\) (si no, podemos multiplicar a \\(u\\) por menos, por ejemplo). Y tenemos un resultado importante que nos será útil, y que explica el nombre de estos vectores: Si \\((u,v)\\) son vectores propios de \\(X\\) asociados a \\(\\sigma\\), entonces \\(v\\) es un vector propio de la matriz cuadrada \\(X^tX\\) (\\(p\\times p\\)) con valor propio \\(\\sigma^2\\). \\(u\\) es un vector propio de la matrix cuadrada \\(XX^t\\) (\\(n\\times n\\)) con valor propio \\(\\sigma^2\\). Observaciones: La demostración es fácil pues aplicando \\(X^t\\) a ambos lados de (4.1), obtenemos \\(X^t X v= \\sigma X^t u\\), que implica \\((X^t X) v= \\sigma (u^tX)^t = \\sigma^2 v\\). Podemos hacer lo mismo para (4.2). Nótese que \\(X^tX\\) es una matriz simétrica. Por el teorema espectral, existe una base ortogonal de vectores propios (usual) \\(v_1, v_2, \\ldots, v_p\\) con valores propios reales. Adicionalmente, como \\(X^tX\\) es positivo-definida, entonces todos estos vectores propios tienen valor propio no negativos. Ejemplo Verifiquemos en el ejemplo del gasto en rubros. Si comparamos \\(Xv\\) con \\(u\\), vemos que son colineales (es decir, \\(Xv=\\sigma u\\)): # qplot(Xv, u), si Xv=sigma*u entonces Xv y u deben ser proporcionales qplot(as.matrix(X_arr) %*% v_años, u_rubros) + geom_smooth(method=&#39;lm&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Y también # qplot(u^tX, v^t), si u^tXv=sigma*v entonces Xv y u deben ser proporcionales qplot(t(as.matrix(X_arr)) %*% u_rubros, (v_años) ) + geom_smooth(method=&#39;lm&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Ahora normalizamos \\(u\\) y \\(v\\) para encontrar \\(\\sigma\\): u_rubros_norm &lt;- normalizar(u_rubros) v_años_norm &lt;- normalizar(v_años) (as.matrix(X_arr) %*% v_años_norm)/u_rubros_norm ## [,1] ## Food and Tobacco 123.4858 ## Household Operation 123.4855 ## Medical and Health 123.4864 ## Personal Care 123.4891 ## Private Education 123.4799 Y efectivamente vemos que \\((u,v)\\) (normalizados) forman satisfacen las ecuaciones mostradas arriba, con \\(\\sigma\\) igual a: first((as.matrix(X_arr) %*% v_años_norm)/u_rubros_norm) ## [1] 123.4858 Si hay varias soluciones, ¿cuál \\(\\sigma\\) escogemos? Supongamos que encontramos dos vectores propios \\((u,v)\\) (izquierdo y derecho) con valor propio asociado \\(\\sigma\\). Podemos evaluar la calidad de la aproximación usando la igualdad \\[\\||A||_F^2 = traza (AA^t)\\] que es fácil de demostrar, pues la componente \\((i,i)\\) de \\(AA^t\\) está dada por el producto punto del renglon \\(i\\) de A por el renglón \\(i\\) de \\(A\\), que es \\(\\sum_{i,j}a_{i,j}^2.\\) Entonces tenemos que \\[||X-\\sigma uv^t||_F^2 = \\mathrm{Tr} ((X-\\sigma uv^t)(X-\\sigma uv^t)^t)\\] que es igual a \\[ \\mathrm{Tr} (XX^t) - 2\\sigma \\mathrm{Tr} ( X(vu^t)) + \\sigma^2\\mathrm{Tr}(uv^tvu^t)\\] Como \\(u\\) y \\(v\\) tienen norma 1, tenemos que \\(v^tv=1\\), y \\(\\textrm{Tr(uu^t)} = \\sum_i u_i^2 = 1\\). Adicionalmente, usando el hecho de que \\(Xv=\\sigma u\\) obtenemos \\[ ||X-\\sigma uv^t||_F^2 = \\mathrm{Tr} (XX^t) - \\sigma^2\\] que es una igualdad interesante: quiere decir que la mejor aproximación se encuentra encontrando el par de valores propios tal que el valor propio asociado \\(\\sigma\\) tiene el valor \\(\\sigma^2\\) más grande posible. La cantidad a la cantidad \\(\\mathrm{Tr} (XX^t)\\) está dada por \\[\\mathrm{Tr} (XX^t) = ||X||_F^2 = \\sum_{i,j} X_{i,j}^2,\\] que es una medida del “tamaño” de la matriz \\(X\\). 4.4 Interpetación de vectores singulares El resultado de arriba nos da una primera forma de interpretar las aproximaciones de rango uno. Nótese que por el argumento de arriba, tenemos que \\[Xv = \\sigma u\\] Y consideremos el lado izquierdo de esta ecuación: \\(Xv\\) son las variables originales de \\(X\\) ponderadas por los pesos del vector \\(v\\), de forma que \\(Xv\\) nos da una nueva variable derivada que combina linealmente las variables originales. Las proyecciones de los reglones o casos \\(x_i\\) de \\(X\\) podemos escribirlas también como (suponiendo \\(v\\) de norma 1): \\[(x_i\\cdot v) v= \\sigma u_iv\\] y su componente \\(j\\) es \\[(x_i\\cdot v) v_j= \\sigma u_iv_j\\] De aquí, vemos que la matriz de proyecciones de los puntos originales sobre \\(v\\) es la matriz \\[\\sigma uv^t\\] Si \\(u\\) y \\(v\\) son vectores singulares de \\(X\\), entonces - El vector \\(\\sigma u\\) nos da las variables ponderadas de \\(X\\) por los pesos de las variables \\(v\\) - La matriz de proyecciones de los renglones de \\(X\\) sobre el vector \\(v\\) es \\(\\sigma uv^t\\) - Si escogemos el par \\(u,v\\) de vectores propios con la \\(\\sigma\\) más grande posible, entonces el vector \\(v\\) es la dirección tal que las proyecciones de los reglones de \\(X\\) sobre \\(v\\) son lo más cercanas posibles a los renglones originales \\(x_i\\) Ejemplo Consideremos unos datos simulados set.seed(3221) x_1 &lt;- rnorm(200,2, 1) x_2 &lt;- rnorm(200,0,1) + x_1 datos &lt;- data_frame(x_1, x_2) ggplot(datos, aes(x=x_1, y=x_2)) + geom_point() + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) Hacemos descomposición en valores singulares y extraemos la primera dimensión: svd_x &lt;- svd(datos) v &lt;- svd_x$v[,1] u &lt;- svd_x$u[,1] d &lt;- svd_x$d[1] #Nota: podemos mover signos para hacer las gráficas y la interpetación # más simples v &lt;- - v u &lt;- - u Graficamos ahora el vector \\(v\\), escalándolo para ver mejor cómo quedan en relación a los datos (esto no es necesario hacerlo): ggplot(datos) + geom_point(aes(x=x_1, y=x_2)) + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) + geom_segment(aes(xend= 4*v[1], yend=4*v[2], x=0, y=0), col=&#39;red&#39;, size=1.1, arrow = arrow(length = unit(0.3,&quot;cm&quot;))) El primer vector \\(v\\) es el “que pasa más cercano a los puntos,” en el sentido de que la distancia ortogonal entre los datos proyectados al vector \\(v\\) y los datos originales es lo más chica posible (mejor aproximación). La proyección de los datos sobre \\(v\\) es igual a \\(Xv=\\sigma_1 u\\), es decir, está dada por \\(\\sigma u\\), y la matriz de aproximaciones es \\(\\sigma uv^t\\) Podemos utilizar \\(u\\) como nuevas coordenadas de los reglones en el nuevo espacio a lo largo de \\(v\\). 4.4.1 Discusión: aproximaciones de rango más alto Vamos a repetir el análisis para dimensión 2, repitiendo el proceso que hicimos arriba. Denotamos como \\(u_1\\) y \\(v_1\\) los vectores \\(u\\) y \\(v\\) que encontramos en el paso anterior. Ahora buscamos minimizar \\[\\min_{u_2,v_2} || X - \\sigma_1 u_1 v_1^t - \\sigma_2 u_2 v_2^{t} ||_F^2\\] Repetimos el argumento de arriba y derivando respecto a las componentes de \\(u_2,v_2\\), y usando el hecho de que \\((u_1, v_1)\\) son vectores propios derecho e izquierdo asociados a \\(\\sigma_1\\), obtenemos: \\(v_2\\) es ortogonal a \\(v_1\\). \\(u_2\\) es ortogonal a \\(u_1\\). \\((u_2, v_2)\\) tienen que ser vectores propios derecho e izquierdo asociados a \\(\\sigma_2\\geq 0\\). Usando el hecho de que \\(v_1\\) y \\(v_2\\) son ortogonales, podemos podemos demostrar igual que arriba que \\[|| X - \\sigma_1 u_1 v_1^t - \\sigma_2 u_2 v_2^{t} ||_F^2 = \\textrm{Tr} (XX^t) - (\\sigma_1^2 + \\sigma_2^2) = ||X||_F^2 - (\\sigma_1^2 + \\sigma_2^2)\\] De modo que obtenemos la mejor aproximación escogiendo los dos valores de \\(\\sigma_1^2\\) y \\(\\sigma_2^2\\) más grandes para los que hay solución de (4.1) y (4.2) y Observaciones: Aunque aquí usamos un argumento incremental o greedy (comenzando con la mejor aproximación de rango 1), es posible demostrar que la mejor aproximación de rango 2 se puede construir de este modo. Ver por ejemplo estas notas. Vemos que la solución es incremental: \\(\\sigma_1, u_1, v_1\\) son los mismos que para la solución de dimensión 1. En dimensión 2, tenemos que buscar el siguiente valor singular más grande después de \\(\\sigma_1\\), de forma que tenemos \\(\\sigma_1^2 \\geq \\sigma_2^2\\). La solución entonces es agregar \\(\\sigma_2 u_2 v_2^t\\), donde \\((u_2,v_2)\\) es el par de vectores propios izquierdo y derecho. Ahora podemos enunciar nuestro teorema: Aproximación de matrices mediante valores singulares Sea \\(X\\) una matriz \\(n\\times p\\), y supongamos que \\(p\\leq n\\). Entonces, para cada \\(k \\leq p\\), la mejor aproximación de rango \\(k\\) a la matriz \\(X\\) se puede escribir como una suma \\(X_k\\) de \\(k\\) matrices de rango 1: \\[X_k = \\sigma_1 u_1v_1^t + \\sigma_2 u_2v_2^t + \\ldots \\sigma_k u_kv_k^t,\\] donde La calidad de la aproximación está dada por \\[||X-X_k||^2_F = ||X||^2_F - (\\sigma_1^2+ \\sigma_2^2 + \\cdots + \\sigma_k^2),\\] de forma que cada aproximación es sucesivamente mejor. \\(\\sigma_1^2 \\geq \\sigma_2^2 \\geq \\cdots \\geq \\sigma_k^2\\geq 0\\) Los vectores \\((u_i,v_i)\\) son un par de vectores propios izquierdo y derechos para \\(X\\) con valor singular \\(\\sigma_i\\). \\(v_1,\\ldots, v_k\\) son vectores ortogonales de norma 1 \\(u_1,\\ldots, u_k\\) son vectores ortogonales de norma 1 Observaciones: Normalmente no optimizamos como hicimos en el ejemplo de la matriz de gastos para encontrar las aproximación de rango bajo, sino que se usan algoritmos para encontrar vectores propios de \\(X^tX\\) (que son las \\(v\\)’s), o más generalmente algoritmos basados en álgebra lineal que intentan encontrar directamente los pares de vectores (u_i, v_i), y otros algoritmos numéricos (por ejemplo, basados en iteraciones). Un resultado interesante (que faltaría por demostrar) es que si tomamos la aproximación de rango \\(p\\) (cuando \\(p\\leq n\\)), obtenemos que \\[X= \\sigma_1 u_1v_1^t + \\sigma_2 u_2v_2^t + \\ldots \\sigma_p u_pv_p^t\\] es decir, la aproximación es exacta. Esto es un fraseo del teorema de descomposición en valores singulares, que normalmente se expresa de otra forma (ver más adelante). Ejemplo Consideremos el ejemplo de los gastos. Podemos usar la función svd de R svd_gasto &lt;- svd(X_arr) El objeto de salida contiene los valores singulares (en d). Nótese que ya habíamos calculado por fuerza bruta el primer valor singular: sigma &lt;- svd_gasto$d sigma ## [1] 123.4857584 4.5673718 0.3762533 Los vectores \\(v_1,v_2,v_3\\) (pesos de las variables) en nuestras tres nuevas dimensiones, que son las columnas de v &lt;- svd_gasto$v rownames(v) &lt;- colnames(X_arr) v ## [,1] [,2] [,3] ## 1940 -0.2007388 -0.3220495 -0.92519623 ## 1950 -0.5423269 -0.7499672 0.37872247 ## 1960 -0.8158342 0.5777831 -0.02410854 y los vectores \\(u_1,u_2,u_3\\), que son los scores de los rubros en cada dimensión dim(svd_gasto$u) ## [1] 5 3 u &lt;- (svd_gasto$u) rownames(u) &lt;- rownames(X_arr) u ## [,1] [,2] [,3] ## Food and Tobacco -0.87130286 -0.3713244 -0.1597823 ## Household Operation -0.44966139 0.3422116 0.4108311 ## Medical and Health -0.18778444 0.8259030 -0.2584369 ## Personal Care -0.04812680 0.2074885 -0.4372590 ## Private Education -0.03250802 0.1408623 0.7400691 Podemos considerar ahora la segunda dimensión que encontramos. En los scores: \\(u_2\\) tiene valores altos en el rubro 3 (salud), y valores negativos en rubro 1. Es un patrón de gasto más alto en todo menos en comida (que es el rubro 1), especialmente en salud. Ahora vemos \\(v_2\\): tiene un valor alto en el año 60 (3a entrada), y valores más negativos para los dos primeros años (40 y 50) Así que decimos que en los 60, el ingreso se desplazó hacia salud (y otros rubros en general), reduciéndose el de comida. Si multiplicamos podemos ver la contribución de esta matriz de rango 1 (en billones (US) de dólares): d &lt;- svd_gasto$d (d[2]*tcrossprod(svd_gasto$u[,2], svd_gasto$v[,2])) |&gt; round(1) ## [,1] [,2] [,3] ## [1,] 0.5 1.3 -1.0 ## [2,] -0.5 -1.2 0.9 ## [3,] -1.2 -2.8 2.2 ## [4,] -0.3 -0.7 0.5 ## [5,] -0.2 -0.5 0.4 Este es un efecto relativamente chico (comparado con el patrón estable de la primera dimensión), pero ilumina todavía un aspecto adicional de esta tabla. La norma de la diferencia entre la matriz \\(X\\) y la aproximación de rango 2 podemos calcularla de dos maneras: sum(X_arr^2) - sum(d[1:2]^2) ## [1] 0.1415665 O calculando la aproximación y la diferencia directamente. Podemos hacerlo de la siguiente forma X_arr_2 &lt;- d[1]*tcrossprod(u[,1], v[,1]) + d[2]*tcrossprod(u[,2], v[,2]) sum((X_arr - X_arr_2)^2) ## [1] 0.1415665 Pero podemos calcular la aproximación \\(X_2\\) en forma matricial, haciendo X_arr_2 &lt;- u[,1:2] %*% diag(d[1:2]) %*% t(v[,1:2]) sum((X_arr - X_arr_2)^2) ## [1] 0.1415665 4.5 Descomposición en valores singulares (SVD o DVS) Aunque ya hemos enunciado los resultados, podemos enunciar el teorema de descomposición en valores singulares en términos matriciales. Supongamos entonces que tenemos una aproximación de rango \\(k\\) \\[X_k = \\sigma_1 u_1v_1^t + \\sigma_2 u_2v_2^t + \\ldots \\sigma_k u_kv_k^t\\] Se puede ver que esta aproximación se escribe como (considera todos los vectores como vectores columna) \\[ X_k = (u_1,u_2, \\ldots, u_k) \\left( {\\begin{array}{ccccc} \\sigma_1 &amp; 0 &amp; \\cdots &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2 &amp; 0 &amp;\\cdots &amp; 0 \\\\ \\vdots &amp; &amp; &amp; \\vdots\\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\sigma_k \\\\ \\end{array} } \\right) \\left ( \\begin{array}{c} v_1^t \\\\ v_2^t \\\\ \\vdots \\\\ v_k^t \\end{array} \\right)\\] o más simplemente, como \\[X_k = U_k \\Sigma_k V_k^t\\] donde \\(U_k\\) (\\(n\\times k\\)) contiene los vectores \\(u_i\\) en sus columnas, \\(V_k\\) (\\(k\\times p\\)) contiene los vectores \\(v_j\\) en sus columnas, y la matriz \\(\\Sigma_k\\) es la matriz diagonal con los primeros \\(\\sigma_1\\geq \\sigma_2\\geq\\cdots \\sigma_k\\) valores singulares. Ver el ejemplo anterior para ver cómo los cálculos son iguales. Descomposición en valores singulares Sea \\(X\\) una matriz de \\(n\\times p\\) con \\(p\\leq n\\). Entonces existe una factorización \\[X=U\\Sigma V^t,\\] \\(\\Sigma\\) es una matriz diagonal con valores no-negativos (valores singulares). Los valores singulares de \\(\\Sigma\\) estan ordenados en orden decreciente. Las columnas de U y V son vectores ortogonales unitarios. La i-ésima columna \\(u_i\\) de \\(V\\) y la i-ésima columna \\(v_i\\) de \\(V\\) son pares de vectores propios \\((u_i, v_i)\\) izquierdo y derecho de \\(X\\) con valor singular \\(\\sigma_i = \\Sigma_{i,i}\\) Una vez que tenemos esta descomposición, podemos extraer la aproximación que nos sea útil: una aproximación \\(X_k\\) de orden \\(k\\) se escribe como \\[X_k = U_k\\Sigma_k V_k^t\\] donde \\(U_k\\) contiene las primeras \\(k\\) columnas de \\(U\\), \\(V_k\\) las primeras \\(k\\) columnas de \\(V\\), y \\(\\Sigma_k\\) es la submatriz cuadrada \\(k\\times k\\) de los primeros \\(k\\) renglones y columnas de \\(\\Sigma\\) : knitr::include_graphics(&quot;images/svd.png&quot;) Frecuenta el teorema de aproximación óptima (teorema de Ekhart-Young) se deriva de la descomposición en valores singulares, que se demuestra antes usando técnicas de álgebra lineal. 4.6 Más de interpretación geométrica Proyecciones Los vectores \\(v_1,v_2, \\ldots, v_p\\) están en el espacio de variables o columnas (son de dimensión \\(p\\)). La componente de la proyección (ver proyección de vectores ) de la matriz de datos sobre una de estas dimensiones está dada por \\[Xv_j,\\] que son iguales a los scores de los casos escalados por \\(\\sigma\\): \\[\\sigma_j u_j\\]. Las proyecciones \\(d_j = \\sigma_j u_j\\) son las variables que normalmente se usan para hacer análisis posterior, aunque cuando la escala de las proyecciones no es importante, también se pueden usar simplemente las \\(u_j\\). Por ejemplo, la projeccion del rengón \\(x_i\\) de la matriz \\(X\\) es \\((x_i\\cdot v_j) v_j\\) (nótese que \\(x_i \\cdot v_j\\) es un escalar, la componente de la proyección). Consideremos nuestro ejemplo anterior: set.seed(3221) x_1 &lt;- rnorm(200,2, 1) x_2 &lt;- rnorm(200,0,1) + x_1 datos &lt;- data_frame(x_1, x_2) ggplot(datos, aes(x=x_1, y=x_2)) + geom_point() + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) Hacemos descomposición en valores singulares y graficamos svd_x &lt;- svd(datos) v &lt;- svd_x$v |&gt; t() |&gt; as_tibble(.name_repair = NULL) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. u &lt;- svd_x$u |&gt; as_tibble(.name_repair = NULL) colnames(v) &lt;- c(&#39;x_1&#39;,&#39;x_2&#39;) colnames(u) &lt;- c(&#39;x_1&#39;,&#39;x_2&#39;) d &lt;- svd_x$d #Nota: podemos mover signos para hacer las gráficas y la interpetación # más simples v[,1] &lt;- - v[,1] u[,1] &lt;- - u[,1] v ## # A tibble: 2 × 2 ## x_1 x_2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.673 -0.740 ## 2 0.740 0.673 Graficamos ahora los dos vectores \\(v_1\\) y \\(v_2\\), escalándolos para ver mejor cómo quedan en relación a los datos (esto no es necesario hacerlo): ggplot(datos) + geom_point(aes(x=x_1, y=x_2)) + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) + geom_segment(data = v, aes(xend= 4*x_1, yend=4*x_2, x=0, y=0), col=&#39;red&#39;, size=1.1, arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + coord_equal() El primer vector es el “que pasa más cercano a los puntos,” en el sentido de que la distancia entre los datos proyectados al vector y los datos es lo más chica posible (mejor aproximación). La proyección de los datos sobre \\(v\\) es igual a \\(Xv_1=\\sigma_1 u_1\\), es decir, está dada por \\(\\sigma u_1\\) Las proyecciones de los datos sobre el segundo vector \\(v_2\\) están dadas igualmente por \\(\\sigma_2 u_2\\). Sumamos esta proyección a la de la primera dimensión para obtener una mejor aproximación a los datos (en este caso, exacta). Por ejemplo, seleccionemos el primer punto y obtengamos sus proyecciones: proy_1 &lt;- (d[1])*as.numeric(u[1,1])*v$x_1 #v_1 por el score en la dimensión 1 u[1,1] proy_2 &lt;- (d[2])*as.numeric(u[1,2])*v$x_2 #v_2 por el score en la dimensión 1 u[1,1] proy_2 + proy_1 ## [1] 3.030313 1.883698 datos[1,] ## # A tibble: 1 × 2 ## x_1 x_2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3.03 1.88 Podemos graficar la aproximación sucesiva: datos$selec &lt;- c(&#39;seleccionado&#39;, rep(&#39;no_seleccionado&#39;, nrow(datos)-1)) ggplot(datos) + geom_point(aes(x=x_1, y=x_2, colour=selec, size=selec)) + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) + geom_segment(aes(xend= proy_1[1], yend=proy_1[2], x=0, y=0), col=&#39;red&#39;, size=1.1, arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + geom_segment(aes(xend= proy_2[1] + proy_1[1], yend=proy_2[2] + proy_1[2], x=proy_1[1], y=proy_1[2]), col=&#39;red&#39;, size=1.1, arrow = arrow(length = unit(0.2,&quot;cm&quot;))) + coord_equal() Las aproximaciones de la descomposión en valores singulares mediante matrices de rango 1 puede entenderse como la búsqueda sucesiva de subespacios de dimensión baja, donde al proyectar los datos perdemos poca información. Las proyecciones sucesivas se hacen sobre vectores ortogonales, y en este sentido la DVS separa la información en partes que no tienen contenido común (desde el punto de vista lineal). Finalmente, muchas veces graficamos las proyecciones en el nuevo espacio creado por las dimensiones de la DVS (nótese la escala distinta de los ejes). proyecciones &lt;- data_frame(dim_1 = d[1]*u$x_1, dim_2 = d[2]*u$x_2, selec = datos$selec) ggplot(proyecciones, aes(x = dim_1, y = dim_2, size=selec, colour=selec)) + geom_point() ## Warning: Using size for a discrete variable is not advised. 4.7 SVD para películas de netflix Vamos a intentar encontrar dimensiones latentes para los datos del concurso de predicción de Netflix (una de las componentes de las soluciones ganadoras fue descomposición en valores singulares). ruta &lt;- &quot;../datos/netflix/dat_muestra_nflix.csv&quot; url &lt;- &quot;https://s3.amazonaws.com/ma-netflix/dat_muestra_nflix.csv&quot; if(!file.exists(ruta)){ evals_tbl &lt;- read_csv(url, col_types = &quot;iii&quot;) write_csv(evals_tbl, ruta) } else { evals_tbl &lt;- read_csv(ruta, col_types = &quot;iii&quot;) } evals_tbl ## # A tibble: 20,968,941 × 5 ## peli_id usuario_id_orig calif fecha usuario_id ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;dbl&gt; ## 1 1 2442 3 2004-04-14 1 ## 2 1 1086807 3 2004-12-28 2 ## 3 1 2165002 4 2004-04-06 3 ## 4 1 1133214 4 2004-03-07 4 ## 5 1 1537427 4 2004-03-29 5 ## 6 1 525356 2 2004-07-11 6 ## 7 1 1910569 4 2004-04-12 7 ## 8 1 2421815 2 2004-02-26 8 ## 9 1 2508819 3 2004-05-18 9 ## 10 1 1342007 3 2004-07-16 10 ## # … with 20,968,931 more rows peliculas_nombres &lt;- read_csv(&quot;../datos/netflix/movies_title_fix.csv&quot;, col_names = c(&quot;peli_id&quot;, &quot;year&quot;, &quot;name&quot;) ) ## Rows: 17770 Columns: 3 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): year, name ## dbl (1): peli_id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. peliculas_nombres ## # A tibble: 17,770 × 3 ## peli_id year name ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 2003 Dinosaur Planet ## 2 2 2004 Isle of Man TT 2004 Review ## 3 3 1997 Character ## 4 4 1994 Paula Abdul&#39;s Get Up &amp; Dance ## 5 5 2004 The Rise and Fall of ECW ## 6 6 1997 Sick ## 7 7 1992 8 Man ## 8 8 2004 What the #$*! Do We Know!? ## 9 9 1991 Class of Nuke &#39;Em High 2 ## 10 10 2001 Fighter ## # … with 17,760 more rows Hay muchas peliculas que no son evaluadas por ningún usuario. Aquí tenemos que decidir cómo tratar estos datos: si los rellenamos con 0, la implicación es que un usuario tiene bajo interés en una película que no ha visto. Hay otras opciones (y quizá un método que trate apropiadamente los datos faltantes es mejor). library(Matrix) library(irlba) if(TRUE){ evals_tbl &lt;- evals_tbl |&gt; group_by(usuario_id) |&gt; mutate(calif_centrada = calif - mean(calif)) |&gt; ungroup() #Usamos matriz rala, de otra manera la matriz es demasiado grande evals_mat &lt;- sparseMatrix(i = as.integer(evals_tbl$usuario_id), j = as.integer(evals_tbl$peli_id), x = evals_tbl$calif_centrada) set.seed(81) svd_parcial &lt;- irlba(evals_mat, 6) } svd_parcial$d ## [1] 917.5007 598.4652 392.0046 342.3649 333.8895 327.0080 #no correr en notas V_peliculas &lt;- svd_parcial$v |&gt; as_tibble(.name_repair = NULL) |&gt; mutate(peli_id = row_number()) |&gt; left_join(peliculas_nombres |&gt; mutate(pelicula_id = peli_id)) ## Joining, by = &quot;peli_id&quot; U_usuarios &lt;- svd_parcial$u |&gt; as_tibble(.name_repair = NULL) Examinamos la primera y segunda componente en los pesos de las películas \\(V\\): library(ggrepel) pel_graf &lt;- V_peliculas |&gt; mutate(dist_0 = sqrt(V1^2 + V2^2)) muestra &lt;- pel_graf |&gt; mutate(etiqueta = name) |&gt; mutate(etiqueta = ifelse(dist_0 &gt; 0.05, name, &#39;&#39;)) ggplot(muestra, aes(x = V1, y = V2, label=etiqueta)) + geom_point(alpha = 0.15) + geom_text_repel(size = 2.0, max.overlaps = 12) ¿Qué nombres pondrías a estas dimensiones? Puedes examinar otras. 4.7.1 Calidad de representación de SVD. Podemos hacer varios cálculos para entender qué tan buena es nuestra aproximación de rango bajo \\(X_k\\). Por ejemplo, podríamos calcular las diferencias de \\(X-X_k\\) y presentarlas de distinta forma. Ejemplo En el ejemplo de rubros de gasto, podríamos mostrar las diferencias en billones (us) de dólares, donde vemos que la aproximación es bastante buena qplot(as.numeric(X_arr-X_arr_2)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Que podríamos resumir, por ejemplo, con la media de errores absolutos: mean(abs(as.numeric(X_arr-X_arr_2))) ## [1] 0.06683576 Otra opción es usar la norma Frobenius, calculando para la apoximación de rango 1 1 - (sum(X_arr^2) - sum(svd_gasto$d[1]^2))/sum(X_arr^2) ## [1] 0.9986246 Lo que indica que capturamos 99.8% de la información, y para la de rango 2: d 1-(sum(X_arr^2) - sum(svd_gasto$d[1:2]^2))/sum(X_arr^2) ## [1] 0.9999907 Lo que indica que estos datos (en 3 variables), podemos entenderlos mediante un análisis de dos dimensiones Podemos medir la calidad de la representación de \\(X\\) (\\(n\\times p\\) con \\(p &lt; n\\)) de una aproximación \\(X_k\\) de SVD mediante \\[1-\\frac{||X-X_k||_F^2}{||X||_F^2} = \\frac{\\sigma_1^2 + \\sigma_2^2 + \\cdots \\sigma_k^2}{\\sigma_1^2 + \\sigma_2^2 + \\cdots \\sigma_p^2},\\] que es un valor entre 0 y 1. Cuanto más cercana a 1 está, mejor es la representación. Observaciones: Dependiendo de nuestro objetivo, nos interesa alcanzar distintos niveles de calidad de representación. Por ejemplo, algunas reglas de dedo: Si queremos usar los datos para un proceso posterior, o dar una descripción casi completa de los datos, quizá buscamos calidad \\(&gt;0.9\\) o mayor. Si nos interesa extraer los patrones más importantes, podemos considerar valores de calidad mucho más chicos, entendiendo que hay una buena parte de la información que no se explica por nuestra aproximación. 4.8 Componentes principales Componentes principales es la descomposición en valores singulares aplicada a una matriz de datos centrada por columna. Esta operación convierte el problema de aproximación de matrices de rango bajo en uno de aproximaciones que buscan explicar la mayoría de la varianza (incluyendo covarianza) de las variables de la matriz de datos \\(X\\). Consideremos entonces una matriz de datos \\(X\\) de tamaño \\(n\\times p\\). Definimos la matrix centrada por columna \\(\\tilde{X}\\) , que se calcula como \\[\\tilde{X}_{i,j} = X_{i,j} - \\mu_j\\] donde \\(\\mu_j = \\frac{1}{n} \\sum_j X_{i,j}\\). - La diferencia en construcción entre Svd y Svd con columnas centradas (componentes principales) es que en Svd las proyecciones se hacen pasando por el origen, pero en componentes principales se hacen a partir del centroide de los datos. ### Ejemplo {-} Veamos primero el último ejemplo simulado que hicimos anterioremnte. Primero centramos los datos por columna: datos_c &lt;- scale(datos |&gt; select(-selec), scale = FALSE) |&gt; as_tibble() ggplot(datos_c, aes(x=x_1, y=x_2)) + geom_point() + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) Y ahora calculamos la descomposición en valores singulares svd_x &lt;- svd(datos_c) v &lt;- t(svd_x$v) |&gt; as_tibble() u &lt;- svd_x$u |&gt; as_tibble() colnames(v) &lt;- c(&#39;x_1&#39;,&#39;x_2&#39;) colnames(u) &lt;- c(&#39;x_1&#39;,&#39;x_2&#39;) d &lt;- svd_x$d v ## # A tibble: 2 × 2 ## x_1 x_2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.507 0.862 ## 2 0.862 -0.507 Notemos que los resultados son similares, pero no son los mismos. Graficamos ahora los dos vectores \\(v_1\\) y \\(v_2\\), que en este contexto se llaman direcciones principales ggplot(datos_c) + geom_point(aes(x=x_1, y=x_2)) + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) + geom_segment(data = v, aes(xend= 5*x_1, yend=5*x_2, x=0, y=0), col=&#39;red&#39;, size=1.1, arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + coord_equal() Las componentes de las proyecciones de los datos sobre las direcciones principales dan las componentes principales (nótese que multiplicamos por los valores singulares): head(svd_x$u %*% diag(svd_x$d)) ## [,1] [,2] ## [1,] 0.3230641 0.9257829 ## [2,] 0.4070429 1.5360770 ## [3,] -1.2788977 -0.2762829 ## [4,] 0.8910247 0.4071926 ## [5,] -4.4466993 -0.5743111 ## [6,] -1.2267878 0.3470759 Que podemos graficar comps &lt;- svd_x$u %*% diag(svd_x$d) |&gt; as_tibble() ggplot(comps, aes(x=V1, y=V2)) + geom_point()+ geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) Este resultado lo podemos obtener directamente usando la función princomp comp_principales &lt;- princomp(datos |&gt; select(-selec)) scores &lt;- comp_principales$scores head(scores) ## Comp.1 Comp.2 ## [1,] 0.3230641 0.9257829 ## [2,] 0.4070429 1.5360770 ## [3,] -1.2788977 -0.2762829 ## [4,] 0.8910247 0.4071926 ## [5,] -4.4466993 -0.5743111 ## [6,] -1.2267878 0.3470759 Y verificamos que los resultados son los mismos: qplot(scores[,1], comps$V1) qplot(scores[,2], -comps$V2) 4.8.1 Varianza en componentes principales. Cuando centramos por columna, la svd es un tipo de análisis de la matriz de varianzas y covarianzas de la matriz \\(X\\), dada por \\[C = \\frac{1}{n} \\tilde{X}^t \\tilde{X}\\] (Nota: asegúrate de que entiendes por qué esta es la matriz de varianzas y covarianzas de \\(X\\)). Nótese que las proyecciones (que se llaman componentes principales) \\(\\tilde{X}v_j = \\sigma_j u_j = d_j\\) satisfacen que 1. La media de las proyecciones \\(d_j\\) es igual a cero Pues \\[\\sigma_j \\sum_k {u_{j,k}} = \\sum_k \\sum_i (\\tilde{X}_{k,i})v_{j,i} = \\sum_i v_{j,i}\\sum_k (\\tilde{X}_{k,i}) = 0,\\] pues las columnas de \\(\\tilde{X}\\) tienen media cero. 2- \\(\\sigma_j^2\\) es la varianza de la proyección \\(d_j\\), pues \\[Var(d_j) = \\sigma_j^2 \\sum_k (u_{j,k} - 0)^2 = \\sigma_j^2,\\] y el vector \\(u_j\\) tienen norma 1. 3. La ortogonalidad de los vectores \\(u_j\\) se interpreta ahora en términos de covarianza: \\[Cov(d_i,d_j) = \\frac{1}{n}\\sum_{k=1}^n (d_{i,k}-0)(d_{j,k}-0) = \\frac{1}{n}\\sum_{k=1}^n \\sigma_j\\sigma_i u_{i,k}u_{j,k} = 0\\] Así que Buscamos sucesivamente direcciones para proyectar que tienen varianza máxima (ver ejemplo anterior), y que sean no correlacionadas de forma que no compartan información lineal entre ellas. Adicionalmente, vimos que podíamos escribir \\[||\\tilde{X}||^2_F = \\sum_{j=1}^p \\sigma_{j}^2\\] Y el lado izquierdo es en este caso una suma de varianzas: \\[\\sum_{j=1}^p Var(X_j) = \\sum_{j=1}^p \\sigma_{j}^2.\\] El lado izquierdo se llama Varianza total de la matriz \\(X\\). Componentes principales particiona la varianza total de la matriz \\(X\\) en componentes . ## ¿Centrar o no centrar por columna? Típicamente, antes de aplicar SVD hacemos algunos pasos de procesamiento de las variables. En componentes principales, este paso de procesamiento es centrar la tabla por columnas. Conviene hacer esto cuando: Centramos si las medias de las columnas no tienen información importante o interesante para nuestros propósitos - es mejor eliminar esta parte de variación desde el principio para no lidiar con esta información en las dimensiones que obtengamos. En otro caso, quizá es mejor no centrar. Centramos si nos interesa más tener una interpretación en términos de varianzas y covarianzas que hacer una aproximación de los datos originales. Sin embargo, también es importante notar que muchas veces los resultados de ambos análisis son similares en cuanto a interpretación y en cuanto a usos posteriores de las dimensiones obtenidas. Pueden ver análisis detallado en este artículo, que hace comparaciones a lo largo de varios conjuntos de datos. ### Ejemplo: resultados similares{-} En el ejemplo de gasto en rubros que vimos arriba, los pesos \\(v_j\\) son muy similares: comps_1 &lt;- princomp(USPersonalExpenditure[,c(1,3,5)]) svd_1 &lt;- svd(USPersonalExpenditure[,c(1,3,5)]) comps_1$loadings[,] ## Comp.1 Comp.2 Comp.3 ## 1940 0.2099702 0.2938755 0.93249650 ## 1950 0.5623341 0.7439168 -0.36106546 ## 1960 0.7998081 -0.6001875 0.00905589 svd_1$v ## [,1] [,2] [,3] ## [1,] -0.2007388 -0.3220495 -0.92519623 ## [2,] -0.5423269 -0.7499672 0.37872247 ## [3,] -0.8158342 0.5777831 -0.02410854 comps_1$scores ## Comp.1 Comp.2 Comp.3 ## Food and Tobacco 68.38962 0.8783065 0.06424607 ## Household Operation 16.25334 -0.9562769 -0.16502902 ## Medical and Health -16.13276 -2.2900370 0.07312028 ## Personal Care -33.29512 1.0003211 0.23036177 ## Private Education -35.21507 1.3676863 -0.20269910 svd_1$u %*% diag(svd_1$d) ## [,1] [,2] [,3] ## [1,] -107.593494 -1.6959766 -0.06011861 ## [2,] -55.526778 1.5630078 0.15457655 ## [3,] -23.188704 3.7722060 -0.09723774 ## [4,] -5.942974 0.9476773 -0.16452015 ## [5,] -4.014277 0.6433704 0.27845344 Llegaríamos a conclusiones similares si interpretamos cualquiera de los dos análisis (verifica por ejemplo el ordenamiento de rubros y años en cada dimensión). ## Ejemplos: donde es buena idea centrar {-} Por ejemplo, si hacemos componentes principales con los siguientes datos: whisky &lt;- read_csv(&#39;../datos/whiskies.csv&#39;) head(whisky) ## # A tibble: 6 × 17 ## RowID Distillery Body Sweetness Smoky Medicinal Tobacco Honey Spicy Winey ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Aberfeldy 2 2 2 0 0 2 1 2 ## 2 02 Aberlour 3 3 1 0 0 4 3 2 ## 3 03 AnCnoc 1 3 2 0 0 2 0 0 ## 4 04 Ardbeg 4 1 4 4 0 0 2 0 ## 5 05 Ardmore 2 2 2 0 0 1 1 1 ## 6 06 ArranIsleOf 2 3 1 1 0 1 1 1 ## # … with 7 more variables: Nutty &lt;dbl&gt;, Malty &lt;dbl&gt;, Fruity &lt;dbl&gt;, ## # Floral &lt;dbl&gt;, Postcode &lt;chr&gt;, Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt; whisky_sabor &lt;- whisky |&gt; select(Body:Floral) comp_w &lt;- princomp(whisky_sabor) Veamos los pesos de las primeras cuatro dimensiones round(comp_w$loadings[, 1:4], 2) ## Comp.1 Comp.2 Comp.3 Comp.4 ## Body 0.36 0.49 0.03 0.07 ## Sweetness -0.20 0.05 -0.26 0.37 ## Smoky 0.48 0.07 0.22 -0.09 ## Medicinal 0.58 -0.16 0.04 -0.08 ## Tobacco 0.09 -0.02 0.00 0.03 ## Honey -0.22 0.42 0.11 -0.03 ## Spicy 0.06 0.18 0.70 0.17 ## Winey -0.04 0.64 -0.23 0.23 ## Nutty -0.05 0.26 -0.18 -0.85 ## Malty -0.13 0.10 0.11 -0.07 ## Fruity -0.20 0.12 0.40 -0.09 ## Floral -0.38 -0.13 0.34 -0.15 La primera componente separa whisky afrutado/floral/dulce de los whishies ahumados con sabor medicinal. La segunda componente separa whiskies con más cuerpo, características de vino y miel de otros más ligeros. Las siguientes componentes parece oponer Spicy contra Fruity y Floral, y la tercera principalmente contiene la medición de Nutty. Según vimos arriba, podemos ver que porcentaje de la varianza explica cada componente summary(comp_w) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## Standard deviation 1.5268531 1.2197972 0.86033607 0.79922719 0.74822104 ## Proportion of Variance 0.3011098 0.1921789 0.09560193 0.08250322 0.07230864 ## Cumulative Proportion 0.3011098 0.4932887 0.58889059 0.67139381 0.74370245 ## Comp.6 Comp.7 Comp.8 Comp.9 Comp.10 ## Standard deviation 0.6811330 0.62887454 0.59593956 0.52041611 0.49757158 ## Proportion of Variance 0.0599231 0.05108089 0.04587064 0.03498097 0.03197728 ## Cumulative Proportion 0.8036256 0.85470644 0.90057708 0.93555805 0.96753533 ## Comp.11 Comp.12 ## Standard deviation 0.42174644 0.271073661 ## Proportion of Variance 0.02297382 0.009490848 ## Cumulative Proportion 0.99050915 1.000000000 Y vemos que las primeras dos componentes explican casi el 50% de la varianza. Las siguientes componentes aportan relativamente pooca varianza comparada con la primera Podemos graficar los whiskies en estas dos dimensiones: library(ggrepel) scores_w &lt;- comp_w$scores |&gt; as_tibble() scores_w$Distillery &lt;- whisky$Distillery ggplot(scores_w, aes(x=Comp.1, y= -Comp.2, label=Distillery)) + geom_vline(xintercept=0, colour = &#39;red&#39;) + geom_hline(yintercept=0, colour = &#39;red&#39;) + geom_point()+ geom_text_repel(size=2.5, segment.alpha = 0.3, force = 0.1, seed=202) + xlab(&#39;Fruity/Floral vs. Smoky/Medicional&#39;) + ylab(&#39;Winey/Body and Honey&#39;) ## Warning: ggrepel: 12 unlabeled data points (too many overlaps). Consider ## increasing max.overlaps ¿Que pasa si usamos svd sin centrar? Vemos que la primera componente simplemente captura los distintos niveles promedio de las variables. Esta componente no es muy interesante, pues por las características del whisky es normal que Medicinal o Tabaco tengo una media baja, comparado con dulzor, Smoky, etc. Adicionalmente, el vector \\(u\\) asociado a esta dimensión tiene poca variación: svd_w &lt;- svd(whisky_sabor) svd_w$v[,1:2] ## [,1] [,2] ## [1,] -0.39539241 -0.38286900 ## [2,] -0.42475240 0.19108176 ## [3,] -0.28564145 -0.48775482 ## [4,] -0.09556061 -0.57453247 ## [5,] -0.02078706 -0.09187451 ## [6,] -0.24191199 0.20518808 ## [7,] -0.26485793 -0.07103866 ## [8,] -0.19488910 0.01930294 ## [9,] -0.27858538 0.03430020 ## [10,] -0.33669488 0.11644937 ## [11,] -0.34001725 0.18933847 ## [12,] -0.31441595 0.37730436 plot(svd_w$v[,1], apply(whisky_sabor, 2, mean)) mean(svd_w$u[,1]) ## [1] -0.1063501 sd(svd_w$u[,1]) ## [1] 0.01792508 Observación La primera componente de svd está haciendo el trabajo de ajustar la media. Como no nos interesa este hecho, podemos mejor centrar desde el principio y trabajar con las componentes principales. ¿Cómo se ven las siguientes dos dimensiones del análisis no centrado? ## Ejemplo: donde no centrar funciona bien {-} Considera el ejemplo de la tarea con la tabla de gastos en distintas categorías de alimentos según el decil de ingreso del hogar. ¿Por qué en este ejemplo centrar por columna no es tan buena idea? Si hacemos el centrado, quitamos información importante de la tabla, que es que los distintos deciles tienen distintos niveles de gasto. Veamos como lucen los dos análisis. Para componentes principales: deciles &lt;- read_csv(&#39;../datos/enigh_deciles.csv&#39;) deciles |&gt; arrange(desc(d1)) ## # A tibble: 13 × 11 ## nombre d1 d2 d3 d4 d5 d6 d7 dd8 d9 d10 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 CEREAL… 1.33e6 1.87e6 2.25e6 2.33e6 2.58e6 2.59e6 2.84e6 2.77e6 2.74e6 2.71e6 ## 2 CARNES 1.07e6 1.75e6 2.13e6 2.51e6 2.97e6 3.23e6 3.71e6 3.94e6 4.18e6 4.72e6 ## 3 VERDUR… 9.74e5 1.28e6 1.48e6 1.59e6 1.67e6 1.73e6 1.78e6 1.81e6 1.83e6 1.98e6 ## 4 LECHE … 5.86e5 8.95e5 1.24e6 1.40e6 1.58e6 1.78e6 1.97e6 2.12e6 2.36e6 3.09e6 ## 5 OTROS … 2.90e5 4.49e5 6.90e5 7.82e5 1.03e6 1.12e6 1.45e6 1.54e6 2.28e6 2.71e6 ## 6 HUEVO 2.55e5 3.60e5 4.22e5 4.43e5 4.06e5 4.05e5 4.51e5 4.19e5 3.99e5 3.65e5 ## 7 FRUTAS 1.92e5 2.84e5 3.38e5 4.68e5 5.18e5 5.71e5 7.05e5 7.65e5 8.82e5 1.38e6 ## 8 AZUCAR… 1.67e5 2.13e5 2.00e5 1.91e5 2.02e5 1.90e5 1.57e5 1.74e5 1.64e5 1.63e5 ## 9 ACEITE… 1.36e5 1.90e5 1.80e5 1.84e5 1.94e5 1.97e5 1.89e5 1.81e5 1.82e5 2.09e5 ## 10 PESCAD… 1.10e5 1.88e5 2.14e5 2.36e5 2.87e5 2.97e5 3.34e5 4.37e5 4.97e5 8.65e5 ## 11 TUBERC… 1.07e5 1.58e5 1.91e5 2.02e5 2.29e5 2.15e5 2.14e5 2.24e5 2.22e5 2.28e5 ## 12 CAFE, … 7.19e4 1.20e5 1.09e5 9.71e4 1.25e5 1.29e5 1.10e5 1.26e5 1.43e5 2.25e5 ## 13 ESPECI… 5.76e4 8.06e4 9.18e4 1.09e5 1.16e5 1.34e5 1.55e5 1.52e5 1.68e5 1.82e5 deciles &lt;- deciles |&gt; column_to_rownames(var = &quot;nombre&quot;) comp_enigh &lt;- princomp(deciles) Veamos las primeras dos componente, cuyas direcciones principales son: comp_enigh$loadings[,1:2] ## Comp.1 Comp.2 ## d1 0.1224572 0.29709420 ## d2 0.1858230 0.35463967 ## d3 0.2324626 0.34856083 ## d4 0.2610938 0.29531199 ## d5 0.3010861 0.23322242 ## d6 0.3221099 0.16749474 ## d7 0.3650886 0.07154327 ## dd8 0.3783732 -0.02514659 ## d9 0.4019500 -0.30553359 ## d10 0.4425357 -0.62905737 Y los scores son: comp_enigh$scores[,1:2] ## Comp.1 Comp.2 ## CEREALES 4548094.6 1191716.24 ## CARNES 7068531.1 -392100.57 ## PESCADOS Y MARISCOS -1880143.9 -290076.15 ## LECHE Y SUS DERIVADOS 2688171.0 -541441.01 ## HUEVO -1882277.7 346789.97 ## ACEITES Y GRASAS -2525107.6 157761.37 ## TUBERCULOS -2460994.3 134899.61 ## VERDURAS, LEGUMBRES, LEGUMINOSAS 2029622.4 715856.37 ## FRUTAS -960953.6 -445884.20 ## AZUCAR Y MIELES -2551904.3 217378.43 ## CAFE, TE Y CHOCOLATE -2685873.3 33326.41 ## ESPECIAS Y ADEREZOS -2679471.1 33837.02 ## OTROS ALIMENTOS DIVERSOS 1292306.9 -1162063.49 Y la tabla de rango 1 es tab_1 &lt;- tcrossprod(comp_enigh$scores[,1], comp_enigh$loadings[,1]) colnames(tab_1) &lt;- colnames(deciles) tab_1 &lt;- tab_1 |&gt; as_tibble() |&gt; mutate(categoria = rownames(deciles)) |&gt; gather(decil, gasto, d1:d10) tab_1$categoria &lt;- reorder(tab_1$categoria, tab_1$gasto, mean) ggplot(tab_1, aes(x=categoria, y=gasto, colour=decil, group=decil)) + geom_line() + coord_flip() Que podemos comparar con el análisis no centrado: svd_enigh &lt;- svd(deciles) tab_1 &lt;- tcrossprod(svd_enigh$u[,1], svd_enigh$v[,1]) colnames(tab_1) &lt;- colnames(deciles) tab_1 &lt;- tab_1 |&gt; as_tibble() |&gt; mutate(categoria = rownames(deciles)) |&gt; gather(decil, gasto, d1:d10) tab_1$categoria &lt;- reorder(tab_1$categoria, tab_1$gasto, mean) ggplot(tab_1, aes(x=categoria, y=gasto, colour=decil, group=decil)) + geom_line() + coord_flip() Y aunque los resultados son similares, puede ser más simple entender la primera dimensión del svd no centrado que guarda los efectos de los distintos niveles de gasto de los deciles. En el caso del análisis centrado, tenemos una primera componente que sólo se entiende bien sabiendo los niveles promedio de gasto a lo largo de las categorías. Observación: Quizá una solución más natural es hacer el análisis de componentes principales usando la transpuesta de esta matriz (usa la función prcomp), donde tiene más sentido centrar por categoría de alimento, y pensar que las observaciones son los distintos deciles (que en realidad son agrupaciones de observaciones). ### Otros tipos de centrado Es posible hacer doble centrado, por ejemplo (por renglón y por columna). Discute por qué el doble centrado puede ser una buena idea para los datos del tipo de Netflix. ### Reescalando variables Cuando las columnas tienen distintas unidades (especialmente si las escalas son muy diferentes), conviene reescalar la matriz antes de hacer el análisis centrado o no centrado. De otra forma, parte del análisis intenta absorber la diferencia en unidades, lo cual generalmente no es de interés. - En componentes principales, podemos estandarizar las columnas. - En el análisis no centrado, podemos poner las variables en escala 0-1, por ejemplo, o dividir entre la media (si son variables positivas). ### Ejemplo {-} comp &lt;- princomp(attenu |&gt; select(-station, -event)) comp$loadings[,1] ## mag dist accel ## 0.005746131 0.999982853 -0.001129688 Y vemos que la dirección de la primera componente es justamente en la dirección de la variable dist (es decir, la primera componente es dist). Esto es porque la escala de dist es más amplia: apply(attenu |&gt; select(-station), 2, mean) ## event mag dist accel ## 14.7417582 6.0840659 45.6032967 0.1542198 Esto lo corregimos estandarizando las columnas, o equivalentemente, usando cor = TRUE como opción en princomp comp &lt;- princomp(attenu |&gt; select(-station, -event), cor = TRUE) comp$loadings[,1] ## mag dist accel ## 0.5071375 0.7156080 -0.4803298 "],["referencias.html", "Referencias", " Referencias "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
